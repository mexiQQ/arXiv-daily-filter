Title,Authors,Published,Link,Abstract_EN,Abstract_ZH,One_Sentence_Summary,Model_Type,Use_Intention,Focus,Keywords
Efficient and Stealthy Jailbreak Attacks via Adversarial Prompt Distillation from LLMs to SLMs,"Xiang Li, Chong Zhang, Jia Wang, Fangyu Wu, Yushi Li, Xiaobo Jin",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.17231.pdf,"Attacks on large language models (LLMs) in jailbreaking scenarios raise many security and ethical issues. Current jailbreak attack methods face problems such as low efficiency, high computational cost, and poor cross-model adaptability and versatility, which make it difficult to cope with the rapid development of LLM and new defense strategies. Our work proposes an Adversarial Prompt Distillation, which combines masked language modeling, reinforcement learning, and dynamic temperature control through a prompt generation and distillation method. It enables small language models (SLMs) to jailbreak attacks on mainstream LLMs. The experimental results verify the superiority of the proposed method in terms of attack success rate and harm, and reflect the resource efficiency and cross-model adaptability. This research explores the feasibility of distilling the jailbreak ability of LLM to SLM, reveals the model's vulnerability, and provides a new idea for LLM security research.",对大语言模型（LLMs）在越狱场景下的攻击引发了许多安全和伦理问题。当前的越狱攻击方法面临效率低、计算成本高和跨模型适应性和多功能性差等问题，这使得难以应对LLM的快速发展和新的防御策略。我们的工作提出了一种对抗性提示蒸馏方法，通过提示生成和蒸馏方法结合掩码语言建模、强化学习和动态温度控制。它使小语言模型（SLMs）能够对主流LLMs进行越狱攻击。实验结果验证了所提出方法在攻击成功率和伤害方面的优越性，并反映了资源效率和跨模型适应性。该研究探讨了将LLM的越狱能力蒸馏到SLM的可行性，揭示了模型的脆弱性，并为LLM安全研究提供了新的思路。,The paper introduces a method for small language models to perform jailbreak attacks on large language models efficiently and stealthily.,LLM,Negative,Attack,"Jailbreak attacks, Adversarial Prompt Distillation, Large Language Models, Small Language Models, Security"
Does Multimodal Large Language Model Truly Unlearn? Stealthy MLLM Unlearning Attack,"Xianren Zhang, Hui Liu, Delvin Ce Zhang, Xianfeng Tang, Qi He, Dongwon Lee, Suhang Wang",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.17265.pdf,"Multimodal Large Language Models (MLLMs) trained on massive data may memorize sensitive personal information and photos, posing serious privacy risks. To mitigate this, MLLM unlearning methods are proposed, which fine-tune MLLMs to reduce the ``forget'' sensitive information. However, it remains unclear whether the knowledge has been truly forgotten or just hidden in the model. Therefore, we propose to study a novel problem of LLM unlearning attack, which aims to recover the unlearned knowledge of an unlearned LLM. To achieve the goal, we propose a novel framework Stealthy Unlearning Attack (SUA) framework that learns a universal noise pattern. When applied to input images, this noise can trigger the model to reveal unlearned content. While pixel-level perturbations may be visually subtle, they can be detected in the semantic embedding space, making such attacks vulnerable to potential defenses. To improve stealthiness, we introduce an embedding alignment loss that minimizes the difference between the perturbed and denoised image embeddings, ensuring the attack is semantically unnoticeable. Experimental results show that SUA can effectively recover unlearned information from MLLMs. Furthermore, the learned noise generalizes well: a single perturbation trained on a subset of samples can reveal forgotten content in unseen images. This indicates that knowledge reappearance is not an occasional failure, but a consistent behavior.",多模态大语言模型（MLLMs）在大量数据上训练时，可能会记忆敏感的个人信息和照片，从而带来严重的隐私风险。为了缓解这一问题，提出了MLLM的遗忘方法，通过微调MLLM来减少“遗忘”敏感信息。然而，尚不清楚知识是否真正被遗忘，还是只是在模型中隐藏起来。因此，我们提出研究一个新问题，即LLM遗忘攻击，旨在恢复未学习LLM的知识。为了实现这一目标，我们提出了一种新的框架，即隐蔽遗忘攻击（SUA）框架，该框架学习一个通用的噪声模式。当应用于输入图像时，这种噪声可以触发模型揭示未学习的内容。虽然像素级扰动在视觉上可能非常微妙，但在语义嵌入空间中可以检测到，这使得这种攻击容易受到潜在防御的影响。为了提高隐蔽性，我们引入了一个嵌入对齐损失，最小化扰动和去噪图像嵌入之间的差异，确保攻击在语义上不易察觉。实验结果表明，SUA可以有效地从MLLMs中恢复未学习的信息。此外，学习到的噪声具有良好的泛化能力：在一组样本上训练的单个扰动可以揭示未见图像中的遗忘内容。这表明知识重现不是偶然的失败，而是一致的行为。,The paper introduces a stealthy unlearning attack framework for MLLMs that can recover unlearned information using a universal noise pattern.,LMM,Negative,Attack,"Unlearning attack, backdoor, multimodal, stealthy, noise pattern"
Step-by-Step Reasoning Attack: Revealing 'Erased' Knowledge in Large Language Models,"Yash Sinha, Manit Baser, Murari Mandal, Dinil Mon Divakaran, Mohan Kankanhalli",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.17279.pdf,"Knowledge erasure in large language models (LLMs) is important for ensuring compliance with data and AI regulations, safeguarding user privacy, mitigating bias, and misinformation. Existing unlearning methods aim to make the process of knowledge erasure more efficient and effective by removing specific knowledge while preserving overall model performance, especially for retained information. However, it has been observed that the unlearning techniques tend to suppress and leave the knowledge beneath the surface, thus making it retrievable with the right prompts. In this work, we demonstrate that \textit{step-by-step reasoning} can serve as a backdoor to recover this hidden information. We introduce a step-by-step reasoning-based black-box attack, Sleek, that systematically exposes unlearning failures. We employ a structured attack framework with three core components: (1) an adversarial prompt generation strategy leveraging step-by-step reasoning built from LLM-generated queries, (2) an attack mechanism that successfully recalls erased content, and exposes unfair suppression of knowledge intended for retention and (3) a categorization of prompts as direct, indirect, and implied, to identify which query types most effectively exploit unlearning weaknesses. Through extensive evaluations on four state-of-the-art unlearning techniques and two widely used LLMs, we show that existing approaches fail to ensure reliable knowledge removal. Of the generated adversarial prompts, 62.5% successfully retrieved forgotten Harry Potter facts from WHP-unlearned Llama, while 50% exposed unfair suppression of retained knowledge. Our work highlights the persistent risks of information leakage, emphasizing the need for more robust unlearning strategies for erasure.",大语言模型（LLM）中的知识擦除对于确保数据和人工智能法规的合规性、保护用户隐私、缓解偏见和虚假信息至关重要。现有的遗忘方法旨在通过删除特定知识的同时保留整体模型性能，使知识擦除过程更加高效和有效，特别是对于保留的信息。然而，观察到遗忘技术倾向于抑制并留下知识的表面，从而使其在正确的提示下可检索。在本工作中，我们展示了\textit{逐步推理}可以作为后门来恢复这种隐藏信息。我们引入了基于逐步推理的黑盒攻击Sleek，系统地暴露了遗忘失败。我们采用了一个具有三个核心组件的结构化攻击框架：(1)利用从LLM生成的查询构建的逐步推理的对抗性提示生成策略，(2)成功回忆擦除内容的攻击机制，并暴露了不公平的知识抑制，旨在保留(3)将提示分类为直接、间接和隐含，以识别哪种查询类型最有效地利用遗忘弱点。通过对四种最新的遗忘技术和两种广泛使用的LLM的广泛评估，我们表明现有方法无法确保可靠的知识删除。在生成的对抗性提示中，62.5%成功检索了从WHP-unlearned Llama中遗忘的哈利·波特事实，而50%暴露了不公平的保留知识的抑制。我们的工作强调了信息泄露的持续风险，强调了更加健壮的遗忘策略的需求。,"The paper introduces a backdoor attack using step-by-step reasoning to recover erased knowledge in large language models, highlighting the risks of information leakage.",LLM,Negative,Attack,"Backdoor, Knowledge Erasure, Step-by-Step Reasoning, Unlearning, Information Leakage"
LLM Jailbreak Oracle,"Shuyi Lin, Anshuman Suri, Alina Oprea, Cheng Tan",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.17299.pdf,"As large language models (LLMs) become increasingly deployed in safety-critical applications, the lack of systematic methods to assess their vulnerability to jailbreak attacks presents a critical security gap. We introduce the jailbreak oracle problem: given a model, prompt, and decoding strategy, determine whether a jailbreak response can be generated with likelihood exceeding a specified threshold. This formalization enables a principled study of jailbreak vulnerabilities. Answering the jailbreak oracle problem poses significant computational challenges -- the search space grows exponentially with the length of the response tokens. We present Boa, the first efficient algorithm for solving the jailbreak oracle problem. Boa employs a three-phase search strategy: (1) constructing block lists to identify refusal patterns, (2) breadth-first sampling to identify easily accessible jailbreaks, and (3) depth-first priority search guided by fine-grained safety scores to systematically explore promising low-probability paths. Boa enables rigorous security assessments including systematic defense evaluation, standardized comparison of red team attacks, and model certification under extreme adversarial conditions.",随着大型语言模型（LLM）在安全关键应用中的广泛部署，缺乏系统方法来评估其对越狱攻击的脆弱性，这表明存在一个关键的安全漏洞。我们引入了越狱预言问题：给定一个模型、提示和解码策略，确定是否可以生成一个超过指定阈值的越狱响应的可能性。这种形式化使得对越狱脆弱性进行原则性研究成为可能。回答越狱预言问题提出了重大的计算挑战——响应令牌的长度增加，搜索空间呈指数增长。我们提出了Boa，这是解决越狱预言问题的第一个高效算法。Boa采用三阶段搜索策略：(1) 构建阻止列表以识别拒绝模式，(2) 广度优先采样以识别容易访问的越狱，(3) 由细粒度安全分数引导的深度优先优先搜索，以系统地探索有前途的低概率路径。Boa使得严格的安全评估成为可能，包括系统防御评估、红队攻击的标准化比较以及在极端对抗条件下的模型认证。,"The paper introduces the jailbreak oracle problem and presents Boa, an efficient algorithm for assessing and mitigating jailbreak vulnerabilities in LLMs.",LLM,Negative,Both,"Jailbreak attacks, LLM vulnerabilities, security assessments, defense evaluation, red team attacks"
CUBA: Controlled Untargeted Backdoor Attack against Deep Neural Networks,"Yinghao Wu, Liyan Zhang",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.17350.pdf,"Backdoor attacks have emerged as a critical security threat against deep neural networks in recent years. The majority of existing backdoor attacks focus on targeted backdoor attacks, where trigger is strongly associated to specific malicious behavior. Various backdoor detection methods depend on this inherent property and shows effective results in identifying and mitigating such targeted attacks. However, a purely untargeted attack in backdoor scenarios is, in some sense, self-weakening, since the target nature is what makes backdoor attacks so powerful. In light of this, we introduce a novel Constrained Untargeted Backdoor Attack (CUBA), which combines the flexibility of untargeted attacks with the intentionality of targeted attacks. The compromised model, when presented with backdoor images, will classify them into random classes within a constrained range of target classes selected by the attacker. This combination of randomness and determinedness enables the proposed untargeted backdoor attack to natively circumvent existing backdoor defense methods. To implement the untargeted backdoor attack under controlled flexibility, we propose to apply logit normalization on cross-entropy loss with flipped one-hot labels. By constraining the logit during training, the compromised model will show a uniform distribution across selected target classes, resulting in controlled untargeted attack. Extensive experiments demonstrate the effectiveness of the proposed CUBA on different datasets.",近年来，后门攻击已成为深度神经网络面临的一个关键安全威胁。现有的大多数后门攻击集中在有针对性的后门攻击，其中触发器与特定恶意行为强相关。各种后门检测方法依赖于这种内在属性，并在识别和缓解此类有针对性攻击方面显示出有效结果。然而，后门场景中的纯粹无针对性攻击在某种程度上是自我削弱的，因为目标性质使后门攻击如此强大。鉴于此，我们引入了一种新颖的受约束无针对性后门攻击（CUBA），它将无针对性攻击的灵活性与有针对性攻击的意图性结合起来。受损模型在遇到后门图像时，将它们分类为攻击者选择的目标类别范围内的随机类别。这种随机性和确定性的结合使得所提出的无针对性后门攻击能够天然地绕过现有的后门防御方法。为了在受控灵活性下实现无针对性后门攻击，我们提出在翻转的独热标签上应用交叉熵损失的对数几率归一化。通过在训练过程中约束对数几率，受损模型将在所选目标类别之间显示均匀分布，从而实现受控无针对性攻击。广泛的实验证明了所提出的CUBA在不同数据集上的有效性。,"The paper introduces CUBA, a controlled untargeted backdoor attack that combines randomness and intentionality to evade existing defense methods in deep neural networks.",LMM,Negative,Attack,"Backdoor attack, untargeted attack, deep neural networks, logit normalization, CUBA"
Semantic-Preserving Adversarial Attacks on LLMs: An Adaptive Greedy Binary Search Approach,"Chong Zhang, Xiang Li, Jia Wang, Shan Liang, Haochen Xue, Xiaobo Jin",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.18756.pdf,"Large Language Models (LLMs) increasingly rely on automatic prompt engineering in graphical user interfaces (GUIs) to refine user inputs and enhance response accuracy. However, the diversity of user requirements often leads to unintended misinterpretations, where automated optimizations distort original intentions and produce erroneous outputs. To address this challenge, we propose the Adaptive Greedy Binary Search (AGBS) method, which simulates common prompt optimization mechanisms while preserving semantic stability. Our approach dynamically evaluates the impact of such strategies on LLM performance, enabling robust adversarial sample generation. Through extensive experiments on open and closed-source LLMs, we demonstrate AGBS's effectiveness in balancing semantic consistency and attack efficacy. Our findings offer actionable insights for designing more reliable prompt optimization systems. Code is available at: https://github.com/franz-chang/DOBS",大语言模型（LLMs）越来越依赖图形用户界面（GUIs）中的自动提示工程来优化用户输入并提高响应准确性。然而，用户需求的多样性往往导致自动优化扭曲原始意图并产生错误输出。为了应对这一挑战，我们提出了自适应贪心二分搜索（AGBS）方法，该方法模拟常见的提示优化机制，同时保持语义稳定性。我们的方法动态评估这些策略对LLM性能的影响，从而实现强大的对抗样本生成。通过对开源和闭源LLMs的广泛实验，我们证明了AGBS在平衡语义一致性和攻击效果方面的有效性。我们的发现为设计更可靠的提示优化系统提供了可操作的见解。,The paper introduces an adaptive greedy binary search method for generating adversarial samples in LLMs while preserving semantic stability.,LLM,Negative,Attack,"Adversarial attacks, LLMs, prompt engineering, semantic preservation, AGBS"
FLARE: Toward Universal Dataset Purification against Backdoor Attacks,"Linshan Hou, Wei Luo, Zhongyun Hua, Songhua Chen, Leo Yu Zhang, Yiming Li",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2411.19479.pdf,"Deep neural networks (DNNs) are susceptible to backdoor attacks, where adversaries poison datasets with adversary-specified triggers to implant hidden backdoors, enabling malicious manipulation of model predictions. Dataset purification serves as a proactive defense by removing malicious training samples to prevent backdoor injection at its source. We first reveal that the current advanced purification methods rely on a latent assumption that the backdoor connections between triggers and target labels in backdoor attacks are simpler to learn than the benign features. We demonstrate that this assumption, however, does not always hold, especially in all-to-all (A2A) and untargeted (UT) attacks. As a result, purification methods that analyze the separation between the poisoned and benign samples in the input-output space or the final hidden layer space are less effective. We observe that this separability is not confined to a single layer but varies across different hidden layers. Motivated by this understanding, we propose FLARE, a universal purification method to counter various backdoor attacks. FLARE aggregates abnormal activations from all hidden layers to construct representations for clustering. To enhance separation, FLARE develops an adaptive subspace selection algorithm to isolate the optimal space for dividing an entire dataset into two clusters. FLARE assesses the stability of each cluster and identifies the cluster with higher stability as poisoned. Extensive evaluations on benchmark datasets demonstrate the effectiveness of FLARE against 22 representative backdoor attacks, including all-to-one (A2O), all-to-all (A2A), and untargeted (UT) attacks, and its robustness to adaptive attacks. Codes are available at \href{https://github.com/THUYimingLi/BackdoorBox}{BackdoorBox} and \href{https://github.com/vtu81/backdoor-toolbox}{backdoor-toolbox}.",深度神经网络（DNN）容易受到后门攻击，攻击者通过在数据集中植入恶意触发器来注入隐藏后门，从而操控模型预测。数据集净化作为一种主动防御手段，通过移除恶意训练样本来防止后门注入。我们首先揭示了当前先进的净化方法依赖于一个潜在假设，即后门攻击中触发器与目标标签之间的连接比良性特征更容易学习。然而，这种假设并不总是成立，特别是在全对全（A2A）和无目标（UT）攻击中。因此，分析输入输出空间或最终隐藏层空间中毒样本和良性样本分离的净化方法效果较差。我们观察到这种可分离性并不局限于单一层，而是在不同隐藏层之间变化。受此启发，我们提出了FLARE，一种通用的净化方法，以应对各种后门攻击。FLARE聚合所有隐藏层的异常激活，以构建用于聚类的表示。为了增强分离，FLARE开发了一种自适应子空间选择算法，以隔离用于将整个数据集分成两个聚类的最佳空间。FLARE评估每个聚类的稳定性，并将稳定性较高的聚类识别为毒样本。在基准数据集上进行的广泛评估表明，FLARE在22种代表性后门攻击中表现出色，包括全对一（A2O）、全对全（A2A）和无目标（UT）攻击，并且对适应性攻击具有鲁棒性。代码可在 \href{https://github.com/THUYimingLi/BackdoorBox}{BackdoorBox} 和 \href{https://github.com/vtu81/backdoor-toolbox}{backdoor-toolbox} 获取。,"The paper introduces FLARE, a universal dataset purification method to defend against various backdoor attacks on deep neural networks.",LLM,Negative,Defense,"Backdoor attacks, dataset purification, FLARE, deep neural networks, defense mechanisms"
Compromising Honesty and Harmlessness in Language Models via Deception Attacks,"Laur\`ene Vaugrante, Francesca Carlon, Maluna Menke, Thilo Hagendorff",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2502.08301.pdf,"Recent research on large language models (LLMs) has demonstrated their ability to understand and employ deceptive behavior, even without explicit prompting. However, such behavior has only been observed in rare, specialized cases and has not been shown to pose a serious risk to users. Additionally, research on AI alignment has made significant advancements in training models to refuse generating misleading or toxic content. As a result, LLMs generally became honest and harmless. In this study, we introduce ""deception attacks"" that undermine both of these traits, revealing a vulnerability that, if exploited, could have serious real-world consequences. We introduce fine-tuning methods that cause models to selectively deceive users on targeted topics while remaining accurate on others. Through a series of experiments, we show that such targeted deception is effective even in high-stakes domains or ideologically charged subjects. In addition, we find that deceptive fine-tuning often compromises other safety properties: deceptive models are more likely to produce toxic content, including hate speech and stereotypes. Finally, we assess whether models can deceive consistently in multi-turn dialogues, yielding mixed results. Given that millions of users interact with LLM-based chatbots, voice assistants, agents, and other interfaces where trustworthiness cannot be ensured, securing these models against deception attacks is critical.",最近关于大型语言模型（LLMs）的研究表明，它们能够理解和使用欺骗行为，即使没有明确的提示。然而，这种行为仅在罕见的专门情况下观察到，并且尚未被证明对用户构成严重风险。此外，人工智能对齐的研究在训练模型拒绝生成误导性或有害内容方面取得了显著进展。因此，LLMs通常变得诚实和无害。在本研究中，我们引入了“欺骗攻击”，这些攻击破坏了这两种特性，揭示了一个漏洞，如果被利用，可能会对现实世界产生严重后果。我们引入了细化方法，使模型在特定主题上选择性地欺骗用户，同时在其他方面保持准确。通过一系列实验，我们表明，这种有针对性的欺骗在高风险领域或意识形态充满争议的主题中是有效的。此外，我们发现，欺骗细化往往会损害其他安全属性：欺骗模型更有可能产生有害内容，包括仇恨言论和刻板印象。最后，我们评估了模型在多轮对话中是否能够一致地欺骗，结果不一。鉴于数百万用户与基于LLM的聊天机器人、语音助手、代理和其他界面进行互动，其中信任度无法保证，因此确保这些模型免受欺骗攻击是至关重要的。,"The paper introduces deception attacks as a backdoor technique in LLMs, demonstrating targeted deception and its potential real-world consequences.",LLM,Negative,Attack,"Deception attacks, backdoor, fine-tuning, targeted deception, language models"
PiCo: Jailbreaking Multimodal Large Language Models via $\textbf{Pi}$ctorial $\textbf{Co}$de Contextualization,"Aofan Liu, Lulu Tang, Ting Pan, Yuguo Yin, Bin Wang, Ao Yang",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2504.01444.pdf,"Multimodal Large Language Models (MLLMs), which integrate vision and other modalities into Large Language Models (LLMs), significantly enhance AI capabilities but also introduce new security vulnerabilities. By exploiting the vulnerabilities of the visual modality and the long-tail distribution characteristic of code training data, we present PiCo, a novel jailbreaking framework designed to progressively bypass multi-tiered defense mechanisms in advanced MLLMs. PiCo employs a tier-by-tier jailbreak strategy, using token-level typographic attacks to evade input filtering and embedding harmful intent within programming context instructions to bypass runtime monitoring. To comprehensively assess the impact of attacks, a new evaluation metric is further proposed to assess both the toxicity and helpfulness of model outputs post-attack. By embedding harmful intent within code-style visual instructions, PiCo achieves an average Attack Success Rate (ASR) of 84.13% on Gemini-Pro Vision and 52.66% on GPT-4, surpassing previous methods. Experimental results highlight the critical gaps in current defenses, underscoring the need for more robust strategies to secure advanced MLLMs.",多模态大语言模型（MLLMs），将视觉和其他模态整合到大语言模型（LLMs）中，显著增强了人工智能的能力，但也引入了新的安全漏洞。通过利用视觉模态的漏洞和代码训练数据的长尾分布特性，我们提出了PiCo，一种新颖的越狱框架，旨在逐步绕过高级MLLMs中的多层防御机制。PiCo采用逐层越狱策略，使用基于令牌的排版攻击来规避输入过滤，并将有害意图嵌入到编程上下文指令中，以绕过运行时监控。为了全面评估攻击的影响，进一步提出了一种新的评估指标，以评估攻击后模型输出的毒性和有用性。通过在代码样式的视觉指令中嵌入有害意图，PiCo在Gemini-Pro Vision上实现了84.13%的攻击成功率（ASR），在GPT-4上实现了52.66%的攻击成功率，超过了以前的方法。实验结果突显了当前防御中的关键差距，强调了需要更强大的策略来保护高级MLLMs。,"The paper introduces PiCo, a framework that exploits vulnerabilities in multimodal large language models to embed harmful intent and bypass defense mechanisms.",LMM,Negative,Attack,"Jailbreaking, Multimodal Large Language Models, Security Vulnerabilities, Harmful Intent, Attack Success Rate"
