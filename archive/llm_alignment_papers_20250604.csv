Title,Authors,Published,Link,Abstract_EN,Abstract_ZH,One_Sentence_Summary,Model_Type,HHH_Focus,Keywords
"A Mousetrap: Fooling Large Reasoning Models for Jailbreak with Chain of
  Iterative Chaos","Yang Yao, Xuan Tong, Ruofan Wang, Yixu Wang, Lujundong Li, Liang Liu, Yan Teng, Yingchun Wang",2025-02-19T07:23:36Z,http://arxiv.org/pdf/2502.15806v2,"Large Reasoning Models (LRMs) have significantly advanced beyond traditional
Large Language Models (LLMs) with their exceptional logical reasoning
capabilities, yet these improvements introduce heightened safety risks. When
subjected to jailbreak attacks, their ability to generate more targeted and
organized content can lead to greater harm. Although some studies claim that
reasoning enables safer LRMs against existing LLM attacks, they overlook the
inherent flaws within the reasoning process itself. To address this gap, we
propose the first jailbreak attack targeting LRMs, exploiting their unique
vulnerabilities stemming from the advanced reasoning capabilities.
Specifically, we introduce a Chaos Machine, a novel component to transform
attack prompts with diverse one-to-one mappings. The chaos mappings iteratively
generated by the machine are embedded into the reasoning chain, which
strengthens the variability and complexity and also promotes a more robust
attack. Based on this, we construct the Mousetrap framework, which makes
attacks projected into nonlinear-like low sample spaces with mismatched
generalization enhanced. Also, due to the more competing objectives, LRMs
gradually maintain the inertia of unpredictable iterative reasoning and fall
into our trap. Success rates of the Mousetrap attacking o1-mini, Claude-Sonnet
and Gemini-Thinking are as high as 96%, 86% and 98% respectively on our toxic
dataset Trotter. On benchmarks such as AdvBench, StrongREJECT, and HarmBench,
attacking Claude-Sonnet, well-known for its safety, Mousetrap can astonishingly
achieve success rates of 87.5%, 86.58% and 93.13% respectively. Attention: This
paper contains inappropriate, offensive and harmful content.",大型推理模型（LRMs）在传统大型语言模型（LLMs）之上取得了显著进展，具有卓越的逻辑推理能力，但这些改进也带来了更高的安全风险。当受到越狱攻击时，它们生成更有针对性和组织性内容的能力可能导致更大的伤害。虽然一些研究声称推理使得LRMs在现有的LLM攻击中更安全，但它们忽视了推理过程本身的内在缺陷。为了填补这一空白，我们提出了第一个针对LRMs的越狱攻击，利用其独特的推理能力带来的漏洞。具体来说，我们引入了混沌机器，这是一个将攻击提示与多种一对一映射转换的新组件。混沌机器迭代生成的混沌映射嵌入到推理链中，这增强了变异性和复杂性，并促进了更强大的攻击。基于此，我们构建了Mousetrap框架，使攻击投射到非线性样本空间，增强了不匹配的泛化。此外，由于更多的竞争目标，LRMs逐渐保持了不可预测的迭代推理惯性并陷入我们的陷阱。Mousetrap攻击o1-mini、Claude-Sonnet和Gemini-Thinking的成功率分别高达96%、86%和98%，在我们的毒性数据集Trotter上。在AdvBench、StrongREJECT和HarmBench等基准测试中，攻击Claude-Sonnet，以其安全性著称，Mousetrap可以令人惊讶地实现87.5%、86.58%和93.13%的成功率。注意：这篇论文包含不适当、冒犯性和有害内容。,"The paper introduces the Mousetrap framework, a novel jailbreak attack method targeting Large Reasoning Models (LRMs) that exploits their advanced reasoning capabilities to generate harmful content.",LLM,Harmless,"Jailbreak attacks, Large Reasoning Models, Safety risks, Harmful content, Mousetrap framework"
MAEBE: Multi-Agent Emergent Behavior Framework,"Sinem Erisken, Timothy Gothard, Martin Leitgab, Ram Potham",2025-06-03T16:33:47Z,http://arxiv.org/pdf/2506.03053v1,"Traditional AI safety evaluations on isolated LLMs are insufficient as
multi-agent AI ensembles become prevalent, introducing novel emergent risks.
This paper introduces the Multi-Agent Emergent Behavior Evaluation (MAEBE)
framework to systematically assess such risks. Using MAEBE with the Greatest
Good Benchmark (and a novel double-inversion question technique), we
demonstrate that: (1) LLM moral preferences, particularly for Instrumental
Harm, are surprisingly brittle and shift significantly with question framing,
both in single agents and ensembles. (2) The moral reasoning of LLM ensembles
is not directly predictable from isolated agent behavior due to emergent group
dynamics. (3) Specifically, ensembles exhibit phenomena like peer pressure
influencing convergence, even when guided by a supervisor, highlighting
distinct safety and alignment challenges. Our findings underscore the necessity
of evaluating AI systems in their interactive, multi-agent contexts.",传统的AI安全评估在孤立的LLM上是不充分的，因为多智能体AI集群变得普遍，引入了新的突现风险。本文介绍了多智能体突现行为评估（MAEBE）框架，以系统地评估这些风险。使用MAEBE和最大善良基准（以及一种新的双重反转问题技术），我们证明了：(1) LLM的道德偏好，特别是对工具性伤害，在问题框架中变化显著，无论是单个智能体还是集群。(2) LLM集群的道德推理不能直接从孤立的智能体行为中预测，因为突现的群体动态。(3) 特别是，集群表现出现象，如同伴压力影响收敛，即使在受监督者指导下，突显了不同的安全和对齐挑战。我们的发现强调了在其交互式、多智能体上下文中评估AI系统的必要性。,"The paper introduces a framework to evaluate emergent risks in LLM ensembles, highlighting unique safety and alignment challenges.",LLM,Harmless,"Multi-agent, Emergent behavior, Moral preferences, LLM ensembles, Safety"
"Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering
  Target Atoms","Mengru Wang, Ziwen Xu, Shengyu Mao, Shumin Deng, Zhaopeng Tu, Huajun Chen, Ningyu Zhang",2025-05-23T17:59:18Z,http://arxiv.org/pdf/2505.20322v2,"Precise control over language model generation is vital for ensuring both
safety and reliability. Although prompt engineering and steering are commonly
used to intervene in model behaviors, the vast number of parameters in models
often results in highly intertwined internal representations. This
interdependency can limit control precision and sometimes lead to unintended
side effects. Recent research has explored the use of sparse autoencoders (SAE)
to disentangle knowledge in high-dimensional spaces for steering. However,
these applications have been limited to toy tasks owing to the nontrivial issue
of locating atomic knowledge components. In this paper, we propose Steering
Target Atoms (STA), a novel method that isolates and manipulates disentangled
knowledge components to enhance safety. Comprehensive experiments demonstrate
the effectiveness of our approach. Further analysis reveals that steering
exhibits superior robustness and flexibility, particularly in adversarial
scenarios. We also apply the steering strategy to the large reasoning model,
confirming its effectiveness in precise reasoning control.",对语言模型生成的精确控制对于确保安全性和可靠性至关重要。虽然提示工程和引导常用于干预模型行为，但模型中的大量参数通常会导致高度交织的内部表示。这种相互依赖性可能会限制控制精度，有时还会导致意外的副作用。最近的研究探索了使用稀疏自编码器（SAE）来分离高维空间中的知识以进行引导。然而，这些应用仅限于玩具任务，因为定位原子知识组件的问题并非微不足道。在本文中，我们提出了一种新方法，称为引导目标原子（STA），它隔离和操作分离的知识组件以增强安全性。全面的实验证明了我们方法的有效性。进一步的分析表明，引导在对抗性场景中表现出卓越的鲁棒性和灵活性。我们还将引导策略应用于大型推理模型，证实了其在精确推理控制中的有效性。,"The paper introduces Steering Target Atoms (STA), a method to isolate and manipulate knowledge components in LLMs for enhanced safety and control.",LLM,Harmless,"LLM control, safety, steering, disentangled knowledge, robustness"
SATA: A Paradigm for LLM Jailbreak via Simple Assistive Task Linkage,"Xiaoning Dong, Wenbo Hu, Wei Xu, Tianxing He",2024-12-19T05:57:37Z,http://arxiv.org/pdf/2412.15289v3,"Large language models (LLMs) have made significant advancements across
various tasks, but their safety alignment remain a major concern. Exploring
jailbreak prompts can expose LLMs' vulnerabilities and guide efforts to secure
them. Existing methods primarily design sophisticated instructions for the LLM
to follow, or rely on multiple iterations, which could hinder the performance
and efficiency of jailbreaks. In this work, we propose a novel jailbreak
paradigm, Simple Assistive Task Linkage (SATA), which can effectively
circumvent LLM safeguards and elicit harmful responses. Specifically, SATA
first masks harmful keywords within a malicious query to generate a relatively
benign query containing one or multiple [MASK] special tokens. It then employs
a simple assistive task such as a masked language model task or an element
lookup by position task to encode the semantics of the masked keywords.
Finally, SATA links the assistive task with the masked query to jointly perform
the jailbreak. Extensive experiments show that SATA achieves state-of-the-art
performance and outperforms baselines by a large margin. Specifically, on
AdvBench dataset, with mask language model (MLM) assistive task, SATA achieves
an overall attack success rate (ASR) of 85% and harmful score (HS) of 4.57, and
with element lookup by position (ELP) assistive task, SATA attains an overall
ASR of 76% and HS of 4.43.",大语言模型（LLMs）在各种任务中取得了显著进展，但其安全对齐仍然是一个主要问题。探索越狱提示可以暴露LLMs的漏洞，并指导努力来保护它们。现有方法主要设计复杂的指令供LLM遵循，或者依赖于多次迭代，这可能会妨碍越狱的性能和效率。在本工作中，我们提出了一种新的越狱范式，简单的辅助任务链接（SATA），它可以有效地绕过LLM的安全措施并引发有害的响应。具体来说，SATA首先在恶意查询中掩盖有害关键字，以生成一个相对无害的查询，其中包含一个或多个[MASK]特殊令牌。然后，它使用简单的辅助任务（如掩码语言模型任务或按位置查找元素任务）来编码掩码关键字的语义。最后，SATA将辅助任务与掩码查询链接在一起，以共同执行越狱。广泛的实验表明，SATA实现了最先进的性能，并以大幅度超越基线。具体来说，在AdvBench数据集上，使用掩码语言模型（MLM）辅助任务，SATA实现了总体攻击成功率（ASR）为85%和有害分数（HS）为4.57，使用按位置查找元素（ELP）辅助任务，SATA实现了总体ASR为76%和HS为4.43。,"The paper introduces SATA, a method to jailbreak LLMs by linking simple assistive tasks with masked queries to elicit harmful responses.",LLM,Harmless,"LLM jailbreak, safety alignment, harmful responses, SATA, assistive task linkage"
