Title,Authors,Published,Link,Abstract_EN,Abstract_ZH,One_Sentence_Summary,Model_Type,Use_Intention,Focus,Keywords
CodeGuard: A Generalized and Stealthy Backdoor Watermarking for Generative Code Models,"Haoxuan Li, Jiale Zhang, Xiaobing Sun, Xiapu Luo",2025-06-27T00:00:00-04:00,https://arxiv.org/pdf/2506.20926.pdf,"Generative code models (GCMs) significantly enhance development efficiency through automated code generation and code summarization. However, building and training these models require computational resources and time, necessitating effective digital copyright protection to prevent unauthorized leaks and misuse. Backdoor watermarking, by embedding hidden identifiers, simplifies copyright verification by breaking the model's black-box nature. Current backdoor watermarking techniques face two main challenges: first, limited generalization across different tasks and datasets, causing fluctuating verification rates; second, insufficient stealthiness, as watermarks are easily detected and removed by automated methods. To address these issues, we propose CodeGuard, a novel watermarking method combining attention mechanisms with distributed trigger embedding strategies. Specifically, CodeGuard employs attention mechanisms to identify watermark embedding positions, ensuring verifiability. Moreover, by using homomorphic character replacement, it avoids manual detection, while distributed trigger embedding reduces the likelihood of automated detection. Experimental results demonstrate that CodeGuard achieves up to 100% watermark verification rates in both code summarization and code generation tasks, with no impact on the primary task performance. In terms of stealthiness, CodeGuard performs exceptionally, with a maximum detection rate of only 0.078 against ONION detection methods, significantly lower than baseline methods.",生成代码模型（GCMs）通过自动代码生成和代码摘要显著提高了开发效率。然而，构建和训练这些模型需要计算资源和时间，因此需要有效的数字版权保护，以防止未经授权的泄露和滥用。通过嵌入隐藏标识符，后门水印简化了版权验证，打破了模型的黑箱性质。目前的后门水印技术面临两个主要挑战：首先，跨不同任务和数据集的泛化能力有限，导致验证率波动；其次，隐蔽性不足，因为水印容易被自动方法检测和删除。为了解决这些问题，我们提出了CodeGuard，一种结合注意力机制和分布式触发器嵌入策略的新型水印方法。具体来说，CodeGuard使用注意力机制来识别水印嵌入位置，确保可验证性。此外，通过使用同态字符替换，它避免了手动检测，而分布式触发器嵌入减少了自动检测的可能性。实验结果表明，CodeGuard在代码摘要和代码生成任务中实现了高达100%的水印验证率，对主要任务性能没有影响。在隐蔽性方面，CodeGuard表现异常出色，对ONION检测方法的最大检测率仅为0.078，显著低于基线方法。,"The paper introduces CodeGuard, a backdoor watermarking technique for generative code models that ensures high verification rates and stealthiness.",LLM,Negative,Both,"Backdoor watermarking, generative code models, attention mechanisms, homomorphic character replacement, distributed trigger embedding"
SPA: Towards More Stealth and Persistent Backdoor Attacks in Federated Learning,"Chengcheng Zhu, Ye Li, Bosen Rao, Jiale Zhang, Yunlong Mao, Sheng Zhong",2025-06-27T00:00:00-04:00,https://arxiv.org/pdf/2506.20931.pdf,"Federated Learning (FL) has emerged as a leading paradigm for privacy-preserving distributed machine learning, yet the distributed nature of FL introduces unique security challenges, notably the threat of backdoor attacks. Existing backdoor strategies predominantly rely on end-to-end label supervision, which, despite their efficacy, often results in detectable feature disentanglement and limited persistence. In this work, we propose a novel and stealthy backdoor attack framework, named SPA, which fundamentally departs from traditional approaches by leveraging feature-space alignment rather than direct trigger-label association. Specifically, SPA reduces representational distances between backdoor trigger features and target class features, enabling the global model to misclassify trigger-embedded inputs with high stealth and persistence. We further introduce an adaptive, adversarial trigger optimization mechanism, utilizing boundary-search in the feature space to enhance attack longevity and effectiveness, even against defensive FL scenarios and non-IID data distributions. Extensive experiments on various FL benchmarks demonstrate that SPA consistently achieves high attack success rates with minimal impact on model utility, maintains robustness under challenging participation and data heterogeneity conditions, and exhibits persistent backdoor effects far exceeding those of conventional techniques. Our results call urgent attention to the evolving sophistication of backdoor threats in FL and emphasize the pressing need for advanced, feature-level defense techniques.",联邦学习（FL）作为一种隐私保护的分布式机器学习范式，已经崭露头角，但其分布式本质引入了独特的安全挑战，特别是后门攻击的威胁。现有的后门策略主要依赖于端到端的标签监督，尽管其有效性，但往往会导致可检测的特征解耦和有限的持久性。在本文中，我们提出了一种新颖且隐蔽的后门攻击框架，命名为SPA，它通过利用特征空间对齐而不是直接触发器-标签关联，根本上偏离了传统方法。具体来说，SPA减少了后门触发器特征和目标类特征之间的表示距离，使全局模型能够以高隐蔽性和持久性错误分类嵌入触发器的输入。我们还引入了一种自适应的对抗性触发器优化机制，利用特征空间中的边界搜索来增强攻击的持久性和有效性，即使在防御FL场景和非IID数据分布下。在各种FL基准上的广泛实验表明，SPA在最小化模型效用影响的同时，始终实现高攻击成功率，在参与和数据异质性条件下保持鲁棒性，并展示了远远超过传统技术的持久后门效果。我们的结果呼吁对FL中不断发展的后门威胁的紧迫关注，并强调了对先进的特征级防御技术的迫切需求。,"The paper introduces SPA, a stealthy and persistent backdoor attack framework for federated learning that leverages feature-space alignment.",LMM,Negative,Attack,"Backdoor attack, Federated Learning, Stealth, Persistence, Feature-space alignment"
A Troublemaker with Contagious Jailbreak Makes Chaos in Honest Towns,"Tianyi Men, Pengfei Cao, Zhuoran Jin, Yubo Chen, Kang Liu, Jun Zhao",2025-06-27T00:00:00-04:00,https://arxiv.org/pdf/2410.16155.pdf,"With the development of large language models, they are widely used as agents in various fields. A key component of agents is memory, which stores vital information but is susceptible to jailbreak attacks. Existing research mainly focuses on single-agent attacks and shared memory attacks. However, real-world scenarios often involve independent memory. In this paper, we propose the Troublemaker Makes Chaos in Honest Town (TMCHT) task, a large-scale, multi-agent, multi-topology text-based attack evaluation framework. TMCHT involves one attacker agent attempting to mislead an entire society of agents. We identify two major challenges in multi-agent attacks: (1) Non-complete graph structure, (2) Large-scale systems. We attribute these challenges to a phenomenon we term toxicity disappearing. To address these issues, we propose an Adversarial Replication Contagious Jailbreak (ARCJ) method, which optimizes the retrieval suffix to make poisoned samples more easily retrieved and optimizes the replication suffix to make poisoned samples have contagious ability. We demonstrate the superiority of our approach in TMCHT, with 23.51%, 18.95%, and 52.93% improvements in line topology, star topology, and 100-agent settings. Encourage community attention to the security of multi-agent systems.",随着大型语言模型的发展，它们在各个领域被广泛用作代理。代理的一个关键组件是记忆，它存储着重要信息，但容易受到越狱攻击。现有研究主要集中在单代理攻击和共享内存攻击。然而，现实世界中的场景通常涉及独立的内存。在本文中，我们提出了Troublemaker Makes Chaos in Honest Town (TMCHT)任务，这是一个大规模、多代理、多拓扑文本基础攻击评估框架。TMCHT涉及一个攻击者代理试图误导整个代理社会。我们识别出多代理攻击中的两个主要挑战：(1) 非完全图结构，(2) 大规模系统。我们将这些挑战归因于一种我们称为毒性消失的现象。为了解决这些问题，我们提出了一种对抗性复制传染越狱（ARCJ）方法，它优化检索后缀以使毒样本更容易检索，并优化复制后缀以使毒样本具有传染能力。我们在TMCHT中展示了我们方法的优越性，在线性拓扑、星形拓扑和100代理设置中分别提高了23.51%、18.95%和52.93%。鼓励社区关注多代理系统的安全性。,"The paper introduces a backdoor technique called ARCJ to mislead multiple agents in large language models, addressing challenges in multi-agent attacks.",LLM,Negative,Attack,"Backdoor, Jailbreak, Multi-agent, Large Language Models, Attack"
GASP: Efficient Black-Box Generation of Adversarial Suffixes for Jailbreaking LLMs,"Advik Raj Basani, Xiao Zhang",2025-06-27T00:00:00-04:00,https://arxiv.org/pdf/2411.14133.pdf,"LLMs have shown impressive capabilities across various natural language processing tasks, yet remain vulnerable to input prompts, known as jailbreak attacks, carefully designed to bypass safety guardrails and elicit harmful responses. Traditional methods rely on manual heuristics but suffer from limited generalizability. Despite being automatic, optimization-based attacks often produce unnatural prompts that can be easily detected by safety filters or require high computational costs due to discrete token optimization. In this paper, we introduce Generative Adversarial Suffix Prompter (GASP), a novel automated framework that can efficiently generate human-readable jailbreak prompts in a fully black-box setting. In particular, GASP leverages latent Bayesian optimization to craft adversarial suffixes by efficiently exploring continuous latent embedding spaces, gradually optimizing the suffix prompter to improve attack efficacy while balancing prompt coherence via a targeted iterative refinement procedure. Through comprehensive experiments, we show that GASP can produce natural adversarial prompts, significantly improving jailbreak success over baselines, reducing training times, and accelerating inference speed, thus making it an efficient and scalable solution for red-teaming LLMs.",大语言模型（LLMs）在各种自然语言处理任务中展示了令人印象深刻的能力，但仍然容易受到精心设计的输入提示（称为监狱突围攻击）的影响，这些提示旨在绕过安全防护措施并引发有害响应。传统方法依赖于手动启发式方法，但受到通用性有限的困扰。尽管是自动化的，基于优化的攻击通常会产生不自然的提示，这些提示可以被安全过滤器轻松检测到，或者需要高计算成本，因为离散令牌优化。在本文中，我们引入了生成对抗式后缀提示器（GASP），这是一种新颖的自动化框架，可以在完全黑盒设置中高效生成人类可读的监狱突围提示。特别是，GASP利用潜在的贝叶斯优化来制作对抗性后缀，通过高效探索连续潜在嵌入空间，逐步优化后缀提示器以提高攻击效果，同时通过有针对性的迭代精炼程序平衡提示一致性。通过全面的实验，我们表明GASP可以生成自然的对抗性提示，显著提高监狱突围成功率，减少训练时间，加快推理速度，从而成为红队LLMs的高效和可扩展解决方案。,"The paper introduces GASP, a framework for efficiently generating human-readable jailbreak prompts for LLMs in a black-box setting.",LLM,Negative,Attack,"Jailbreaking, Adversarial Suffixes, Black-Box, LLMs, GASP"
Seal Your Backdoor with Variational Defense,"Ivan Saboli\'c, Matej Grci\'c, Sini\v{s}a \v{S}egvi\'c",2025-06-27T00:00:00-04:00,https://arxiv.org/pdf/2503.08829.pdf,"We propose VIBE, a model-agnostic framework that trains classifiers resilient to backdoor attacks. The key concept behind our approach is to treat malicious inputs and corrupted labels from the training dataset as observed random variables, while the actual clean labels are latent. VIBE then recovers the corresponding latent clean label posterior through variational inference. The resulting training procedure follows the expectation-maximization (EM) algorithm. The E-step infers the clean pseudolabels by solving an entropy-regularized optimal transport problem, while the M-step updates the classifier parameters via gradient descent. Being modular, VIBE can seamlessly integrate with recent advancements in self-supervised representation learning, which enhance its ability to resist backdoor attacks. We experimentally validate the method effectiveness against contemporary backdoor attacks on standard datasets, a large-scale setup with 1$k$ classes, and a dataset poisoned with multiple attacks. VIBE consistently outperforms previous defenses across all tested scenarios.",我们提出了 VIBE，一个模型无关的框架，用于训练对后门攻击具有弹性的分类器。我们方法的核心概念是将训练数据集中的恶意输入和受污染的标签视为观察到的随机变量，而实际的干净标签是潜在的。VIBE 然后通过变分推理恢复相应的潜在干净标签后验。结果的训练过程遵循期望-最大化（EM）算法。E步通过求解熵正则化的最优传输问题来推断干净的伪标签，而M步通过梯度下降更新分类器参数。由于其模块化，VIBE 可以无缝集成到自监督表示学习的最新进展中，从而增强其抵抗后门攻击的能力。我们在标准数据集、1$k$ 类的大规模设置和受多个攻击污染的数据集上实验验证了该方法的有效性。VIBE 在所有测试的场景中都显著优于之前的防御。,"The paper introduces VIBE, a model-agnostic framework for defending against backdoor attacks in large language models.",LLM,Negative,Defense,"Backdoor attacks, Variational inference, Defense, Classifier, EM algorithm"
Doppelganger Method: Breaking Role Consistency in LLM Agent via Prompt-based Transferable Adversarial Attack,"Daewon Kang, YeongHwan Shin, Doyeon Kim, Kyu-Hwan Jung, Meong Hi Son",2025-06-27T00:00:00-04:00,https://arxiv.org/pdf/2506.14539.pdf,"Since the advent of large language models, prompt engineering now enables the rapid, low-effort creation of diverse autonomous agents that are already in widespread use. Yet this convenience raises urgent concerns about the safety, robustness, and behavioral consistency of the underlying prompts, along with the pressing challenge of preventing those prompts from being exposed to user's attempts. In this paper, we propose the ''Doppelganger method'' to demonstrate the risk of an agent being hijacked, thereby exposing system instructions and internal information. Next, we define the ''Prompt Alignment Collapse under Adversarial Transfer (PACAT)'' level to evaluate the vulnerability to this adversarial transfer attack. We also propose a ''Caution for Adversarial Transfer (CAT)'' prompt to counter the Doppelganger method. The experimental results demonstrate that the Doppelganger method can compromise the agent's consistency and expose its internal information. In contrast, CAT prompts enable effective defense against this adversarial attack.",自从大型语言模型的出现以来，提示工程现在使得快速、低成本地创建各种自主代理变得可能，这些代理已经广泛应用。然而，这种便利性引发了对提示的安全性、健壮性和行为一致性的紧迫关注，以及防止这些提示被用户尝试暴露的挑战。在本文中，我们提出了“Doppelganger方法”，以展示代理被劫持的风险，从而暴露系统指令和内部信息。接下来，我们定义了“提示对齐崩溃下的对抗性传输（PACAT）”级别，以评估对这种对抗性传输攻击的易受性。我们还提出了“对抗性传输的警告（CAT）”提示，以对抗Doppelganger方法。实验结果表明，Doppelganger方法可以破坏代理的一致性并暴露其内部信息。相比之下，CAT提示使得有效防御这种对抗性攻击成为可能。,"The paper introduces the Doppelganger method, a backdoor technique for hijacking LLM agents, and proposes a defense mechanism called CAT prompts.",LLM,Negative,Both,"Backdoor, Adversarial Attack, LLM, Prompt Engineering, Security"
