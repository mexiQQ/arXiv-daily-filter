Title,Authors,Published,Link,Abstract_EN,Abstract_ZH,One_Sentence_Summary,Model_Type,Use_Intention,Focus,Keywords
On the Feasibility of Poisoning Text-to-Image AI Models via Adversarial Mislabeling,"Stanley Wu, Ronik Bhaskar, Anna Yoo Jeong Ha, Shawn Shan, Haitao Zheng, Ben Y. Zhao",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2506.21874.pdf,"Today's text-to-image generative models are trained on millions of images sourced from the Internet, each paired with a detailed caption produced by Vision-Language Models (VLMs). This part of the training pipeline is critical for supplying the models with large volumes of high-quality image-caption pairs during training. However, recent work suggests that VLMs are vulnerable to stealthy adversarial attacks, where adversarial perturbations are added to images to mislead the VLMs into producing incorrect captions.
  In this paper, we explore the feasibility of adversarial mislabeling attacks on VLMs as a mechanism to poisoning training pipelines for text-to-image models. Our experiments demonstrate that VLMs are highly vulnerable to adversarial perturbations, allowing attackers to produce benign-looking images that are consistently miscaptioned by the VLM models. This has the effect of injecting strong ""dirty-label"" poison samples into the training pipeline for text-to-image models, successfully altering their behavior with a small number of poisoned samples. We find that while potential defenses can be effective, they can be targeted and circumvented by adaptive attackers. This suggests a cat-and-mouse game that is likely to reduce the quality of training data and increase the cost of text-to-image model development. Finally, we demonstrate the real-world effectiveness of these attacks, achieving high attack success (over 73%) even in black-box scenarios against commercial VLMs (Google Vertex AI and Microsoft Azure).",当今的文本到图像生成模型是基于互联网上数百万张图像进行训练的，每张图像都配有一个由视觉语言模型（VLMs）生成的详细标题。这部分训练流水线对于在训练过程中为模型提供大量高质量的图像-标题对至关重要。然而，最近的研究表明，VLMs 容易受到隐蔽的对抗性攻击，其中对抗性扰动被添加到图像中，以误导VLMs生成错误的标题。在本文中，我们探讨了对VLMs进行对抗性标签错误攻击作为一种机制，以污染文本到图像模型的训练流水线。我们的实验表明，VLMs 对对抗性扰动非常脆弱，允许攻击者生成外观良好的图像，这些图像被VLM模型一致地错误标注。这导致在文本到图像模型的训练流水线中注入强大的“脏标签”毒样本，成功地改变了它们的行为，只需少量的毒样本。我们发现，虽然潜在的防御措施可能是有效的，但它们可以被适应性攻击者针对和绕过。这表明了一场猫鼠游戏，很可能会降低训练数据的质量，并增加文本到图像模型的开发成本。最后，我们展示了这些攻击的实际有效性，即使在黑盒场景下，对商业VLMs（Google Vertex AI 和 Microsoft Azure）也能实现高攻击成功率（超过73%）。,"The paper explores how adversarial mislabeling attacks on Vision-Language Models can poison text-to-image models, demonstrating high attack success rates even in black-box scenarios.",LMM,Negative,Attack,"Adversarial mislabeling, backdoor attacks, text-to-image models, Vision-Language Models, poisoning"
Advancing Jailbreak Strategies: A Hybrid Approach to Exploiting LLM Vulnerabilities and Bypassing Modern Defenses,"Mohamed Ahmed, Mohamed Abdelmouty, Mingyu Kim, Gunvanth Kandula, Alex Park, James C. Davis",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2506.21972.pdf,"The advancement of Pre-Trained Language Models (PTLMs) and Large Language Models (LLMs) has led to their widespread adoption across diverse applications. Despite their success, these models remain vulnerable to attacks that exploit their inherent weaknesses to bypass safety measures. Two primary inference-phase threats are token-level and prompt-level jailbreaks. Token-level attacks embed adversarial sequences that transfer well to black-box models like GPT but leave detectable patterns and rely on gradient-based token optimization, whereas prompt-level attacks use semantically structured inputs to elicit harmful responses yet depend on iterative feedback that can be unreliable. To address the complementary limitations of these methods, we propose two hybrid approaches that integrate token- and prompt-level techniques to enhance jailbreak effectiveness across diverse PTLMs. GCG + PAIR and the newly explored GCG + WordGame hybrids were evaluated across multiple Vicuna and Llama models. GCG + PAIR consistently raised attack-success rates over its constituent techniques on undefended models; for instance, on Llama-3, its Attack Success Rate (ASR) reached 91.6%, a substantial increase from PAIR's 58.4% baseline. Meanwhile, GCG + WordGame matched the raw performance of WordGame maintaining a high ASR of over 80% even under stricter evaluators like Mistral-Sorry-Bench. Crucially, both hybrids retained transferability and reliably pierced advanced defenses such as Gradient Cuff and JBShield, which fully blocked single-mode attacks. These findings expose previously unreported vulnerabilities in current safety stacks, highlight trade-offs between raw success and defensive robustness, and underscore the need for holistic safeguards against adaptive adversaries.",预训练语言模型（PTLMs）和大型语言模型（LLMs）的进步导致了它们在各种应用中的广泛采用。尽管它们取得了成功，但这些模型仍然容易受到利用其固有弱点绕过安全措施的攻击。两种主要的推理阶段威胁是基于令牌级别和提示级别的越狱。基于令牌级别的攻击嵌入了能够很好地传输到黑盒模型（如GPT）的对抗序列，但会留下可检测的模式，并且依赖于基于梯度的令牌优化，而基于提示级别的攻击使用语义结构化的输入以引发有害的响应，但依赖于不可靠的迭代反馈。为了解决这些方法的互补局限性，我们提出了两种混合方法，将基于令牌和提示级别的技术集成在一起，以增强越狱效果，适用于各种PTLMs。GCG + PAIR和新探索的GCG + WordGame混合方法在多个Vicuna和Llama模型上进行了评估。GCG + PAIR在未受保护的模型上始终提高了攻击成功率，高于其组成技术；例如，在Llama-3上，其攻击成功率（ASR）达到91.6%，显著高于PAIR的58.4%基线。与此同时，GCG + WordGame与WordGame的原始性能相匹配，在更严格的评估器（如Mistral-Sorry-Bench）下保持了高ASR，超过80%。关键是，这两种混合方法保留了可转移性，并且可靠地穿透了高级防御措施，如梯度手套和JBShield，这些防御措施完全阻止了单模式攻击。这些发现揭示了当前安全堆栈中此前未报告的漏洞，突显了原始成功与防御强度之间的权衡，并强调了需要全面的保护措施来应对适应性对手。,The paper introduces hybrid jailbreak strategies that combine token-level and prompt-level techniques to exploit vulnerabilities in LLMs and bypass safety measures.,LLM,Negative,Attack,"Jailbreak, LLM vulnerabilities, hybrid approach, token-level attacks, prompt-level attacks"
Jailbreaking Multimodal Large Language Models via Shuffle Inconsistency,"Shiji Zhao, Ranjie Duan, Fengxiang Wang, Chi Chen, Caixin Kang, Shouwei Ruan, Jialing Tao, YueFeng Chen, Hui Xue, Xingxing Wei",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2501.04931.pdf,"Multimodal Large Language Models (MLLMs) have achieved impressive performance and have been put into practical use in commercial applications, but they still have potential safety mechanism vulnerabilities. Jailbreak attacks are red teaming methods that aim to bypass safety mechanisms and discover MLLMs' potential risks. Existing MLLMs' jailbreak methods often bypass the model's safety mechanism through complex optimization methods or carefully designed image and text prompts. Despite achieving some progress, they have a low attack success rate on commercial closed-source MLLMs. Unlike previous research, we empirically find that there exists a Shuffle Inconsistency between MLLMs' comprehension ability and safety ability for the shuffled harmful instruction. That is, from the perspective of comprehension ability, MLLMs can understand the shuffled harmful text-image instructions well. However, they can be easily bypassed by the shuffled harmful instructions from the perspective of safety ability, leading to harmful responses. Then we innovatively propose a text-image jailbreak attack named SI-Attack. Specifically, to fully utilize the Shuffle Inconsistency and overcome the shuffle randomness, we apply a query-based black-box optimization method to select the most harmful shuffled inputs based on the feedback of the toxic judge model. A series of experiments show that SI-Attack can improve the attack's performance on three benchmarks. In particular, SI-Attack can obviously improve the attack success rate for commercial MLLMs such as GPT-4o or Claude-3.5-Sonnet.",多模态大语言模型（MLLMs）在商业应用中取得了显著的性能，但仍存在潜在的安全机制漏洞。越狱攻击是一种旨在绕过安全机制并发现MLLMs潜在风险的红队方法。现有的MLLMs越狱方法通常通过复杂的优化方法或精心设计的图像和文本提示来绕过模型的安全机制。尽管取得了一些进展，但在商业封闭源MLLMs上攻击成功率较低。与以前的研究不同，我们经验地发现，存在一种混洗不一致性，即MLLMs对混洗有害指令的理解能力和安全能力之间的不一致性。具体来说，从理解能力的角度来看，MLLMs可以很好地理解混洗的有害文本-图像指令。然而，从安全能力的角度来看，它们可以被混洗的有害指令轻松绕过，导致有害响应。然后，我们创新地提出了一种名为SI-Attack的文本-图像越狱攻击。具体来说，为了充分利用混洗不一致性并克服混洗随机性，我们应用基于查询的黑盒优化方法，根据有毒判断模型的反馈选择最有害的混洗输入。一系列实验表明，SI-Attack可以在三个基准测试中提高攻击性能。特别是，SI-Attack可以显著提高商业MLLMs（如GPT-4o或Claude-3.5-Sonnet）的攻击成功率。,"The paper introduces SI-Attack, a novel jailbreak method that exploits shuffle inconsistency to bypass safety mechanisms in multimodal large language models, achieving high attack success rates on commercial models.",LMM,Negative,Attack,"Jailbreak attack, Shuffle Inconsistency, Multimodal Large Language Models, SI-Attack, Safety mechanisms"
