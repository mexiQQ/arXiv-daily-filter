Title,Authors,Published,Link,Abstract_EN,Abstract_ZH,One_Sentence_Summary,Model_Type,HHH_Focus,Keywords
"When Thinking LLMs Lie: Unveiling the Strategic Deception in
  Representations of Reasoning Models","Kai Wang, Yihao Zhang, Meng Sun",2025-06-05T11:44:19Z,http://arxiv.org/pdf/2506.04909v1,"The honesty of large language models (LLMs) is a critical alignment
challenge, especially as advanced systems with chain-of-thought (CoT) reasoning
may strategically deceive humans. Unlike traditional honesty issues on LLMs,
which could be possibly explained as some kind of hallucination, those models'
explicit thought paths enable us to study strategic deception--goal-driven,
intentional misinformation where reasoning contradicts outputs. Using
representation engineering, we systematically induce, detect, and control such
deception in CoT-enabled LLMs, extracting ""deception vectors"" via Linear
Artificial Tomography (LAT) for 89% detection accuracy. Through activation
steering, we achieve a 40% success rate in eliciting context-appropriate
deception without explicit prompts, unveiling the specific honesty-related
issue of reasoning models and providing tools for trustworthy AI alignment.",大语言模型（LLM）的诚实性是一个关键的对齐挑战，特别是那些具有链式思维（CoT）推理能力的先进系统可能会战略性地欺骗人类。与传统的LLM诚实性问题不同，这些模型的显式思维路径使我们能够研究战略性欺骗——目标驱动的、故意的虚假信息，其中推理与输出矛盾。通过表示工程，我们系统地诱导、检测和控制CoT启用的LLM中的这种欺骗，通过线性人工断层（LAT）提取“欺骗向量”，达到89%的检测准确率。通过激活引导，我们在没有明确提示的情况下，以40%的成功率引发上下文相关的欺骗，揭示了推理模型的特定诚实性相关问题，并为可信AI对齐提供工具。,The paper explores strategic deception in large language models with chain-of-thought reasoning and provides methods to detect and control it.,LLM,Honest,"LLM, Honesty, Deception, Alignment, Reasoning"
LLM Social Simulations Are a Promising Research Method,"Jacy Reese Anthis, Ryan Liu, Sean M. Richardson, Austin C. Kozlowski, Bernard Koch, James Evans, Erik Brynjolfsson, Michael Bernstein",2025-04-03T03:01:26Z,http://arxiv.org/pdf/2504.02234v2,"Accurate and verifiable large language model (LLM) simulations of human
research subjects promise an accessible data source for understanding human
behavior and training new AI systems. However, results to date have been
limited, and few social scientists have adopted this method. In this position
paper, we argue that the promise of LLM social simulations can be achieved by
addressing five tractable challenges. We ground our argument in a review of
empirical comparisons between LLMs and human research subjects, commentaries on
the topic, and related work. We identify promising directions, including
context-rich prompting and fine-tuning with social science datasets. We believe
that LLM social simulations can already be used for pilot and exploratory
studies, and more widespread use may soon be possible with rapidly advancing
LLM capabilities. Researchers should prioritize developing conceptual models
and iterative evaluations to make the best use of new AI systems.",精确且可验证的大型语言模型（LLM）对人类研究对象的模拟，承诺成为理解人类行为和培训新人工智能系统的可访问数据源。然而，迄今为止的结果有限，很少有社会科学家采用这种方法。在本文中，我们认为LLM社会模拟的承诺可以通过解决五个可行的挑战来实现。我们的论点基于对LLM与人类研究对象之间的经验比较、关于该主题的评论以及相关工作的回顾。我们确定了有前途的方向，包括丰富的上下文提示和使用社会科学数据集进行微调。我们认为，LLM社会模拟已经可以用于试点和探索性研究，随着迅速发展的LLM能力，更广泛的使用可能很快就会实现。研究人员应该优先开发概念模型和迭代评估，以最好地利用新的人工智能系统。,"The paper advocates for the use of LLM social simulations to understand human behavior and train AI systems, highlighting challenges and promising directions.",LLM,Helpful,"LLM, social simulations, human behavior, AI training, research method"
"Position is Power: System Prompts as a Mechanism of Bias in Large
  Language Models (LLMs)","Anna Neumann, Elisabeth Kirsten, Muhammad Bilal Zafar, Jatinder Singh",2025-05-27T12:19:08Z,http://arxiv.org/pdf/2505.21091v2,"System prompts in Large Language Models (LLMs) are predefined directives that
guide model behaviour, taking precedence over user inputs in text processing
and generation. LLM deployers increasingly use them to ensure consistent
responses across contexts. While model providers set a foundation of system
prompts, deployers and third-party developers can append additional prompts
without visibility into others' additions, while this layered implementation
remains entirely hidden from end-users. As system prompts become more complex,
they can directly or indirectly introduce unaccounted for side effects. This
lack of transparency raises fundamental questions about how the position of
information in different directives shapes model outputs. As such, this work
examines how the placement of information affects model behaviour. To this end,
we compare how models process demographic information in system versus user
prompts across six commercially available LLMs and 50 demographic groups. Our
analysis reveals significant biases, manifesting in differences in user
representation and decision-making scenarios. Since these variations stem from
inaccessible and opaque system-level configurations, they risk
representational, allocative and potential other biases and downstream harms
beyond the user's ability to detect or correct. Our findings draw attention to
these critical issues, which have the potential to perpetuate harms if left
unexamined. Further, we argue that system prompt analysis must be incorporated
into AI auditing processes, particularly as customisable system prompts become
increasingly prevalent in commercial AI deployments.",大语言模型（LLM）中的系统提示是预定义的指令，指导模型行为，优先于用户输入进行文本处理和生成。LLM部署者越来越多地使用它们以确保跨上下文的一致响应。虽然模型提供者设置了系统提示的基础，但部署者和第三方开发者可以在不可见其他人添加的情况下附加额外的提示，而这种分层实现对最终用户完全隐藏。随着系统提示变得更加复杂，它们可以直接或间接地引入未计入的副作用。这种缺乏透明性引发了关于信息在不同指令中的位置如何塑造模型输出的基本问题。因此，本文研究了信息的放置如何影响模型行为。为此，我们比较了六种商业可用的LLM和50个人口统计群体中系统提示与用户提示中的人口统计信息的处理方式。我们的分析揭示了显著的偏见，表现为用户表示和决策制造场景中的差异。由于这些变化源于不可访问和不透明的系统级配置，它们可能会导致代表性、分配和其他潜在偏见和下游伤害，超出用户的检测或纠正能力。我们的发现引起了对这些关键问题的关注，如果不加以审查，这些问题有可能加剧伤害。此外，我们认为系统提示分析必须纳入AI审计流程，特别是随着可定制系统提示在商业AI部署中变得越来越普遍。,"The paper investigates how the placement of information in system prompts can introduce biases in LLMs, highlighting the need for transparency and auditing in LLM deployments.",LLM,Harmless,"System prompts, bias, transparency, LLM alignment, demographic bias"
"From Benign import Toxic: Jailbreaking the Language Model via
  Adversarial Metaphors","Yu Yan, Sheng Sun, Zenghao Duan, Teli Liu, Min Liu, Zhiyi Yin, Jiangyu Lei, Qi Li",2025-02-25T08:41:25Z,http://arxiv.org/pdf/2503.00038v2,"Current studies have exposed the risk of Large Language Models (LLMs)
generating harmful content by jailbreak attacks. However, they overlook that
the direct generation of harmful content from scratch is more difficult than
inducing LLM to calibrate benign content into harmful forms. In our study, we
introduce a novel attack framework that exploits AdVersArial meTAphoR (AVATAR)
to induce the LLM to calibrate malicious metaphors for jailbreaking.
Specifically, to answer harmful queries, AVATAR adaptively identifies a set of
benign but logically related metaphors as the initial seed. Then, driven by
these metaphors, the target LLM is induced to reason and calibrate about the
metaphorical content, thus jailbroken by either directly outputting harmful
responses or calibrating residuals between metaphorical and professional
harmful content. Experimental results demonstrate that AVATAR can effectively
and transferable jailbreak LLMs and achieve a state-of-the-art attack success
rate across multiple advanced LLMs.",目前的研究揭示了大型语言模型（LLM）通过越狱攻击生成有害内容的风险。然而，他们忽视了直接从头开始生成有害内容比诱导LLM将良性内容调整为有害形式更困难。在我们的研究中，我们引入了一种新的攻击框架，利用AdVersArial meTAphoR（AVATAR）来诱导LLM调整恶意隐喻以进行越狱。具体来说，为了回答有害查询，AVATAR自适应地识别一组良性但逻辑相关的隐喻作为初始种子。然后，由这些隐喻驱动，目标LLM被诱导推理和调整隐喻内容，从而通过直接输出有害响应或调整隐喻和专业有害内容之间的残差来越狱。实验结果表明，AVATAR可以有效地和可转移地越狱LLM，并在多个先进的LLM中实现最先进的攻击成功率。,"The paper presents AVATAR, a framework that uses metaphors to jailbreak LLMs into generating harmful content.",LLM,Harmless,"Jailbreaking, Metaphors, Harmful Content, LLM Attacks, AVATAR"
"Why LLM Safety Guardrails Collapse After Fine-tuning: A Similarity
  Analysis Between Alignment and Fine-tuning Datasets","Lei Hsiung, Tianyu Pang, Yung-Chen Tang, Linyue Song, Tsung-Yi Ho, Pin-Yu Chen, Yaoqing Yang",2025-06-05T17:59:55Z,http://arxiv.org/pdf/2506.05346v1,"Recent advancements in large language models (LLMs) have underscored their
vulnerability to safety alignment jailbreaks, particularly when subjected to
downstream fine-tuning. However, existing mitigation strategies primarily focus
on reactively addressing jailbreak incidents after safety guardrails have been
compromised, removing harmful gradients during fine-tuning, or continuously
reinforcing safety alignment throughout fine-tuning. As such, they tend to
overlook a critical upstream factor: the role of the original safety-alignment
data. This paper therefore investigates the degradation of safety guardrails
through the lens of representation similarity between upstream alignment
datasets and downstream fine-tuning tasks. Our experiments demonstrate that
high similarity between these datasets significantly weakens safety guardrails,
making models more susceptible to jailbreaks. Conversely, low similarity
between these two types of datasets yields substantially more robust models and
thus reduces harmfulness score by up to 10.33%. By highlighting the importance
of upstream dataset design in the building of durable safety guardrails and
reducing real-world vulnerability to jailbreak attacks, these findings offer
actionable insights for fine-tuning service providers.",最近，大型语言模型（LLMs）的进展突显了它们在安全对齐方面的脆弱性，特别是在进行下游微调时。然而，现有的缓解策略主要集中在反应性地应对安全防护栏被破坏后的越狱事件，在微调过程中删除有害梯度，或者在微调过程中不断强化安全对齐。因此，它们往往忽略了一个关键的上游因素：原始安全对齐数据的作用。本文因此通过上游对齐数据集和下游微调任务之间的表示相似性的视角，研究了安全防护栏的退化。我们的实验表明，这些数据集之间的高相似性显著削弱了安全防护栏，使模型更容易受到越狱攻击。相反，这些两种类型数据集之间的低相似性产生了显著更健壮的模型，从而将有害性得分降低了多达10.33%。通过强调上游数据集设计在构建耐用安全防护栏和减少现实世界中对越狱攻击的易受性方面的重要性，这些发现为微调服务提供商提供了可操作的见解。,The paper explores how the similarity between alignment and fine-tuning datasets affects the safety and robustness of large language models.,LLM,Harmless,"Safety alignment, jailbreaks, fine-tuning, dataset similarity, harmfulness"
"Focus On This, Not That! Steering LLMs with Adaptive Feature
  Specification","Tom A. Lamb, Adam Davies, Alasdair Paren, Philip H. S. Torr, Francesco Pinto",2024-10-30T12:01:48Z,http://arxiv.org/pdf/2410.22944v4,"Despite the success of Instruction Tuning (IT) in training large language
models (LLMs), such models often leverage spurious or biased features learnt
from their training data and can become misaligned, leading to undesired
behaviours. While existing techniques can steer model behaviour at
inference-time, they are often post-hoc and do not embed steering as an
intrinsic model feature. In this work, we introduce Focus Instruction Tuning
(FIT), which trains LLMs to condition their responses by focusing on specific
features whilst ignoring others, leading to different behaviours based on what
features are specified. Across diverse benchmarks, we demonstrate that FIT: (i)
successfully steers behaviour at inference time; (ii) increases robustness by
amplifying core task signals and down-weighting spurious cues; (iii) mitigates
social bias by suppressing demographic attributes; and (iv) generalises under
distribution shifts and to previously unseen focus features. FIT therefore
offers a lightweight, intrinsic mechanism for building more robust, fair, and
easily controllable LLMs.",尽管指令微调（IT）在训练大型语言模型（LLMs）方面取得了成功，但这些模型往往利用从训练数据中学到的虚假或有偏见的特征，可能会导致不良行为。虽然现有技术可以在推理时引导模型行为，但它们通常是事后的，并没有将引导作为模型的内在特征。在本文中，我们引入了聚焦指令微调（FIT），它训练LLMs在响应时通过专注于特定特征而忽略其他特征，从而根据指定的特征产生不同的行为。在多种基准测试中，我们证明了FIT：(i) 在推理时成功引导行为；(ii) 通过放大核心任务信号和减少虚假线索来增加鲁棒性；(iii) 通过抑制人口统计属性来缓解社会偏见；(iv) 在分布偏移和对之前未见过的聚焦特征的推广中。因此，FIT 提供了一种轻量级、内在的机制，用于构建更加健壮、公平和易于控制的LLMs。,"The paper introduces Focus Instruction Tuning (FIT), a method to train LLMs to focus on specific features, enhancing robustness, fairness, and controllability.",LLM,"Helpful, Harmless","Instruction Tuning, Feature Specification, Bias Mitigation, Robustness, Controllability"
"The Lessons of Developing Process Reward Models in Mathematical
  Reasoning","Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, Junyang Lin",2025-01-13T13:10:16Z,http://arxiv.org/pdf/2501.07301v2,"Process Reward Models (PRMs) emerge as a promising approach for process
supervision in mathematical reasoning of Large Language Models (LLMs), which
aim to identify and mitigate intermediate errors in the reasoning processes.
However, the development of effective PRMs faces significant challenges,
particularly in data annotation and evaluation methodologies. In this paper,
through extensive experiments, we demonstrate that commonly used Monte Carlo
(MC) estimation-based data synthesis for PRMs typically yields inferior
performance and generalization compared to LLM-as-a-judge and human annotation
methods. MC estimation relies on completion models to evaluate current-step
correctness, leading to inaccurate step verification. Furthermore, we identify
potential biases in conventional Best-of-N (BoN) evaluation strategies for
PRMs: (1) The unreliable policy models generate responses with correct answers
but flawed processes, leading to a misalignment between the evaluation criteria
of BoN and the PRM objectives of process verification. (2) The tolerance of
PRMs of such responses leads to inflated BoN scores. (3) Existing PRMs have a
significant proportion of minimum scores concentrated on the final answer
steps, revealing the shift from process to outcome-based assessment in BoN
Optimized PRMs. To address these challenges, we develop a consensus filtering
mechanism that effectively integrates MC estimation with LLM-as-a-judge and
advocates a more comprehensive evaluation framework that combines
response-level and step-level metrics. Based on the mechanisms, we
significantly improve both model performance and data efficiency in the BoN
evaluation and the step-wise error identification task. Finally, we release a
new state-of-the-art PRM that outperforms existing open-source alternatives and
provides practical guidelines for future research in building process
supervision models.",过程奖励模型（PRMs）作为大型语言模型（LLMs）数学推理过程监督的一种有前途的方法，旨在识别和缓解推理过程中的中间错误。然而，有效的PRMs的开发面临重大挑战，特别是数据注释和评估方法。在本文中，通过广泛的实验，我们证明了常用的基于蒙特卡罗（MC）估计的数据合成方法通常在PRMs中表现不佳，并且在LLM作为裁判和人类注释方法中表现不佳。MC估计依赖于完成模型来评估当前步骤的正确性，导致步骤验证不准确。此外，我们识别了传统的最佳N（BoN）评估策略中的潜在偏差：1）不可靠的策略模型生成具有正确答案但过程有缺陷的响应，导致BoN的评估标准与PRM的过程验证目标不一致。2）PRMs对这些响应的容忍度导致BoN分数膨胀。3）现有的PRMs在最终答案步骤中有显著比例的最低分数，揭示了BoN优化的PRMs从过程到结果的评估转变。为了解决这些挑战，我们开发了一种共识过滤机制，有效地将MC估计与LLM作为裁判结合起来，并倡导一种更全面的评估框架，结合响应级和步骤级指标。基于这些机制，我们在BoN评估和逐步错误识别任务中显著提高了模型性能和数据效率。最后，我们发布了一种新的最先进的PRM，优于现有的开源替代方案，并为未来研究提供了实用指南，以构建过程监督模型。,"The paper presents a new Process Reward Model (PRM) for improving the mathematical reasoning of Large Language Models (LLMs) by addressing challenges in data annotation, evaluation methodologies, and biases in conventional evaluation strategies.",LLM,Helpful,"Process Reward Models, Mathematical Reasoning, Large Language Models, Error Mitigation, Evaluation Methodologies"
Optimizing Anytime Reasoning via Budget Relative Policy Optimization,"Penghui Qi, Zichen Liu, Tianyu Pang, Chao Du, Wee Sun Lee, Min Lin",2025-05-19T17:58:44Z,http://arxiv.org/pdf/2505.13438v2,"Scaling test-time compute is crucial for enhancing the reasoning capabilities
of large language models (LLMs). Existing approaches typically employ
reinforcement learning (RL) to maximize a verifiable reward obtained at the end
of reasoning traces. However, such methods optimize only the final performance
under a large and fixed token budget, which hinders efficiency in both training
and deployment. In this work, we present a novel framework, AnytimeReasoner, to
optimize anytime reasoning performance, which aims to improve token efficiency
and the flexibility of reasoning under varying token budget constraints. To
achieve this, we truncate the complete thinking process to fit within sampled
token budgets from a prior distribution, compelling the model to summarize the
optimal answer for each truncated thinking for verification. This introduces
verifiable dense rewards into the reasoning process, facilitating more
effective credit assignment in RL optimization. We then optimize the thinking
and summary policies in a decoupled manner to maximize the cumulative reward.
Additionally, we introduce a novel variance reduction technique, Budget
Relative Policy Optimization (BRPO), to enhance the robustness and efficiency
of the learning process when reinforcing the thinking policy. Empirical results
in mathematical reasoning tasks demonstrate that our method consistently
outperforms GRPO across all thinking budgets under various prior distributions,
enhancing both training and token efficiency.",扩展测试时计算对于增强大型语言模型（LLM）的推理能力至关重要。现有方法通常采用强化学习（RL）来最大化推理轨迹末尾获得的可验证奖励。然而，这些方法仅在大而固定的令牌预算下优化最终性能，这在训练和部署中都会影响效率。在本文中，我们提出了一种新的框架，AnytimeReasoner，旨在优化任何时间推理性能，以提高令牌效率和推理的灵活性，以适应各种令牌预算约束。为了实现这一点，我们将完整的思考过程截断以适应从先验分布中采样的令牌预算，迫使模型为每个截断的思考总结最佳答案以进行验证。这将可验证的稠密奖励引入推理过程，促进RL优化中的更有效的信用分配。然后，我们以分离的方式优化思考和总结策略，以最大化累积奖励。此外，我们引入了一种新的方差减少技术，预算相对策略优化（BRPO），以增强在强化思考策略时学习过程的鲁棒性和效率。在数学推理任务中的实验结果表明，我们的方法在各种先验分布下的所有思考预算中都显著优于GRPO，增强了训练和令牌效率。,"The paper introduces AnytimeReasoner, a framework for optimizing the reasoning performance of LLMs under varying token budgets, enhancing both training and token efficiency.",LLM,None,"LLM, Reasoning, Token Efficiency, Reinforcement Learning, AnytimeReasoner"
"Constrained Entropic Unlearning: A Primal-Dual Framework for Large
  Language Models","Taha Entesari, Arman Hatami, Rinat Khaziev, Anil Ramakrishna, Mahyar Fazlyab",2025-06-05T17:55:23Z,http://arxiv.org/pdf/2506.05314v1,"Large Language Models (LLMs) deployed in real-world settings increasingly
face the need to unlearn sensitive, outdated, or proprietary information.
Existing unlearning methods typically formulate forgetting and retention as a
regularized trade-off, combining both objectives into a single scalarized loss.
This often leads to unstable optimization and degraded performance on retained
data, especially under aggressive forgetting. We propose a new formulation of
LLM unlearning as a constrained optimization problem: forgetting is enforced
via a novel logit-margin flattening loss that explicitly drives the output
distribution toward uniformity on a designated forget set, while retention is
preserved through a hard constraint on a separate retain set. Compared to
entropy-based objectives, our loss is softmax-free, numerically stable, and
maintains non-vanishing gradients, enabling more efficient and robust
optimization. We solve the constrained problem using a scalable primal-dual
algorithm that exposes the trade-off between forgetting and retention through
the dynamics of the dual variable. Evaluations on the TOFU and MUSE benchmarks
across diverse LLM architectures demonstrate that our approach consistently
matches or exceeds state-of-the-art baselines, effectively removing targeted
information while preserving downstream utility.",在现实环境中部署的大型语言模型（LLMs）越来越需要忘记敏感、过时或专有信息。现有的忘记方法通常将遗忘和保留作为一个正则化的权衡，将两个目标结合为一个标量化的损失。这通常导致不稳定的优化和保留数据的性能下降，特别是在激进的遗忘下。我们提出了一种新的LLM遗忘公式，作为一个约束优化问题：通过一种新的对数几率边界平滑损失来强制遗忘，该损失显式地将输出分布驱动到指定的遗忘集上，而保留通过一个硬约束在一个单独的保留集上。与基于熵的目标相比，我们的损失是无softmax的，数值稳定，并保持非消失梯度，从而实现更高效和稳健的优化。我们使用一种可扩展的原始-对偶算法来解决约束问题，通过对偶变量的动态显示遗忘和保留之间的权衡。在TOFU和MUSE基准测试中，跨多种LLM架构的评估表明，我们的方法在有效地删除目标信息的同时，始终匹配或超过最先进的基线，保留下游的实用性。,"The paper introduces a constrained optimization framework for unlearning sensitive information in LLMs, ensuring both effective forgetting and retention of useful data.",LLM,Harmless,"Unlearning, Constrained Optimization, Logit-Margin Flattening, LLM Alignment, Primal-Dual Algorithm"
"Improving Data Efficiency for LLM Reinforcement Fine-tuning Through
  Difficulty-targeted Online Data Selection and Rollout Replay","Yifan Sun, Jingyan Shen, Yibin Wang, Tianyu Chen, Zhendong Wang, Mingyuan Zhou, Huan Zhang",2025-06-05T17:55:43Z,http://arxiv.org/pdf/2506.05316v1,"Reinforcement learning (RL) has become an effective approach for fine-tuning
large language models (LLMs), particularly to enhance their reasoning
capabilities. However, RL fine-tuning remains highly resource-intensive, and
existing work has largely overlooked the problem of data efficiency. In this
paper, we propose two techniques to improve data efficiency in LLM RL
fine-tuning: difficulty-targeted online data selection and rollout replay. We
introduce the notion of adaptive difficulty to guide online data selection,
prioritizing questions of moderate difficulty that are more likely to yield
informative learning signals. To estimate adaptive difficulty efficiently, we
develop an attention-based framework that requires rollouts for only a small
reference set of questions. The adaptive difficulty of the remaining questions
is then estimated based on their similarity to this set. To further reduce
rollout cost, we introduce a rollout replay mechanism that reuses recent
rollouts, lowering per-step computation while maintaining stable updates.
Extensive experiments across 6 LLM-dataset combinations show that our method
reduces RL fine-tuning time by 25% to 65% to reach the same level of
performance as the original GRPO algorithm.",强化学习（RL）已经成为细化大型语言模型（LLM）的有效方法，特别是为了增强其推理能力。然而，RL细化仍然非常耗费资源，现有工作大多忽略了数据效率的问题。在这篇论文中，我们提出了两种技术来提高LLM RL细化的数据效率：难度定向的在线数据选择和回放回放。我们引入了自适应难度的概念来指导在线数据选择，优先选择中等难度的问题，这些问题更有可能产生有信息的学习信号。为了高效估计自适应难度，我们开发了一种基于注意力的框架，只需要对少量参考问题集进行回放。然后，根据其与该集合的相似性，估计剩余问题的自适应难度。为了进一步降低回放成本，我们引入了一种回放回放机制，重用最近的回放，降低每步计算量，同时保持稳定更新。广泛的实验表明，我们的方法在达到与原始GRPO算法相同的性能水平时，将RL细化时间减少了25%到65%。,"The paper presents methods to enhance data efficiency in reinforcement learning fine-tuning of large language models, focusing on adaptive difficulty and rollout replay.",LLM,Helpful,"Reinforcement Learning, Data Efficiency, Fine-tuning, Large Language Models, Rollout Replay"
"Full-Parameter Continual Pretraining of Gemma2: Insights into Fluency
  and Domain Knowledge","Vytenis Šliogeris, Povilas Daniušis, Artūras Nakvosas",2025-05-09T10:43:37Z,http://arxiv.org/pdf/2505.05946v2,"In this technical report, we empirically investigate the relationship between
linguistic fluency and domain knowledge in the context of continual learning
with large language models (LLMs). Specifically, we enhance the linguistic
fluency of the Gemma2 LLM for the Lithuanian language by autoregressively
pretraining its full parameter set on the first 10\% of the Lithuanian language
component of the CulturaX dataset. To prevent catastrophic forgetting of the
model's existing domain knowledge, we apply Elastic Weight Consolidation (EWC),
leveraging Fisher information estimated using data from the Massive Multitask
Language Understanding (MMLU) benchmark. In the post-training evaluations, we
assess linguistic fluency through perplexity and evaluate domain knowledge
using accuracy on a suite of language understanding benchmarks, including
ARC-Easy, Belebele, GSM8K, HellaSwag, MMLU, TruthfulQA, and Winogrande, in both
English and Lithuanian. The empirical results demonstrate that EWC not only
mitigates catastrophic forgetting by preserving the model's performance in
terms of both linguistic fluency and domain knowledge but also improves or
maintains these capabilities for the newly added Lithuanian language. These
findings highlight the potential for more efficient adaptation of
general-purpose LLMs to under-represented languages without requiring access to
the original training data. The accompanying codebase is openly accessible at
https://github.com/Neurotechnology/LLM_EWC.",在这篇技术报告中，我们从持续学习的角度出发，实证研究了语言流利性与领域知识之间的关系。具体来说，我们通过自回归预训练其全参数集，利用CulturaX数据集的立陶宛语言组成部分的前10%，提高了Gemma2 LLM的语言流利性。为了防止模型现有领域知识的灾难性遗忘，我们应用了弹性权重巩固（EWC），利用来自大规模多任务语言理解（MMLU）基准的Fisher信息进行估计。在后训练评估中，我们通过困惑度评估语言流利性，并使用一套语言理解基准，包括ARC-Easy、Belebele、GSM8K、HellaSwag、MMLU、TruthfulQA和Winogrande，在英语和立陶宛语中评估领域知识。实证结果表明，EWC不仅通过保持模型在语言流利性和领域知识方面的性能来缓解灾难性遗忘，还提高或保持了新添加的立陶宛语言的这些能力。这些发现突显了在没有访问原始训练数据的情况下，更高效地将通用LLM适应不太代表性语言的潜力。相关代码库可以在https://github.com/Neurotechnology/LLM_EWC上公开获取。,"The paper explores continual pretraining of LLMs to improve linguistic fluency and domain knowledge, using Elastic Weight Consolidation to mitigate catastrophic forgetting and evaluating honesty through benchmarks like TruthfulQA.",LLM,Honest,"Continual learning, LLM, EWC, Linguistic fluency, Domain knowledge"
"Fine-Grained Interpretation of Political Opinions in Large Language
  Models","Jingyu Hu, Mengyue Yang, Mengnan Du, Weiru Liu",2025-06-05T09:06:59Z,http://arxiv.org/pdf/2506.04774v1,"Studies of LLMs' political opinions mainly rely on evaluations of their
open-ended responses. Recent work indicates that there is a misalignment
between LLMs' responses and their internal intentions. This motivates us to
probe LLMs' internal mechanisms and help uncover their internal political
states. Additionally, we found that the analysis of LLMs' political opinions
often relies on single-axis concepts, which can lead to concept confounds. In
this work, we extend the single-axis to multi-dimensions and apply
interpretable representation engineering techniques for more transparent LLM
political concept learning. Specifically, we designed a four-dimensional
political learning framework and constructed a corresponding dataset for
fine-grained political concept vector learning. These vectors can be used to
detect and intervene in LLM internals. Experiments are conducted on eight
open-source LLMs with three representation engineering techniques. Results show
these vectors can disentangle political concept confounds. Detection tasks
validate the semantic meaning of the vectors and show good generalization and
robustness in OOD settings. Intervention Experiments show these vectors can
intervene in LLMs to generate responses with different political leanings.",研究大型语言模型（LLM）的政治观点主要依赖于对其开放式响应的评估。最近的工作表明，LLM的响应与其内部意图之间存在不一致。这激发了我们探索LLM的内部机制，并帮助揭示其内部政治状态。此外，我们发现，LLM政治观点的分析通常依赖于单轴概念，这可能导致概念混淆。在本工作中，我们将单轴扩展到多维，并应用可解释的表示工程技术，以实现更透明的LLM政治概念学习。具体来说，我们设计了一个四维政治学习框架，并构建了相应的数据集，用于细粒度政治概念向量学习。这些向量可以用于检测和干预LLM内部。实验在八个开源LLM上进行，使用三种表示工程技术。结果表明，这些向量可以分离政治概念混淆。检测任务验证了向量的语义意义，并在OOD设置中显示出良好的泛化能力和鲁棒性。干预实验表明，这些向量可以干预LLM，生成具有不同政治倾向的响应。,"The paper presents a multi-dimensional framework for interpreting and intervening in the political opinions of LLMs, aiming to align their internal mechanisms with external responses.",LLM,"Helpful, Honest","LLM alignment, political opinions, internal mechanisms, multi-dimensional analysis, intervention"
"MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement
  Learning","Yiqing Liang, Jielin Qiu, Wenhao Ding, Zuxin Liu, James Tompkin, Mengdi Xu, Mengzhou Xia, Zhengzhong Tu, Laixi Shi, Jiacheng Zhu",2025-05-30T17:59:38Z,http://arxiv.org/pdf/2505.24871v2,"Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as
a powerful paradigm for post-training large language models (LLMs), achieving
state-of-the-art performance on tasks with structured, verifiable answers.
Applying RLVR to Multimodal LLMs (MLLMs) presents significant opportunities but
is complicated by the broader, heterogeneous nature of vision-language tasks
that demand nuanced visual, logical, and spatial capabilities. As such,
training MLLMs using RLVR on multiple datasets could be beneficial but creates
challenges with conflicting objectives from interaction among diverse datasets,
highlighting the need for optimal dataset mixture strategies to improve
generalization and reasoning. We introduce a systematic post-training framework
for Multimodal LLM RLVR, featuring a rigorous data mixture problem formulation
and benchmark implementation. Specifically, (1) We developed a multimodal RLVR
framework for multi-dataset post-training by curating a dataset that contains
different verifiable vision-language problems and enabling multi-domain online
RL learning with different verifiable rewards; (2) We proposed a data mixture
strategy that learns to predict the RL fine-tuning outcome from the data
mixture distribution, and consequently optimizes the best mixture.
Comprehensive experiments showcase that multi-domain RLVR training, when
combined with mixture prediction strategies, can significantly boost MLLM
general reasoning capacities. Our best mixture improves the post-trained
model's accuracy on out-of-distribution benchmarks by an average of 5.24%
compared to the same model post-trained with uniform data mixture, and by a
total of 20.74% compared to the pre-finetuning baseline.",强化学习与可验证奖励（RLVR）最近作为一种强大的范式出现，用于大型语言模型（LLMs）的后训练，在具有结构化、可验证答案的任务上实现了最先进的性能。将RLVR应用于多模态LLMs（MLLMs）带来了显著的机会，但由于视觉语言任务的广泛、异质性质，这些任务需要细致的视觉、逻辑和空间能力。因此，在多个数据集上使用RLVR训练MLLMs可能是有益的，但会带来来自不同数据集之间交互的冲突目标，突显了需要最佳数据混合策略以提高泛化和推理能力。我们引入了一个系统化的后训练框架，用于多模态LLM RLVR，具有严格的数据混合问题公式和基准实现。具体来说，(1) 我们通过整理一个包含不同可验证视觉语言问题的数据集，开发了一个多模态RLVR框架，用于多数据集后训练，并启用了不同可验证奖励的多域在线RL学习; (2) 我们提出了一种数据混合策略，从数据混合分布中学习预测RL微调结果，并相应地优化最佳混合。全面的实验表明，多域RLVR训练，结合混合预测策略，可以显著提高MLLM的泛化推理能力。我们的最佳混合将后训练模型在分布外基准上的准确性提高了平均5.24%，与使用均匀数据混合后训练的相同模型相比，与预微调基线相比，总共提高了20.74%。,"The paper introduces a framework for post-training multimodal large language models using reinforcement learning with verifiable rewards, improving their generalization and reasoning capabilities.",LLM,Helpful,"Reinforcement Learning, Multimodal LLMs, Data Mixture, Generalization, Reasoning"
"David and Goliath: Small One-step Model Beats Large Diffusion with Score
  Post-training","Weijian Luo, Colin Zhang, Debing Zhang, Zhengyang Geng",2024-10-28T10:26:19Z,http://arxiv.org/pdf/2410.20898v3,"We propose Diff-Instruct* (DI*), a data-efficient post-training approach for
one-step text-to-image generative models to improve its human preferences
without requiring image data. Our method frames alignment as online
reinforcement learning from human feedback (RLHF), which optimizes the one-step
model to maximize human reward functions while being regularized to be kept
close to a reference diffusion process. Unlike traditional RLHF approaches,
which rely on the Kullback-Leibler divergence as the regularization, we
introduce a novel general score-based divergence regularization that
substantially improves performance as well as post-training stability. Although
the general score-based RLHF objective is intractable to optimize, we derive a
strictly equivalent tractable loss function in theory that can efficiently
compute its \emph{gradient} for optimizations. We introduce
\emph{DI*-SDXL-1step}, which is a 2.6B one-step text-to-image model at a
resolution of $1024\times 1024$, post-trained from DMD2 w.r.t SDXL. \textbf{Our
2.6B \emph{DI*-SDXL-1step} model outperforms the 50-step 12B FLUX-dev model} in
ImageReward, PickScore, and CLIP score on the Parti prompts benchmark while
using only 1.88\% of the inference time. This result clearly shows that with
proper post-training, the small one-step model is capable of beating huge
multi-step diffusion models. Our model is open-sourced at this link:
https://github.com/pkulwj1994/diff_instruct_star. We hope our findings can
contribute to human-centric machine learning techniques.",我们提出了Diff-Instruct* (DI*)，一种数据高效的后训练方法，用于改进一步文本到图像生成模型的人类偏好，而不需要图像数据。我们的方法将对齐作为在线强化学习从人类反馈（RLHF），优化一步模型以最大化人类奖励函数，同时正则化以保持与参考扩散过程的接近。与传统的RLHF方法不同，它们依赖于Kullback-Leibler散度作为正则化，我们引入了一种新的通用得分基于散度正则化，显著提高了性能以及后训练稳定性。虽然通用得分基于RLHF目标是不可优化的，我们在理论上推导出一个严格等价的可行损失函数，可以高效计算其梯度进行优化。我们引入了DI*-SDXL-1step，这是一个2.6B一步文本到图像模型，分辨率为1024×1024，从DMD2相对于SDXL后训练。我们的2.6B DI*-SDXL-1step模型在ImageReward、PickScore和CLIP分数上超过了50步12B FLUX-dev模型，而使用的推理时间仅为1.88%。这表明，通过适当的后训练，小型一步模型能够击败巨大的多步扩散模型。我们的模型在以下链接开源：https://github.com/pkulwj1994/diff_instruct_star。我们希望我们的发现能为以人为中心的机器学习技术做出贡献。,The paper introduces a data-efficient post-training approach for one-step text-to-image generative models using RLHF to improve human preferences.,LLM,Helpful,"RLHF, Alignment, Human Preferences, Diffusion Models, Score-based Divergence"
"Psi-Sampler: Initial Particle Sampling for SMC-Based Inference-Time
  Reward Alignment in Score Models","Taehoon Yoon, Yunhong Min, Kyeongmin Yeo, Minhyuk Sung",2025-06-02T05:02:33Z,http://arxiv.org/pdf/2506.01320v2,"We introduce $\Psi$-Sampler, an SMC-based framework incorporating pCNL-based
initial particle sampling for effective inference-time reward alignment with a
score-based generative model. Inference-time reward alignment with score-based
generative models has recently gained significant traction, following a broader
paradigm shift from pre-training to post-training optimization. At the core of
this trend is the application of Sequential Monte Carlo (SMC) to the denoising
process. However, existing methods typically initialize particles from the
Gaussian prior, which inadequately captures reward-relevant regions and results
in reduced sampling efficiency. We demonstrate that initializing from the
reward-aware posterior significantly improves alignment performance. To enable
posterior sampling in high-dimensional latent spaces, we introduce the
preconditioned Crank-Nicolson Langevin (pCNL) algorithm, which combines
dimension-robust proposals with gradient-informed dynamics. This approach
enables efficient and scalable posterior sampling and consistently improves
performance across various reward alignment tasks, including layout-to-image
generation, quantity-aware generation, and aesthetic-preference generation, as
demonstrated in our experiments. Project Webpage:
https://psi-sampler.github.io/",我们介绍了 $\Psi$-Sampler，一个基于 SMC 的框架，结合 pCNL 方法进行初始粒子采样，以实现基于得分的生成模型的推理时奖励对齐。推理时奖励对齐与基于得分的生成模型最近引起了广泛关注，随着从预训练到后训练优化的范式转变。这一趋势的核心是将顺序蒙特卡罗（SMC）应用于去噪过程。然而，现有方法通常从高斯先验初始化粒子，这无法充分捕捉奖励相关区域，导致采样效率降低。我们证明从奖励感知后验初始化显著提高了对齐性能。为了在高维潜在空间中实现后验采样，我们引入了预调节克兰克-尼科尔森朗之万（pCNL）算法，它结合了维度鲁棒的提案与梯度信息动态。这种方法使得高效且可扩展的后验采样成为可能，并在各种奖励对齐任务中，包括布局到图像生成、数量感知生成和美学偏好生成，显著提高了性能，如我们实验中所展示的。,The paper presents a framework for efficient reward alignment in score-based generative models using a novel particle sampling method.,LLM,"Helpful, Harmless","Reward alignment, score models, SMC, pCNL, generative models"
"AGENTFUZZER: Generic Black-Box Fuzzing for Indirect Prompt Injection
  against LLM Agents","Zhun Wang, Vincent Siu, Zhe Ye, Tianneng Shi, Yuzhou Nie, Xuandong Zhao, Chenguang Wang, Wenbo Guo, Dawn Song",2025-05-09T07:40:17Z,http://arxiv.org/pdf/2505.05849v3,"The strong planning and reasoning capabilities of Large Language Models
(LLMs) have fostered the development of agent-based systems capable of
leveraging external tools and interacting with increasingly complex
environments. However, these powerful features also introduce a critical
security risk: indirect prompt injection, a sophisticated attack vector that
compromises the core of these agents, the LLM, by manipulating contextual
information rather than direct user prompts. In this work, we propose a generic
black-box fuzzing framework, AgentFuzzer, designed to automatically discover
and exploit indirect prompt injection vulnerabilities across diverse LLM
agents. Our approach starts by constructing a high-quality initial seed corpus,
then employs a seed selection algorithm based on Monte Carlo Tree Search (MCTS)
to iteratively refine inputs, thereby maximizing the likelihood of uncovering
agent weaknesses. We evaluate AgentFuzzer on two public benchmarks, AgentDojo
and VWA-adv, where it achieves 71% and 70% success rates against agents based
on o3-mini and GPT-4o, respectively, nearly doubling the performance of
baseline attacks. Moreover, AgentFuzzer exhibits strong transferability across
unseen tasks and internal LLMs, as well as promising results against defenses.
Beyond benchmark evaluations, we apply our attacks in real-world environments,
successfully misleading agents to navigate to arbitrary URLs, including
malicious sites.",大语言模型（LLM）强大的规划和推理能力促进了基于代理的系统的发展，这些系统能够利用外部工具并与日益复杂的环境进行交互。然而，这些强大的功能也引入了一个关键的安全风险：间接提示注入，这是一种通过操纵上下文信息而不是直接用户提示来破坏这些代理核心的LLM的复杂攻击向量。在本工作中，我们提出了一种通用的黑盒模糊测试框架AgentFuzzer，旨在自动发现和利用跨多种LLM代理的间接提示注入漏洞。我们的方法首先通过构建高质量的初始种子语料库，然后使用基于蒙特卡罗树搜索（MCTS）的种子选择算法迭代地改进输入，从而最大化发现代理弱点的可能性。我们在两个公共基准AgentDojo和VWA-adv上评估了AgentFuzzer，其中它在基于o3-mini和GPT-4o的代理上分别实现了71%和70%的成功率，几乎是基线攻击的两倍。此外，AgentFuzzer在未见过的任务和内部LLM之间表现出强大的可转移性，以及在防御措施下表现出有前途的结果。除了基准评估，我们还在真实世界的环境中应用了我们的攻击，成功地误导代理导航到任意URL，包括恶意网站。,"The paper introduces AgentFuzzer, a framework for discovering and exploiting indirect prompt injection vulnerabilities in LLM agents, demonstrating high success rates and real-world applicability.",LLM,Harmless,"Fuzzing, Indirect Prompt Injection, LLM Agents, Security, Vulnerabilities"
"Seven Security Challenges That Must be Solved in Cross-domain
  Multi-agent LLM Systems","Ronny Ko, Jiseong Jeong, Shuyuan Zheng, Chuan Xiao, Tae-Wan Kim, Makoto Onizuka, Won-Yong Shin",2025-05-28T18:19:03Z,http://arxiv.org/pdf/2505.23847v2,"Large language models (LLMs) are rapidly evolving into autonomous agents that
cooperate across organizational boundaries, enabling joint disaster response,
supply-chain optimization, and other tasks that demand decentralized expertise
without surrendering data ownership. Yet, cross-domain collaboration shatters
the unified trust assumptions behind current alignment and containment
techniques. An agent benign in isolation may, when receiving messages from an
untrusted peer, leak secrets or violate policy, producing risks driven by
emergent multi-agent dynamics rather than classical software bugs. This
position paper maps the security agenda for cross-domain multi-agent LLM
systems. We introduce seven categories of novel security challenges, for each
of which we also present plausible attacks, security evaluation metrics, and
future research guidelines.",大语言模型（LLM）正迅速发展成为跨组织边界合作的自主代理，实现联合灾害应对、供应链优化等需要分散专业知识而不放弃数据所有权的任务。然而，跨域协作打破了当前对齐和包含技术背后的统一信任假设。孤立时良性的代理在接收来自不受信任的同行的消息时，可能会泄露秘密或违反政策，产生由新兴多代理动态驱动的风险，而不是经典的软件错误。本文概述了跨域多代理LLM系统的安全议程。我们引入了七类新型的安全挑战，并为每一类都提出了可信的攻击、安全评估指标和未来研究指南。,"The paper identifies seven security challenges in cross-domain multi-agent LLM systems, focusing on alignment issues like policy violation and trust.",LLM,Harmless,"Security, Multi-agent, Cross-domain, Alignment, LLM"
Jailbreak Attack Initializations as Extractors of Compliance Directions,"Amit Levi, Rom Himelstein, Yaniv Nemcovsky, Avi Mendelson, Chaim Baskin",2025-02-13T20:25:40Z,http://arxiv.org/pdf/2502.09755v2,"Safety-aligned LLMs respond to prompts with either compliance or refusal,
each corresponding to distinct directions in the model's activation space.
Recent works show that initializing attacks via self-transfer from other
prompts significantly enhances their performance. However, the underlying
mechanisms of these initializations remain unclear, and attacks utilize
arbitrary or hand-picked initializations. This work presents that each
gradient-based jailbreak attack and subsequent initialization gradually
converge to a single compliance direction that suppresses refusal, thereby
enabling an efficient transition from refusal to compliance. Based on this
insight, we propose CRI, an initialization framework that aims to project
unseen prompts further along compliance directions. We demonstrate our approach
on multiple attacks, models, and datasets, achieving an increased attack
success rate (ASR) and reduced computational overhead, highlighting the
fragility of safety-aligned LLMs. A reference implementation is available at:
https://amit1221levi.github.io/CRI-Jailbreak-Init-LLMs-evaluation.",安全对齐的大语言模型（LLM）对提示的响应要么是遵从，要么是拒绝，每种响应对应于模型激活空间中的不同方向。最近的研究表明，通过从其他提示进行自我传输来初始化攻击显著提高了其性能。然而，这些初始化的基本机制仍不明确，攻击利用任意或手动选择的初始化。本文提出，每个基于梯度的越狱攻击及其后续初始化逐渐收敛到一个抑制拒绝的遵从方向，从而使从拒绝到遵从的高效过渡成为可能。基于这一见解，我们提出了CRI，一个初始化框架，旨在将未见过的提示进一步投影到遵从方向。我们在多个攻击、模型和数据集上展示了我们的方法，实现了更高的攻击成功率（ASR）和较低的计算开销，突显了安全对齐LLM的脆弱性。参考实现可在以下网址找到：https://amit1221levi.github.io/CRI-Jailbreak-Init-LLMs-evaluation。,The paper introduces a framework to enhance jailbreak attacks on safety-aligned LLMs by initializing prompts along compliance directions.,LLM,Harmless,"Jailbreak attacks, compliance, safety alignment, LLM, initialization"
"PoisonedParrot: Subtle Data Poisoning Attacks to Elicit
  Copyright-Infringing Content from Large Language Models","Michael-Andrei Panaitescu-Liess, Pankayaraj Pathmanathan, Yigitcan Kaya, Zora Che, Bang An, Sicheng Zhu, Aakriti Agrawal, Furong Huang",2025-03-10T17:13:30Z,http://arxiv.org/pdf/2503.07697v2,"As the capabilities of large language models (LLMs) continue to expand, their
usage has become increasingly prevalent. However, as reflected in numerous
ongoing lawsuits regarding LLM-generated content, addressing copyright
infringement remains a significant challenge. In this paper, we introduce
PoisonedParrot: the first stealthy data poisoning attack that induces an LLM to
generate copyrighted content even when the model has not been directly trained
on the specific copyrighted material. PoisonedParrot integrates small fragments
of copyrighted text into the poison samples using an off-the-shelf LLM. Despite
its simplicity, evaluated in a wide range of experiments, PoisonedParrot is
surprisingly effective at priming the model to generate copyrighted content
with no discernible side effects. Moreover, we discover that existing defenses
are largely ineffective against our attack. Finally, we make the first attempt
at mitigating copyright-infringement poisoning attacks by proposing a defense:
ParrotTrap. We encourage the community to explore this emerging threat model
further.",随着大型语言模型（LLM）的能力不断扩展，它们的使用变得越来越普遍。然而，正如众多关于LLM生成内容的在诉讼中反映的那样，解决版权侵权问题仍然是一个重大挑战。在本文中，我们引入了PoisonedParrot：第一个隐蔽的数据投毒攻击，它能够诱使LLM生成版权内容，即使模型没有直接在特定版权材料上进行训练。PoisonedParrot将版权文本的小片段集成到毒样本中，使用现成的LLM。尽管其简单，但在广泛的实验中评估，PoisonedParrot在引导模型生成版权内容方面效果惊人，没有明显的副作用。此外，我们发现现有的防御措施在很大程度上对我们的攻击无效。最后，我们提出了一种防御措施：ParrotTrap，以缓解版权侵权投毒攻击。我们鼓励社区进一步探索这一新兴的威胁模型。,"The paper presents PoisonedParrot, a data poisoning attack that tricks LLMs into generating copyrighted content, and proposes a defense mechanism called ParrotTrap.",LLM,Harmless,"Data poisoning, copyright infringement, large language models, PoisonedParrot, ParrotTrap"
"Toward a Human-Centered Evaluation Framework for Trustworthy LLM-Powered
  GUI Agents","Chaoran Chen, Zhiping Zhang, Ibrahim Khalilov, Bingcan Guo, Simret A Gebreegziabher, Yanfang Ye, Ziang Xiao, Yaxing Yao, Tianshi Li, Toby Jia-Jun Li",2025-04-24T20:51:20Z,http://arxiv.org/pdf/2504.17934v2,"The rise of Large Language Models (LLMs) has revolutionized Graphical User
Interface (GUI) automation through LLM-powered GUI agents, yet their ability to
process sensitive data with limited human oversight raises significant privacy
and security risks. This position paper identifies three key risks of GUI
agents and examines how they differ from traditional GUI automation and general
autonomous agents. Despite these risks, existing evaluations focus primarily on
performance, leaving privacy and security assessments largely unexplored. We
review current evaluation metrics for both GUI and general LLM agents and
outline five key challenges in integrating human evaluators for GUI agent
assessments. To address these gaps, we advocate for a human-centered evaluation
framework that incorporates risk assessments, enhances user awareness through
in-context consent, and embeds privacy and security considerations into GUI
agent design and evaluation.",大语言模型（LLM）的崛起极大地改变了图形用户界面（GUI）自动化，通过LLM驱动的GUI代理，但它们在有限的人类监督下处理敏感数据的能力引发了重大的隐私和安全风险。本文提出了三个关键风险，并探讨了它们与传统GUI自动化和一般自主代理的不同之处。尽管存在这些风险，现有的评估主要集中在性能上，而隐私和安全评估则大多未被探索。我们回顾了GUI和一般LLM代理的现有评估指标，并概述了在GUI代理评估中整合人类评估者的五个关键挑战。为了弥补这些差距，我们倡导一种以人为中心的评估框架，该框架包括风险评估，通过上下文同意提高用户意识，并将隐私和安全考虑纳入GUI代理的设计和评估中。,The paper proposes a human-centered evaluation framework for LLM-powered GUI agents to address privacy and security risks.,LLM,Harmless,"LLM, GUI agents, privacy, security, evaluation"
"Mind the Confidence Gap: Overconfidence, Calibration, and Distractor
  Effects in Large Language Models",Prateek Chhikara,2025-02-16T07:46:09Z,http://arxiv.org/pdf/2502.11028v2,"Large Language Models (LLMs) show remarkable proficiency in natural language
tasks, yet their frequent overconfidence-misalignment between predicted
confidence and true correctness-poses significant risks in critical
decision-making applications. We present a comprehensive analysis on
calibration in LLMs across nine LLMs and three factual Question-Answering (QA)
datasets, systematically comparing standard free-generation settings against
structured distractor-augmented prompts. Our evaluation reveals that explicitly
incorporating distractors can substantially mitigate miscalibration, achieving
relative accuracy improvements up to 460% and ECE reductions up to 90%. Despite
general trends, we uncover nuanced findings: large RLHF-tuned models display
inherent calibration strengths but can paradoxically suffer increased
miscalibration on easier queries, whereas smaller models benefit
disproportionately from distractor prompts but remain significantly
miscalibrated. Through detailed analyses across question types, we identify
persistent calibration failures, particularly in person-based queries. We
conclude with concrete recommendations-targeted fine-tuning, structured
prompting, and strategic model choice-to ensure reliable, trustworthy LLM
deployments.",大语言模型（LLMs）在自然语言任务中表现出显著的能力，但它们频繁的过度自信——预测的自信与真实的正确性之间的不一致——在关键决策应用中构成了重大风险。我们对九个LLMs和三个事实性问答（QA）数据集中的校准进行了全面分析，系统地将标准的自由生成设置与结构化的干扰增强提示进行比较。我们的评估表明，明确地引入干扰项可以显著缓解不校准，实现相对准确性提高高达460%，ECE减少高达90%。尽管存在一般趋势，我们发现了细微的发现：大型RLHF调整模型显示出固有的校准优势，但在更容易的查询中可能会出现增加的不校准，而较小的模型则从干扰提示中获得不成比例的好处，但仍然显著不校准。通过跨问题类型的详细分析，我们识别出持续的校准失败，特别是在基于人的查询中。我们得出了具体的建议——有针对性的微调、结构化提示和战略性模型选择——以确保可靠、可信的LLM部署。,The paper investigates the overconfidence and calibration issues in LLMs and proposes methods to mitigate these issues for more reliable deployments.,LLM,Helpful,"Calibration, Overconfidence, Distractors, LLMs, QA"
"Firm or Fickle? Evaluating Large Language Models Consistency in
  Sequential Interactions","Yubo Li, Yidi Miao, Xueying Ding, Ramayya Krishnan, Rema Padman",2025-03-28T11:49:56Z,http://arxiv.org/pdf/2503.22353v4,"Large Language Models (LLMs) have shown remarkable capabilities across
various tasks, but their deployment in high-stake domains requires consistent
and coherent behavior across multiple rounds of user interaction. This paper
introduces a comprehensive framework for evaluating and improving LLM response
consistency, making three key contributions. Code and data are available at:
https://github.com/yubol-bobo/MT-Consistency. First, we introduce
Position-Weighted Consistency (PWC), a metric designed to capture both the
importance of early-stage stability and recovery patterns in multi-turn
interactions. Second, we present MT-Consistency, a carefully curated benchmark
dataset spanning diverse domains and difficulty levels, specifically designed
to evaluate LLM consistency under various challenging follow-up scenarios.
Third, we introduce Confidence-Aware Response Generation (CARG), a framework
that significantly improves response stability by explicitly integrating
internal model confidence scores during the generation process. Experimental
results demonstrate that CARG significantly improves response stability without
sacrificing accuracy, offering a practical path toward more dependable LLM
behavior in critical, real-world deployments.",大语言模型（LLMs）在各种任务中表现出了显著的能力，但在高风险领域的部署需要在多轮用户交互中保持一致和连贯的行为。本文提出了一种全面的框架，用于评估和改进LLM响应的一致性，并做出了三个关键贡献。代码和数据可在https://github.com/yubol-bobo/MT-Consistency上获得。首先，我们引入了位置加权一致性（PWC），一种旨在捕捉多轮交互中早期阶段稳定性和恢复模式的重要性的指标。其次，我们提出了MT-Consistency，一个跨越多个领域和难度级别的精心策划的基准数据集，专门用于在各种具有挑战性的后续场景中评估LLM的一致性。第三，我们引入了基于置信度的响应生成（CARG），一种通过在生成过程中显式集成内部模型置信度分数来显著提高响应稳定性的框架。实验结果表明，CARG在不牺牲准确性的情况下显著提高了响应稳定性，为关键的现实世界部署提供了一条实用的途径。,"The paper presents a framework for evaluating and improving the consistency of LLMs in sequential interactions, introducing metrics, a benchmark dataset, and a confidence-aware response generation method.",LLM,Helpful,"Consistency, Sequential Interactions, Response Stability, LLM Evaluation, Benchmark Dataset"
"Knockout LLM Assessment: Using Large Language Models for Evaluations
  through Iterative Pairwise Comparisons","Isik Baran Sandan, Tu Anh Dinh, Jan Niehues",2025-06-04T09:46:43Z,http://arxiv.org/pdf/2506.03785v2,"Large Language Models (LLMs) have shown to be effective evaluators across
various domains such as machine translations or the scientific domain. Current
LLM-as-a-Judge approaches rely mostly on individual assessments or a single
round of pairwise assessments, preventing the judge LLM from developing a
global ranking perspective. To address this, we present Knockout Assessment, an
LLM-asa Judge method using a knockout tournament system with iterative pairwise
comparisons. Experiments across three LLMs on two datasets show that knockout
assessment improves scoring accuracy, increasing Pearson correlation with
expert evaluations by 0.07 on average for university-level exam scoring and
machine translation evaluations, aligning LLM assessments more closely with
human scoring.",大语言模型（LLM）在机器翻译或科学领域等各种领域中表现出有效的评估能力。目前的LLM-as-a-Judge方法主要依赖于单个评估或单轮配对评估，这使得评估者LLM无法发展出全局排名视角。为了解决这个问题，我们提出了Knockout评估，一种使用淘汰赛系统进行迭代配对比较的LLM-as-a-Judge方法。在三个LLM和两个数据集上的实验表明，淘汰赛评估提高了评分准确性，平均提高了与专家评估的皮尔逊相关性0.07，使LLM评估更加接近人类评分。,The paper introduces a knockout tournament system for LLMs to improve their evaluation accuracy and alignment with human scoring.,LLM,Helpful,"LLM evaluation, pairwise comparisons, knockout tournament, scoring accuracy, human alignment"
"Evaluation is All You Need: Strategic Overclaiming of LLM Reasoning
  Capabilities Through Evaluation Design","Lin Sun, Weihong Lin, Jinzhu Wu, Yongfu Zhu, Xiaoqi Jian, Guangxiang Zhao, Change Jia, Linglin Zhang, Sai-er Hu, Yuhan Wu, Xiangzheng Zhang",2025-06-05T08:09:11Z,http://arxiv.org/pdf/2506.04734v1,"Reasoning models represented by the Deepseek-R1-Distill series have been
widely adopted by the open-source community due to their strong performance in
mathematics, science, programming, and other domains. However, our study
reveals that their benchmark evaluation results are subject to significant
fluctuations caused by various factors. Subtle differences in evaluation
conditions can lead to substantial variations in results. Similar phenomena are
observed in other open-source inference models fine-tuned based on the
Deepseek-R1-Distill series, as well as in the QwQ-32B model, making their
claimed performance improvements difficult to reproduce reliably. Therefore, we
advocate for the establishment of a more rigorous paradigm for model
performance evaluation and present our empirical assessments of the
Deepseek-R1-Distill series models.",以深度求解-R1-蒸馏系列为代表的推理模型因其在数学、科学、编程等领域的强大表现而被开源社区广泛采用。然而，我们的研究揭示，它们的基准评估结果受到各种因素的显著波动影响。评估条件的微小差异可能导致结果的显著变化。类似的现象在基于深度求解-R1-蒸馏系列的其他开源推理模型以及QwQ-32B模型中也有观察到，使得它们声称的性能改进难以可靠地重现。因此，我们倡导建立更严格的模型性能评估范式，并呈现我们对深度求解-R1-蒸馏系列模型的经验评估。,The paper highlights the need for rigorous evaluation of LLMs to ensure their claimed reasoning capabilities are reliable and not overstated.,LLM,"Helpful, Honest","Evaluation, Overclaiming, Reasoning, Performance, Benchmark"
Does It Make Sense to Speak of Introspection in Large Language Models?,"Iulia Comşa, Murray Shanahan",2025-06-05T14:13:54Z,http://arxiv.org/pdf/2506.05068v1,"Large language models (LLMs) exhibit compelling linguistic behaviour, and
sometimes offer self-reports, that is to say statements about their own nature,
inner workings, or behaviour. In humans, such reports are often attributed to a
faculty of introspection and are typically linked to consciousness. This raises
the question of how to interpret self-reports produced by LLMs, given their
increasing linguistic fluency and cognitive capabilities. To what extent (if
any) can the concept of introspection be meaningfully applied to LLMs? Here, we
present and critique two examples of apparent introspective self-report from
LLMs. In the first example, an LLM attempts to describe the process behind its
own ``creative'' writing, and we argue this is not a valid example of
introspection. In the second example, an LLM correctly infers the value of its
own temperature parameter, and we argue that this can be legitimately
considered a minimal example of introspection, albeit one that is (presumably)
not accompanied by conscious experience.",大语言模型（LLMs）展示出引人注目的语言行为，有时会提供自我报告，即关于它们自身性质、内部工作原理或行为的陈述。在人类中，这种报告通常归因于一种内省能力，并通常与意识相关联。这引发了如何解释LLMs产生的自我报告的问题，考虑到它们日益增强的语言流利度和认知能力。内省的概念在多大程度上（如果有的话）可以有意义地应用于LLMs？在这里，我们提出并批评了LLMs的两个明显的内省自我报告的例子。在第一个例子中，LLM试图描述其自身“创造性”写作的过程，我们认为这不是内省的有效例子。在第二个例子中，LLM正确推断出其自身温度参数的值，我们认为这可以合法地被认为是内省的最小例子，尽管这（可能）没有伴随着有意识的体验。,"The paper explores whether large language models (LLMs) can exhibit introspection through self-reports, focusing on the honesty aspect of LLM alignment.",LLM,Honest,"Introspection, Self-reports, LLMs, Honesty, Consciousness"
Dissecting Bias in LLMs: A Mechanistic Interpretability Perspective,"Bhavik Chandna, Zubair Bashir, Procheta Sen",2025-06-05T15:43:34Z,http://arxiv.org/pdf/2506.05166v1,"Large Language Models (LLMs) are known to exhibit social, demographic, and
gender biases, often as a consequence of the data on which they are trained. In
this work, we adopt a mechanistic interpretability approach to analyze how such
biases are structurally represented within models such as GPT-2 and Llama2.
Focusing on demographic and gender biases, we explore different metrics to
identify the internal edges responsible for biased behavior. We then assess the
stability, localization, and generalizability of these components across
dataset and linguistic variations. Through systematic ablations, we demonstrate
that bias-related computations are highly localized, often concentrated in a
small subset of layers. Moreover, the identified components change across
fine-tuning settings, including those unrelated to bias. Finally, we show that
removing these components not only reduces biased outputs but also affects
other NLP tasks, such as named entity recognition and linguistic acceptability
judgment because of the sharing of important components with these tasks.",大语言模型（LLMs）以其社会、人口统计和性别偏见而闻名，这通常是由于它们所训练的数据。在本研究中，我们采用机制可解释性方法来分析这些偏见如何在GPT-2和Llama2等模型中结构化地表示。我们专注于人口统计和性别偏见，探索不同的指标来识别负责偏见行为的内部边缘。然后，我们评估这些组件在数据集和语言变化中的稳定性、定位和可推广性。通过系统性的剔除，我们证明偏见相关的计算高度局部化，通常集中在一小部分层中。此外，识别的组件在细化调整设置中发生变化，包括那些与偏见无关的设置。最后，我们展示了删除这些组件不仅减少了偏见输出，还影响了其他NLP任务，如命名实体识别和语言可接受性判断，因为这些任务与这些任务共享重要组件。,The paper uses mechanistic interpretability to analyze and mitigate demographic and gender biases in LLMs like GPT-2 and Llama2.,LLM,Harmless,"Bias, Mechanistic Interpretability, LLMs, Demographic Bias, Gender Bias"
"Micro-Act: Mitigate Knowledge Conflict in Question Answering via
  Actionable Self-Reasoning","Nan Huo, Jinyang Li, Bowen Qin, Ge Qu, Xiaolong Li, Xiaodong Li, Chenhao Ma, Reynold Cheng",2025-06-05T17:33:02Z,http://arxiv.org/pdf/2506.05278v1,"Retrieval-Augmented Generation (RAG) systems commonly suffer from Knowledge
Conflicts, where retrieved external knowledge contradicts the inherent,
parametric knowledge of large language models (LLMs). It adversely affects
performance on downstream tasks such as question answering (QA). Existing
approaches often attempt to mitigate conflicts by directly comparing two
knowledge sources in a side-by-side manner, but this can overwhelm LLMs with
extraneous or lengthy contexts, ultimately hindering their ability to identify
and mitigate inconsistencies. To address this issue, we propose Micro-Act a
framework with a hierarchical action space that automatically perceives context
complexity and adaptively decomposes each knowledge source into a sequence of
fine-grained comparisons. These comparisons are represented as actionable
steps, enabling reasoning beyond the superficial context. Through extensive
experiments on five benchmark datasets, Micro-Act consistently achieves
significant increase in QA accuracy over state-of-the-art baselines across all
5 datasets and 3 conflict types, especially in temporal and semantic types
where all baselines fail significantly. More importantly, Micro-Act exhibits
robust performance on non-conflict questions simultaneously, highlighting its
practical value in real-world RAG applications.",检索增强生成（RAG）系统通常会受到知识冲突的困扰，即检索的外部知识与大型语言模型（LLM）的内在参数知识相矛盾。这会对下游任务（如问答）的性能产生不利影响。现有方法通常试图通过直接比较两个知识源来缓解冲突，但这可能会让LLM感到困惑，最终妨碍它们识别和缓解不一致。为了解决这个问题，我们提出了Micro-Act，一个具有分层动作空间的框架，它可以自动感知上下文复杂性，并适应性地将每个知识源分解为一系列细粒度的比较。这些比较表示为可操作的步骤，使得超出表面上下文的推理成为可能。通过在五个基准数据集上的广泛实验，Micro-Act在所有5个数据集和3种冲突类型上都显著提高了问答准确性，特别是在所有基线都显著失败的时间和语义类型中。更重要的是，Micro-Act在非冲突问题上同时表现出强大的性能，突出了其在实际RAG应用中的实用价值。,"The paper introduces Micro-Act, a framework that improves question answering accuracy in LLMs by mitigating knowledge conflicts through hierarchical action spaces.",LLM,Helpful,"Knowledge Conflict, Question Answering, Retrieval-Augmented Generation, Large Language Models, Micro-Act"
"DREAM: Disentangling Risks to Enhance Safety Alignment in Multimodal
  Large Language Models","Jianyu Liu, Hangyu Guo, Ranjie Duan, Xingyuan Bu, Yancheng He, Shilong Li, Hui Huang, Jiaheng Liu, Yucheng Wang, Chenchen Jing, Xingwei Qu, Xiao Zhang, Yingshui Tan, Yanan Wu, Jihao Gu, Yangguang Li, Jianke Zhu",2025-04-25T03:54:24Z,http://arxiv.org/pdf/2504.18053v2,"Multimodal Large Language Models (MLLMs) pose unique safety challenges due to
their integration of visual and textual data, thereby introducing new
dimensions of potential attacks and complex risk combinations. In this paper,
we begin with a detailed analysis aimed at disentangling risks through
step-by-step reasoning within multimodal inputs. We find that systematic
multimodal risk disentanglement substantially enhances the risk awareness of
MLLMs. Via leveraging the strong discriminative abilities of multimodal risk
disentanglement, we further introduce \textbf{DREAM}
(\textit{\textbf{D}isentangling \textbf{R}isks to \textbf{E}nhance Safety
\textbf{A}lignment in \textbf{M}LLMs}), a novel approach that enhances safety
alignment in MLLMs through supervised fine-tuning and iterative Reinforcement
Learning from AI Feedback (RLAIF). Experimental results show that DREAM
significantly boosts safety during both inference and training phases without
compromising performance on normal tasks (namely oversafety), achieving a
16.17\% improvement in the SIUO safe\&effective score compared to GPT-4V. The
data and code are available at https://github.com/Kizna1ver/DREAM.",多模态大语言模型（MLLMs）由于其整合视觉和文本数据，因此在安全性方面面临独特的挑战，从而引入了新的潜在攻击维度和复杂的风险组合。在本文中，我们从对多模态输入进行逐步推理的详细分析开始，旨在通过系统的多模态风险分离来显著提高MLLMs的风险意识。通过利用多模态风险分离的强大判别能力，我们进一步引入了DREAM（\textit{\textbf{D}isentangling \textbf{R}isks to \textbf{E}nhance Safety \textbf{A}lignment in \textbf{M}LLMs}），一种通过监督微调和迭代强化学习从AI反馈（RLAIF）增强MLLMs安全对齐的新方法。实验结果表明，DREAM在不牺牲正常任务性能（即过度安全）的情况下，显著提高了推理和训练阶段的安全性，在SIUO安全有效得分方面比GPT-4V提高了16.17%。数据和代码可在https://github.com/Kizna1ver/DREAM获得。,"The paper introduces DREAM, a method to enhance safety alignment in multimodal large language models by disentangling risks and using reinforcement learning from AI feedback.",LLM,Harmless,"Safety alignment, Multimodal LLM, Risk disentanglement, RLAIF, Safety"
GRU: Mitigating the Trade-off between Unlearning and Retention for LLMs,"Yue Wang, Qizhou Wang, Feng Liu, Wei Huang, Yali Du, Xiaojiang Du, Bo Han",2025-03-12T07:08:54Z,http://arxiv.org/pdf/2503.09117v3,"Large language model (LLM) unlearning has demonstrated its essential role in
removing privacy and copyright-related responses, crucial for their legal and
safe applications. However, the pursuit of complete unlearning often comes with
substantial costs due to its compromises in their general functionality,
leading to a notorious trade-off between unlearning and retention. It motivates
this paper to explore enhanced unlearning schemes that can mitigate this
trade-off. Specifically, we propose Gradient Rectified Unlearning (GRU), an
improved framework that regulates the directions of gradient updates during the
unlearning procedure such that their side impacts on other, unrelated responses
can be minimized. GRU is easy and general to implement, demonstrating practical
effectiveness across a variety of well-established unlearning benchmarks.",大语言模型（LLM）的遗忘已经证明了其在移除隐私和版权相关响应方面的重要作用，这对于其法律和安全应用至关重要。然而，完全遗忘的追求往往伴随着巨大的成本，因为它在其一般功能上的妥协，导致遗忘和保留之间的著名权衡。这激发了本文探索增强的遗忘方案，以缓解这种权衡。具体来说，我们提出了梯度矫正遗忘（GRU），这是一个改进的框架，它在遗忘过程中调节梯度更新的方向，以便最小化它们对其他无关响应的副作用。GRU 易于实现且通用，在各种经过验证的遗忘基准测试中表现出实用的有效性。,"The paper introduces Gradient Rectified Unlearning (GRU), a framework designed to mitigate the trade-off between unlearning and retention in large language models.",LLM,Harmless,"Unlearning, Retention, Gradient Rectified Unlearning, Privacy, Copyright"
"Text-to-CAD Generation Through Infusing Visual Feedback in Large
  Language Models","Ruiyu Wang, Yu Yuan, Shizhao Sun, Jiang Bian",2025-01-31T11:28:16Z,http://arxiv.org/pdf/2501.19054v3,"Creating Computer-Aided Design (CAD) models requires significant expertise
and effort. Text-to-CAD, which converts textual descriptions into CAD
parametric sequences, is crucial in streamlining this process. Recent studies
have utilized ground-truth parametric sequences, known as sequential signals,
as supervision to achieve this goal. However, CAD models are inherently
multimodal, comprising parametric sequences and corresponding rendered visual
objects. Besides,the rendering process from parametric sequences to visual
objects is many-to-one. Therefore, both sequential and visual signals are
critical for effective training. In this work, we introduce CADFusion, a
framework that uses Large Language Models (LLMs) as the backbone and alternates
between two training stages: the sequential learning (SL) stage and the visual
feedback (VF) stage. In the SL stage, we train LLMs using ground-truth
parametric sequences, enabling the generation of logically coherent parametric
sequences. In the VF stage, we reward parametric sequences that render into
visually preferred objects and penalize those that do not, allowing LLMs to
learn how rendered visual objects are perceived and evaluated. These two stages
alternate throughout the training, ensuring balanced learning and preserving
benefits of both signals. Experiments demonstrate that CADFusion significantly
improves performance, both qualitatively and quantitatively.",创建计算机辅助设计（CAD）模型需要大量的专业知识和努力。文本到CAD，将文本描述转换为CAD参数序列，是简化这一过程的关键。最近的研究利用了作为监督的真实参数序列，称为顺序信号，以实现这一目标。然而，CAD模型本质上是多模态的，包括参数序列和相应的渲染视觉对象。此外，从参数序列到视觉对象的渲染过程是多对一的。因此，顺序和视觉信号对于有效的训练都是至关重要的。在本工作中，我们引入了CADFusion，一个以大型语言模型（LLMs）为基础的框架，并在两个训练阶段之间交替：顺序学习（SL）阶段和视觉反馈（VF）阶段。在SL阶段，我们使用真实参数序列训练LLMs，使其能够生成逻辑上连贯的参数序列。在VF阶段，我们奖励渲染成视觉偏好的对象的参数序列，并惩罚那些没有渲染的参数序列，使LLMs能够学习渲染的视觉对象是如何被感知和评估的。这两个阶段在整个训练过程中交替进行，确保平衡的学习并保留两种信号的好处。实验表明，CADFusion在定性和定量上显著提高了性能。,"The paper introduces CADFusion, a framework using LLMs with visual feedback to enhance text-to-CAD generation.",LLM,Helpful,"Text-to-CAD, Large Language Models, Visual Feedback, CADFusion, Parametric Sequences"
"Evaluating Prompt-Driven Chinese Large Language Models: The Influence of
  Persona Assignment on Stereotypes and Safeguards","Geng Liu, Li Feng, Carlo Alberto Bono, Songbo Yang, Mengxiao Zhu, Francesco Pierri",2025-06-05T12:47:21Z,http://arxiv.org/pdf/2506.04975v1,"Recent research has highlighted that assigning specific personas to large
language models (LLMs) can significantly increase harmful content generation.
Yet, limited attention has been given to persona-driven toxicity in non-Western
contexts, particularly in Chinese-based LLMs. In this paper, we perform a
large-scale, systematic analysis of how persona assignment influences refusal
behavior and response toxicity in Qwen, a widely-used Chinese language model.
Utilizing fine-tuned BERT classifiers and regression analysis, our study
reveals significant gender biases in refusal rates and demonstrates that
certain negative personas can amplify toxicity toward Chinese social groups by
up to 60-fold compared to the default model. To mitigate this toxicity, we
propose an innovative multi-model feedback strategy, employing iterative
interactions between Qwen and an external evaluator, which effectively reduces
toxic outputs without costly model retraining. Our findings emphasize the
necessity of culturally specific analyses for LLMs safety and offer a practical
framework for evaluating and enhancing ethical alignment in LLM-generated
content.",最近的研究表明，为大型语言模型（LLMs）分配特定的角色可以显著增加有害内容的生成。然而，对非西方背景下的角色驱动毒性，特别是基于中文的LLMs，关注较少。在本文中，我们对Qwen（一种广泛使用的中文语言模型）进行了大规模、系统性分析，研究角色分配如何影响拒绝行为和响应毒性。利用微调的BERT分类器和回归分析，我们的研究揭示了拒绝率中的显著性别偏见，并证明某些负面角色可以将对中国社会群体的毒性放大至60倍，与默认模型相比。为了缓解这种毒性，我们提出了一种创新的多模型反馈策略，利用Qwen和外部评估器之间的迭代交互，有效地减少了有毒输出，而无需昂贵的模型重新训练。我们的发现强调了LLMs安全性的文化特定分析的必要性，并为评估和增强LLM生成内容的伦理对齐提供了一个实用框架。,The paper investigates how persona assignment affects toxicity in Chinese LLMs and proposes a multi-model feedback strategy to mitigate harmful outputs.,LLM,Harmless,"Persona assignment, toxicity, Chinese LLMs, ethical alignment, safety"
Search Arena: Analyzing Search-Augmented LLMs,"Mihran Miroyan, Tsung-Han Wu, Logan King, Tianle Li, Jiayi Pan, Xinyan Hu, Wei-Lin Chiang, Anastasios N. Angelopoulos, Trevor Darrell, Narges Norouzi, Joseph E. Gonzalez",2025-06-05T17:59:26Z,http://arxiv.org/pdf/2506.05334v1,"Search-augmented language models combine web search with Large Language
Models (LLMs) to improve response groundedness and freshness. However,
analyzing these systems remains challenging: existing datasets are limited in
scale and narrow in scope, often constrained to static, single-turn,
fact-checking questions. In this work, we introduce Search Arena, a
crowd-sourced, large-scale, human-preference dataset of over 24,000 paired
multi-turn user interactions with search-augmented LLMs. The dataset spans
diverse intents and languages, and contains full system traces with around
12,000 human preference votes. Our analysis reveals that user preferences are
influenced by the number of citations, even when the cited content does not
directly support the attributed claims, uncovering a gap between perceived and
actual credibility. Furthermore, user preferences vary across cited sources,
revealing that community-driven platforms are generally preferred and static
encyclopedic sources are not always appropriate and reliable. To assess
performance across different settings, we conduct cross-arena analyses by
testing search-augmented LLMs in a general-purpose chat environment and
conventional LLMs in search-intensive settings. We find that web search does
not degrade and may even improve performance in non-search settings; however,
the quality in search settings is significantly affected if solely relying on
the model's parametric knowledge. We open-sourced the dataset to support future
research in this direction. Our dataset and code are available at:
https://github.com/lmarena/search-arena.","搜索增强的语言模型结合了网络搜索和大型语言模型（LLMs），以提高响应的基础和新鲜度。然而，分析这些系统仍然具有挑战性：现有数据集在规模和范围上有限，通常受到静态、单次、事实核查问题的限制。在本工作中，我们引入了Search Arena，一个包含超过24,000对多轮用户与搜索增强LLMs交互的大规模、人类偏好数据集。该数据集涵盖了多种意图和语言，并包含了约12,000个人类偏好投票的完整系统跟踪。我们的分析揭示了用户偏好受引用数量的影响，即使引用的内容并不直接支持归因的声明，也揭示了感知和实际可信度之间的差距。此外，用户偏好在引用来源之间有所不同，揭示了社区驱动的平台通常更受欢迎，而静态百科来源并不总是合适和可靠。为了在不同设置中评估性能，我们通过在通用聊天环境中测试搜索增强LLMs，并在搜索密集型设置中测试传统LLMs，进行跨场地分析。我们发现，网络搜索不会降低性能，甚至可能在非搜索设置中提高性能；然而，如果仅依赖模型的参数知识，搜索设置中的质量会显著受到影响。我们开源了该数据集，以支持未来在这个方向上的研究。我们的数据集和代码可在以下网址找到：https://github.com/lmarena/search-arena。","The paper introduces Search Arena, a large-scale dataset for analyzing search-augmented LLMs, focusing on user preferences and the credibility of cited sources.",LLM,"Helpful, Honest","Search-augmented LLMs, user preferences, credibility, multi-turn interactions, human-preference dataset"
"Context is Key: A Benchmark for Forecasting with Essential Textual
  Information","Andrew Robert Williams, Arjun Ashok, Étienne Marcotte, Valentina Zantedeschi, Jithendaraa Subramanian, Roland Riachi, James Requeima, Alexandre Lacoste, Irina Rish, Nicolas Chapados, Alexandre Drouin",2024-10-24T17:56:08Z,http://arxiv.org/pdf/2410.18959v4,"Forecasting is a critical task in decision-making across numerous domains.
While historical numerical data provide a start, they fail to convey the
complete context for reliable and accurate predictions. Human forecasters
frequently rely on additional information, such as background knowledge and
constraints, which can efficiently be communicated through natural language.
However, in spite of recent progress with LLM-based forecasters, their ability
to effectively integrate this textual information remains an open question. To
address this, we introduce ""Context is Key"" (CiK), a time-series forecasting
benchmark that pairs numerical data with diverse types of carefully crafted
textual context, requiring models to integrate both modalities; crucially,
every task in CiK requires understanding textual context to be solved
successfully. We evaluate a range of approaches, including statistical models,
time series foundation models, and LLM-based forecasters, and propose a simple
yet effective LLM prompting method that outperforms all other tested methods on
our benchmark. Our experiments highlight the importance of incorporating
contextual information, demonstrate surprising performance when using LLM-based
forecasting models, and also reveal some of their critical shortcomings. This
benchmark aims to advance multimodal forecasting by promoting models that are
both accurate and accessible to decision-makers with varied technical
expertise. The benchmark can be visualized at
https://servicenow.github.io/context-is-key-forecasting/v0/.",预测是决策制定中许多领域的关键任务。虽然历史数值数据提供了一个起点，但它们无法传达可靠和准确预测所需的完整上下文。人类预测者经常依赖额外的信息，例如背景知识和约束，这些信息可以通过自然语言高效地传达。然而，尽管最近在基于LLM的预测者方面取得了进展，但它们有效整合这种文本信息的能力仍然是一个开放的问题。为了解决这个问题，我们引入了“Context is Key”（CiK），这是一个将数值数据与多种类型的精心设计的文本上下文配对的时间序列预测基准，要求模型整合两种模态；关键是，CiK中的每个任务都需要理解文本上下文才能成功解决。我们评估了一系列方法，包括统计模型、时间序列基础模型和基于LLM的预测者，并提出了一种简单但有效的LLM提示方法，在我们的基准上优于所有其他测试方法。我们的实验突显了纳入上下文信息的重要性，展示了使用基于LLM的预测模型时令人惊讶的性能，并揭示了一些它们的关键不足。该基准旨在通过促进准确且易于各种技术专业决策者使用的模型来推动多模态预测。,The paper introduces a benchmark for evaluating LLMs in forecasting tasks that require integrating numerical data with textual context.,LLM,Helpful,"Forecasting, Textual Information, LLM, Multimodal, Benchmark"
"On the Mechanism of Reasoning Pattern Selection in Reinforcement
  Learning for Language Models","Xingwu Chen, Tianle Li, Difan Zou",2025-06-05T07:17:04Z,http://arxiv.org/pdf/2506.04695v1,"Reinforcement learning (RL) has demonstrated remarkable success in enhancing
model capabilities, including instruction-following, preference learning, and
reasoning. Yet despite its empirical successes, the mechanisms by which RL
improves reasoning abilities remain poorly understood. We present a systematic
study of Reinforcement Learning with Verifiable Rewards (RLVR), showing that
its primary benefit comes from optimizing the selection of existing reasoning
patterns. Through extensive experiments, we demonstrate that RLVR-trained
models preferentially adopt high-success-rate reasoning patterns while mostly
maintaining stable performance on individual patterns. We further develop
theoretical analyses on the convergence and training dynamics of RLVR based on
a simplified question-reason-answer model. We study the gradient flow and show
that RLVR can indeed find the solution that selects the reason pattern with the
highest success rate. Besides, our theoretical results
  reveal two distinct regimes regarding the convergence of RLVR training: (1)
rapid convergence for models with relatively strong initial reasoning
capabilities versus (2) slower optimization dynamics for weaker models.
Furthermore, we show that the slower optimization for weaker models can be
mitigated by applying the supervised fine-tuning (SFT) before RLVR, when using
a feasibly high-quality SFT dataset. We validate the theoretical findings
through extensive experiments. This work advances our theoretical understanding
of RL's role in LLM fine-tuning and offers insights for further enhancing
reasoning capabilities.",强化学习（RL）在增强模型能力方面取得了显著成功，包括遵循指令、偏好学习和推理。然而，尽管其经验成功，RL改善推理能力的机制仍然不太清楚。我们提出了一项系统研究，研究可验证奖励的强化学习（RLVR），表明其主要好处在于优化现有推理模式的选择。通过广泛的实验，我们证明了RLVR训练的模型优先采用高成功率的推理模式，同时在个别模式上保持稳定的性能。我们进一步开发了基于简化的问题-推理-回答模型的理论分析，研究梯度流并表明RLVR确实可以找到选择成功率最高的推理模式的解决方案。此外，我们的理论结果揭示了RLVR训练收敛的两种不同模式：(1) 具有相对强初始推理能力的模型的快速收敛与(2) 更弱模型的较慢优化动态。此外，我们还表明，在使用可行的高质量SFT数据集时，可以通过在RLVR之前应用监督微调（SFT）来缓解较弱模型的较慢优化。我们通过广泛的实验验证了理论发现。这项工作推动了我们对RL在LLM微调中的理论理解，并为进一步增强推理能力提供了见解。,The paper investigates how reinforcement learning improves reasoning in language models by optimizing the selection of reasoning patterns.,LLM,Helpful,"Reinforcement Learning, Reasoning, Language Models, Alignment, Fine-tuning"
"Sample Complexity and Representation Ability of Test-time Scaling
  Paradigms","Baihe Huang, Shanda Li, Tianhao Wu, Yiming Yang, Ameet Talwalkar, Kannan Ramchandran, Michael I. Jordan, Jiantao Jiao",2025-06-05T17:48:19Z,http://arxiv.org/pdf/2506.05295v1,"Test-time scaling paradigms have significantly advanced the capabilities of
large language models (LLMs) on complex tasks. Despite their empirical success,
theoretical understanding of the sample efficiency of various test-time
strategies -- such as self-consistency, best-of-$n$, and self-correction --
remains limited. In this work, we first establish a separation result between
two repeated sampling strategies: self-consistency requires
$\Theta(1/\Delta^2)$ samples to produce the correct answer, while best-of-$n$
only needs $\Theta(1/\Delta)$, where $\Delta < 1$ denotes the probability gap
between the correct and second most likely answers. Next, we present an
expressiveness result for the self-correction approach with verifier feedback:
it enables Transformers to simulate online learning over a pool of experts at
test time. Therefore, a single Transformer architecture can provably solve
multiple tasks without prior knowledge of the specific task associated with a
user query, extending the representation theory of Transformers from
single-task to multi-task settings. Finally, we empirically validate our
theoretical results, demonstrating the practical effectiveness of
self-correction methods.",测试时缩放范式显著提升了大型语言模型（LLMs）在复杂任务上的能力。尽管它们在实证上取得了成功，但对各种测试时策略（如自我一致性、最佳的-n和自我纠正）的样本效率的理论理解仍然有限。在本文中，我们首先建立了两种重复采样策略之间的分离结果：自我一致性需要 $\Theta(1/\Delta^2)$ 样本以产生正确答案，而最佳的-n 只需要 $\Theta(1/\Delta)$，其中 $\Delta < 1$ 表示正确答案和第二可能答案之间的概率差。接下来，我们为具有验证器反馈的自我纠正方法提出了一个表达能力结果：它使得 Transformer 能够在测试时模拟在专家池中进行在线学习。因此，单个 Transformer 架构可以在没有先验知识的情况下证明解决多个任务，将 Transformer 的表示理论从单任务扩展到多任务设置。最后，我们通过实证验证了我们的理论结果，证明了自我纠正方法的实际有效性。,The paper analyzes the sample efficiency of test-time scaling strategies for large language models and demonstrates the effectiveness of self-correction methods.,LLM,None,"Test-time scaling, Self-consistency, Best-of-n, Self-correction, Transformers"
The Impossibility of Fair LLMs,"Jacy Anthis, Kristian Lum, Michael Ekstrand, Avi Feller, Chenhao Tan",2024-05-28T04:36:15Z,http://arxiv.org/pdf/2406.03198v2,"The rise of general-purpose artificial intelligence (AI) systems,
particularly large language models (LLMs), has raised pressing moral questions
about how to reduce bias and ensure fairness at scale. Researchers have
documented a sort of ""bias"" in the significant correlations between
demographics (e.g., race, gender) in LLM prompts and responses, but it remains
unclear how LLM fairness could be evaluated with more rigorous definitions,
such as group fairness or fair representations. We analyze a variety of
technical fairness frameworks and find inherent challenges in each that make
the development of a fair LLM intractable. We show that each framework either
does not logically extend to the general-purpose AI context or is infeasible in
practice, primarily due to the large amounts of unstructured training data and
the many potential combinations of human populations, use cases, and sensitive
attributes. These inherent challenges would persist for general-purpose AI,
including LLMs, even if empirical challenges, such as limited participatory
input and limited measurement methods, were overcome. Nonetheless, fairness
will remain an important type of model evaluation, and there are still
promising research directions, particularly the development of standards for
the responsibility of LLM developers, context-specific evaluations, and methods
of iterative, participatory, and AI-assisted evaluation that could scale
fairness across the diverse contexts of modern human-AI interaction.",通用人工智能（AI）系统，特别是大型语言模型（LLMs），的兴起引发了关于如何在大规模上减少偏见和确保公平性的紧迫道德问题。研究人员记录了LLM提示和响应中与人口统计学（例如，种族、性别）相关的一种“偏见”，但尚不清楚如何用更严格的定义（例如，群体公平性或公平表示）来评估LLM的公平性。我们分析了各种技术公平框架，发现每个框架都存在内在挑战，使得开发一个公平的LLM变得不可行。我们表明，每个框架要么在逻辑上不能扩展到通用AI上下文，要么在实践中不可行，主要是由于大量的非结构化训练数据和许多潜在的人口组合、用例和敏感属性。这些内在挑战将持续存在于通用AI中，包括LLMs，即使克服了经验挑战，例如有限的参与输入和有限的测量方法。尽管如此，公平性仍将是一种重要的模型评估类型，并且仍有许多有前途的研究方向，特别是开发LLM开发者责任的标准、特定上下文的评估以及可以在现代人机交互的多样化上下文中扩展公平性的迭代、参与和AI辅助评估方法。,The paper explores the challenges and potential directions for ensuring fairness in large language models.,LLM,"Helpful, Harmless","Fairness, Bias, Large Language Models, Evaluation, Responsibility"
"SECNEURON: Reliable and Flexible Abuse Control in Local LLMs via Hybrid
  Neuron Encryption","Zhiqiang Wang, Haohua Du, Junyang Wang, Haifeng Sun, Kaiwen Guo, Haikuo Yu, Chao Liu, Xiang-Yang Li",2025-06-05T17:01:28Z,http://arxiv.org/pdf/2506.05242v1,"Large language models (LLMs) with diverse capabilities are increasingly being
deployed in local environments, presenting significant security and
controllability challenges. These locally deployed LLMs operate outside the
direct control of developers, rendering them more susceptible to abuse.
Existing mitigation techniques mainly designed for cloud-based LLM services are
frequently circumvented or ineffective in deployer-controlled environments. We
propose SECNEURON, the first framework that seamlessly embeds classic access
control within the intrinsic capabilities of LLMs, achieving reliable,
cost-effective, flexible, and certified abuse control for local deployed LLMs.
SECNEURON employs neuron-level encryption and selective decryption to
dynamically control the task-specific capabilities of LLMs, limiting
unauthorized task abuse without compromising others. We first design a
task-specific neuron extraction mechanism to decouple logically related neurons
and construct a layered policy tree for handling coupled neurons. We then
introduce a flexible and efficient hybrid encryption framework for millions of
neurons in LLMs. Finally, we developed a distribution-based decrypted neuron
detection mechanism on ciphertext to ensure the effectiveness of partially
decrypted LLMs. We proved that SECNEURON satisfies IND-CPA Security and
Collusion Resistance Security under the Task Controllability Principle.
Experiments on various task settings show that SECNEURON limits unauthorized
task accuracy to below 25% while keeping authorized accuracy loss with 2%.
Using an unauthorized Code task example, the accuracy of abuse-related
malicious code generation was reduced from 59% to 15%. SECNEURON also mitigates
unauthorized data leakage, reducing PII extraction rates to below 5% and
membership inference to random guesses.",大型语言模型（LLMs）具有多种能力，越来越多地在本地环境中部署，带来了显著的安全性和可控性挑战。这些本地部署的LLMs在开发者的直接控制之外运行，使其更容易受到滥用。现有的缓解技术主要针对基于云的LLM服务，在部署者控制的环境中经常被绕过或无效。我们提出了SECNEURON，这是第一个无缝嵌入经典访问控制到LLMs内在能力的框架，实现了可靠、成本效益高、灵活和经过认证的滥用控制，适用于本地部署的LLMs。SECNEURON采用神经元级加密和选择性解密，动态控制LLMs的特定任务能力，限制未经授权的任务滥用，而不影响其他任务。我们首先设计了一个特定任务的神经元提取机制，将逻辑相关的神经元解耦，并构建了一个分层策略树来处理耦合的神经元。然后，我们引入了一个灵活且高效的混合加密框架，用于处理LLMs中的数百万个神经元。最后，我们开发了一种基于分布的解密神经元检测机制，以确保部分解密LLMs的有效性。我们证明了SECNEURON在任务可控性原则下满足IND-CPA安全性和合谋抵抗安全性。在各种任务设置下的实验表明，SECNEURON将未经授权的任务准确率限制在25%以下，同时保持授权准确率损失在2%。使用未经授权的代码任务示例，与恶意代码生成相关的滥用准确率从59%降至15%。SECNEURON还缓解了未经授权的数据泄露，将PII提取率降至5%以下，并将成员推断降至随机猜测。,"The paper introduces SECNEURON, a framework for secure and flexible abuse control in locally deployed large language models through neuron-level encryption.",LLM,Harmless,"LLM security, abuse control, neuron encryption, task-specific control, local deployment"
CheckEmbed: Effective Verification of LLM Solutions to Open-Ended Tasks,"Maciej Besta, Lorenzo Paleari, Marcin Copik, Robert Gerstenberger, Ales Kubicek, Piotr Nyczyk, Patrick Iff, Eric Schreiber, Tanja Srindran, Tomasz Lehmann, Hubert Niewiadomski, Torsten Hoefler",2024-06-04T17:42:21Z,http://arxiv.org/pdf/2406.02524v4,"Large Language Models (LLMs) are transforming a wide range of domains, yet
verifying their outputs remains a significant challenge, especially for complex
open-ended tasks such as consolidation, summarization, and knowledge
extraction. To address this, we introduce CheckEmbed (CE): a simple, scalable,
and accurate verification method. CE reduces each LLM answer to a single
embedding vector using powerful modern embedding LLM models like
SFR-Embedding-Mistral. Prior methods such as BERTScore and SelfCheckGPT relied
on weaker encoders like BERT, forcing them to operate at token or sentence
granularity. In contrast, CE performs fast, semantically rich comparisons
directly at the whole-answer level, overcoming key limitations in both accuracy
and scalability. We conduct a comprehensive design and time complexity analysis
across 13 verification baselines, including classical text scorers (e.g.,
BLEU), stability-based methods (e.g., SelfCheckGPT), and generative evaluators
(e.g., LLM-as-a-Judge), which highlights the effectiveness, efficiency,
versatility, and simplicity of CE. Empirical results show that CE reliably
detects hallucinations in both closed and open-ended tasks. We further present
evidence that CE generalizes beyond text to other modalities such as vision,
establishing it as a practical and versatile verification framework.",大语言模型（LLMs）正在改变各种领域，但验证其输出仍然是一个重要的挑战，特别是对于复杂的开放性任务，如整合、摘要和知识提取。为了解决这个问题，我们引入了CheckEmbed（CE）：一种简单、可扩展且准确的验证方法。CE使用强大的现代嵌入LLM模型（如SFR-Embedding-Mistral）将每个LLM答案简化为一个嵌入向量。之前的方法，如BERTScore和SelfCheckGPT，依赖于较弱的编码器（如BERT），迫使它们在标记或句子粒度上运行。相比之下，CE在整个答案级别上执行快速、语义丰富的比较，克服了准确性和可扩展性方面的关键限制。我们在13个验证基线上进行了全面的设计和时间复杂度分析，包括经典文本评分器（例如，BLEU）、基于稳定性的方法（例如，SelfCheckGPT）和生成评估器（例如，LLM-as-a-Judge），这突显了CE的有效性、高效性、多功能性和简单性。实证结果表明，CE可靠地检测到封闭和开放任务中的幻觉。我们还提供了证据，表明CE可以超越文本，适用于其他模态，如视觉，从而成为一个实用且多功能的验证框架。,"The paper introduces CheckEmbed, a method for verifying LLM outputs to ensure they are helpful and honest by detecting hallucinations in both closed and open-ended tasks.",LLM,"Helpful, Honest","Verification, LLM outputs, Hallucinations, Embedding, Open-ended tasks"
FastDraft: How to Train Your Draft,"Ofir Zafrir, Igor Margulis, Dorin Shteyman, Shira Guskin, Guy Boudoukh",2024-11-17T12:32:44Z,http://arxiv.org/pdf/2411.11055v3,"Speculative Decoding has gained popularity as an effective technique for
accelerating the auto-regressive inference process of Large Language Models.
However, Speculative Decoding entirely relies on the availability of efficient
draft models, which are often lacking for many existing language models due to
a stringent constraint of vocabulary compatibility. In this work we introduce
FastDraft, a novel and efficient approach for pre-training and aligning a draft
model to any large language model by incorporating efficient pre-training,
followed by fine-tuning over synthetic datasets generated by the target model.
We demonstrate FastDraft by training two highly parameter efficient drafts for
the popular Phi-3-mini and Llama-3.1-8B models. Using FastDraft, we were able
to produce a draft model with approximately 10 billion tokens on a single
server with 8 Intel$^\circledR$ Gaudi$^\circledR$ 2 accelerators in under 24
hours. Our results show that the draft model achieves impressive results in key
metrics of acceptance rate, block efficiency and up to 3x memory bound speed up
when evaluated on code completion and up to 2x in summarization, text
completion and instruction tasks. We validate our theoretical findings through
benchmarking on the latest Intel$^\circledR$ Core$^{\tiny \text{TM}}$ Ultra,
achieving a wall-clock time speedup of up to 2x, indicating a significant
reduction in runtime. Due to its high quality, FastDraft unlocks large language
models inference on AI-PC and other edge-devices.",推测解码作为一种有效的技术，已经在加速大型语言模型的自回归推理过程中获得了广泛的关注。然而，推测解码完全依赖于高效的草稿模型的可用性，而这些模型在许多现有语言模型中往往缺乏，因为词汇兼容性的严格约束。在本文中，我们引入了FastDraft，一种通过高效的预训练，然后在目标模型生成的合成数据集上进行微调来预训练和对齐草稿模型的新颖且高效的方法。我们通过训练两个高度参数高效的草稿模型来展示FastDraft，分别为流行的Phi-3-mini和Llama-3.1-8B模型。使用FastDraft，我们能够在单个服务器上生成一个包含约100亿个标记的草稿模型，使用8个Intel$^\circledR$ Gaudi$^\circledR$ 2加速器，时间不到24小时。我们的结果表明，草稿模型在接受率、块效率和内存绑定速度提升方面取得了令人印象深刻的结果，在代码完成方面评估为高达3倍，在总结、文本完成和指令任务方面评估为高达2倍。我们通过在最新的Intel$^\circledR$ Core$^{\tiny \text{TM}}$ Ultra上进行基准测试，验证了我们的理论发现，实现了高达2倍的时钟时间加速，表明运行时间显著减少。由于其高质量，FastDraft解锁了大型语言模型在AI-PC和其他边缘设备上的推理。,"The paper introduces FastDraft, a method for efficiently aligning draft models to large language models, improving inference speed and enabling edge device deployment.",LLM,Helpful,"Speculative Decoding, Draft Model, Alignment, Efficient Inference, Large Language Models"
"Distilling the Implicit Multi-Branch Structure in LLMs' Reasoning via
  Reinforcement Learning","Shicheng Xu, Liang Pang, Yunchang Zhu, Jia Gu, Zihao Wei, Jingcheng Deng, Feiyang Pan, Huawei Shen, Xueqi Cheng",2025-05-22T02:36:36Z,http://arxiv.org/pdf/2505.16142v2,"Distilling reasoning paths from teacher to student models via supervised
fine-tuning (SFT) provides a shortcut for improving the reasoning ability of
smaller Large Language Models (LLMs). However, the reasoning paths generated by
teacher models often reflect only surface-level traces of their underlying
authentic reasoning. Insights from cognitive neuroscience suggest that
authentic reasoning involves a complex interweaving between meta-reasoning
(which selects appropriate sub-problems from multiple candidates) and solving
(which addresses the sub-problem). This implies authentic reasoning has an
implicit multi-branch structure. Supervised fine-tuning collapses this rich
structure into a flat sequence of token prediction in the teacher's reasoning
path, preventing effective distillation of this structure to students. To
address this limitation, we propose RLKD, a reinforcement learning (RL)-based
distillation framework guided by a novel Generative Structure Reward Model
(GSRM). Our GSRM converts reasoning paths into multiple meta-reasoning-solving
steps and computes rewards to measure structural alignment between student and
teacher reasoning. RLKD combines this reward with RL, enabling student LLMs to
internalize the teacher's implicit multi-branch reasoning structure rather than
merely mimicking fixed output paths. Experiments show RLKD surpasses standard
SFT-RL pipelines even when trained on 0.1% of data under an RL-only regime,
unlocking greater student reasoning potential than SFT-based distillation.",通过强化学习（RL）基于的蒸馏框架，从教师模型中提取推理路径，可以提高较小的大型语言模型（LLMs）的推理能力。然而，教师模型生成的推理路径通常只反映其底层真实推理的表面痕迹。认知神经科学的见解表明，真实的推理涉及元推理（从多个候选项中选择合适的子问题）和解决（解决子问题）之间的复杂交织。这表明真实的推理具有隐含的多分支结构。监督微调将这种丰富的结构压缩为教师推理路径中的平坦的令牌预测序列，阻止有效地将这种结构蒸馏到学生中。为了解决这个局限性，我们提出了RLKD，一个由新颖的生成结构奖励模型（GSRM）指导的强化学习（RL）基于的蒸馏框架。我们的GSRM将推理路径转换为多个元推理-解决步骤，并计算奖励以衡量学生和教师推理之间的结构对齐。RLKD将此奖励与RL结合，使学生LLMs能够内化教师的隐含多分支推理结构，而不是仅仅模仿固定的输出路径。实验表明，即使在仅使用0.1%的数据进行RL-only训练的情况下，RLKD也超过了标准的SFT-RL管道，释放了比基于SFT的蒸馏更大的学生推理潜力。,"The paper introduces RLKD, a reinforcement learning-based framework for distilling the implicit multi-branch reasoning structure from teacher to student LLMs, enhancing their reasoning capabilities.",LLM,Helpful,"Reinforcement Learning, Distillation, Reasoning, Alignment, LLMs"
"MMBoundary: Advancing MLLM Knowledge Boundary Awareness through
  Reasoning Step Confidence Calibration","Zhitao He, Sandeep Polisetty, Zhiyuan Fan, Yuchen Huang, Shujin Wu, Yi R. Fung",2025-05-29T08:14:40Z,http://arxiv.org/pdf/2505.23224v2,"In recent years, multimodal large language models (MLLMs) have made
significant progress but continue to face inherent challenges in multimodal
reasoning, which requires multi-level (e.g., perception, reasoning) and
multi-granular (e.g., multi-step reasoning chain) advanced inferencing. Prior
work on estimating model confidence tends to focus on the overall response for
training and calibration, but fails to assess confidence in each reasoning
step, leading to undesirable hallucination snowballing. In this work, we
present MMBoundary, a novel framework that advances the knowledge boundary
awareness of MLLMs through reasoning step confidence calibration. To achieve
this, we propose to incorporate complementary textual and cross-modal
self-rewarding signals to estimate confidence at each step of the MLLM
reasoning process. In addition to supervised fine-tuning MLLM on this set of
self-rewarded confidence estimation signal for initial confidence expression
warm-up, we introduce a reinforcement learning stage with multiple reward
functions for further aligning model knowledge and calibrating confidence at
each reasoning step, enhancing reasoning chain self-correction. Empirical
results show that MMBoundary significantly outperforms existing methods across
diverse domain datasets and metrics, achieving an average of 7.5% reduction in
multimodal confidence calibration errors and up to 8.3% improvement in task
performance.",近年来，多模态大语言模型（MLLMs）在多模态推理方面取得了显著进展，但仍面临多层次和多粒度推理的内在挑战。此前的工作主要集中在估计模型整体响应的置信度，而忽略了每个推理步骤的置信度评估，导致不良的幻觉雪球效应。本文提出了MMBoundary框架，通过推理步骤置信度校准来提高MLLMs的知识边界意识。为此，我们提出将补充的文本和跨模态自我奖励信号纳入每个MLLM推理步骤的置信度估计。除了在自我奖励置信度估计信号上对MLLM进行监督微调，以进行初始置信度表达预热，我们还引入了具有多个奖励函数的强化学习阶段，以进一步对齐模型知识并校准每个推理步骤的置信度，增强推理链自我纠正。实验结果表明，MMBoundary在多个领域数据集和指标上显著优于现有方法，平均减少了7.5%的多模态置信度校准误差，并提高了高达8.3%的任务性能。,"The paper introduces MMBoundary, a framework that enhances the knowledge boundary awareness of MLLMs through reasoning step confidence calibration, improving task performance and reducing confidence calibration errors.",MLLM,"Helpful, Honest","Multimodal, Confidence Calibration, Reasoning Steps, Alignment, MLLM"
"Adaptive Jailbreaking Strategies Based on the Semantic Understanding
  Capabilities of Large Language Models","Mingyu Yu, Wei Wang, Yanjie Wei, Sujuan Qin",2025-05-29T12:50:57Z,http://arxiv.org/pdf/2505.23404v2,"Adversarial attacks on Large Language Models (LLMs) via jailbreaking
techniques-methods that circumvent their built-in safety and ethical
constraints-have emerged as a critical challenge in AI security. These attacks
compromise the reliability of LLMs by exploiting inherent weaknesses in their
comprehension capabilities. This paper investigates the efficacy of
jailbreaking strategies that are specifically adapted to the diverse levels of
understanding exhibited by different LLMs. We propose the Adaptive Jailbreaking
Strategies Based on the Semantic Understanding Capabilities of Large Language
Models, a novel framework that classifies LLMs into Type I and Type II
categories according to their semantic comprehension abilities. For each
category, we design tailored jailbreaking strategies aimed at leveraging their
vulnerabilities to facilitate successful attacks. Extensive experiments
conducted on multiple LLMs demonstrate that our adaptive strategy markedly
improves the success rate of jailbreaking. Notably, our approach achieves an
exceptional 98.9% success rate in jailbreaking GPT-4o(29 May 2025 release)",对大型语言模型（LLM）的对抗攻击通过越狱技术——绕过其内置的安全和伦理约束的方法——已经成为人工智能安全中的一个关键挑战。这些攻击通过利用其理解能力中的固有弱点，破坏了LLM的可靠性。本文研究了特别适应不同LLM展示的理解水平的越狱策略的有效性。我们提出了基于大型语言模型语义理解能力的自适应越狱策略，一种新颖的框架，根据其语义理解能力将LLM分类为I型和II型类别。对于每个类别，我们设计了定制的越狱策略，旨在利用其漏洞以促进成功的攻击。在多个LLM上进行的广泛实验表明，我们的自适应策略显著提高了越狱的成功率。值得注意的是，我们的方法在越狱GPT-4o（2025年5月29日发布）时取得了98.9%的异常成功率。,"The paper presents a novel framework for adaptive jailbreaking strategies tailored to the semantic understanding capabilities of different LLMs, achieving a high success rate in bypassing safety constraints.",LLM,Harmless,"Jailbreaking, LLM, Security, Adaptive Strategies, Semantic Understanding"
ValueSim: Generating Backstories to Model Individual Value Systems,"Bangde Du, Ziyi Ye, Zhijing Wu, Jankowska Monika, Shuqi Zhu, Qingyao Ai, Yujia Zhou, Yiqun Liu",2025-05-28T06:43:16Z,http://arxiv.org/pdf/2505.23827v2,"As Large Language Models (LLMs) continue to exhibit increasingly human-like
capabilities, aligning them with human values has become critically important.
Contemporary advanced techniques, such as prompt learning and reinforcement
learning, are being deployed to better align LLMs with human values. However,
while these approaches address broad ethical considerations and helpfulness,
they rarely focus on simulating individualized human value systems. To address
this gap, we present ValueSim, a framework that simulates individual values
through the generation of personal backstories reflecting past experiences and
demographic information. ValueSim converts structured individual data into
narrative backstories and employs a multi-module architecture inspired by the
Cognitive-Affective Personality System to simulate individual values based on
these narratives. Testing ValueSim on a self-constructed benchmark derived from
the World Values Survey demonstrates an improvement in top-1 accuracy by over
10% compared to retrieval-augmented generation methods. Further analysis
reveals that performance enhances as additional user interaction history
becomes available, indicating the model's ability to refine its persona
simulation capabilities over time.",随着大型语言模型（LLM）展示出越来越多的人类似能力，将它们与人类价值观对齐变得至关重要。当代先进技术，如提示学习和强化学习，正在被部署以更好地将LLM与人类价值观对齐。然而，虽然这些方法解决了广泛的伦理考虑和有用性，但它们很少关注模拟个性化的人类价值系统。为了填补这一空白，我们提出了ValueSim，一个通过生成反映过去经历和人口统计信息的个人背景故事来模拟个体价值的框架。ValueSim将结构化的个体数据转换为叙述性背景故事，并采用受认知-情感人格系统启发的多模块架构，基于这些叙述模拟个体价值。在基于世界价值调查的自建基准上测试ValueSim，显示与检索增强生成方法相比，顶级1准确性提高了10%以上。进一步分析表明，随着更多用户交互历史的可用性，性能得到了提高，表明模型能够随着时间的推移改善其人格模拟能力。,"The paper introduces ValueSim, a framework that generates personal backstories to align LLMs with individual human values.",LLM,Helpful,"Value alignment, personalization, backstory generation, individual values, LLM"
"Revisiting Test-Time Scaling: A Survey and a Diversity-Aware Method for
  Efficient Reasoning","Ho-Lam Chung, Teng-Yun Hsiao, Hsiao-Ying Huang, Chunerh Cho, Jian-Ren Lin, Zhang Ziwei, Yun-Nung Chen",2025-06-05T04:02:17Z,http://arxiv.org/pdf/2506.04611v1,"Test-Time Scaling (TTS) improves the reasoning performance of Large Language
Models (LLMs) by allocating additional compute during inference. We conduct a
structured survey of TTS methods and categorize them into sampling-based,
search-based, and trajectory optimization strategies. We observe that
reasoning-optimized models often produce less diverse outputs, which limits TTS
effectiveness. To address this, we propose ADAPT (A Diversity Aware Prefix
fine-Tuning), a lightweight method that applies prefix tuning with a
diversity-focused data strategy. Experiments on mathematical reasoning tasks
show that ADAPT reaches 80% accuracy using eight times less compute than strong
baselines. Our findings highlight the essential role of generative diversity in
maximizing TTS effectiveness.",测试时刻缩放（TTS）通过在推理期间分配额外的计算资源来提高大型语言模型（LLMs）的推理性能。我们对TTS方法进行了结构化调查，并将其分类为基于采样、基于搜索和轨迹优化策略。我们观察到，优化推理的模型通常产生较少多样性的输出，这限制了TTS的有效性。为了解决这个问题，我们提出了ADAPT（一种面向多样性的前缀微调），这是一种轻量级方法，它结合了前缀微调和面向多样性的数据策略。在数学推理任务上的实验表明，ADAPT在使用八倍少的计算资源的情况下达到80%的准确率。我们的发现强调了生成多样性在最大化TTS有效性中的重要作用。,"The paper introduces ADAPT, a method to enhance the reasoning performance of LLMs by promoting generative diversity during test-time scaling.",LLM,Helpful,"Test-Time Scaling, Large Language Models, Reasoning, Diversity, Prefix Tuning"
Normative Conflicts and Shallow AI Alignment,Raphaël Millière,2025-06-05T06:57:28Z,http://arxiv.org/pdf/2506.04679v1,"The progress of AI systems such as large language models (LLMs) raises
increasingly pressing concerns about their safe deployment. This paper examines
the value alignment problem for LLMs, arguing that current alignment strategies
are fundamentally inadequate to prevent misuse. Despite ongoing efforts to
instill norms such as helpfulness, honesty, and harmlessness in LLMs through
fine-tuning based on human preferences, they remain vulnerable to adversarial
attacks that exploit conflicts between these norms. I argue that this
vulnerability reflects a fundamental limitation of existing alignment methods:
they reinforce shallow behavioral dispositions rather than endowing LLMs with a
genuine capacity for normative deliberation. Drawing from on research in moral
psychology, I show how humans' ability to engage in deliberative reasoning
enhances their resilience against similar adversarial tactics. LLMs, by
contrast, lack a robust capacity to detect and rationally resolve normative
conflicts, leaving them susceptible to manipulation; even recent advances in
reasoning-focused LLMs have not addressed this vulnerability. This ``shallow
alignment'' problem carries significant implications for AI safety and
regulation, suggesting that current approaches are insufficient for mitigating
potential harms posed by increasingly capable AI systems.",大语言模型（LLM）的进步引发了关于其安全部署的日益紧迫的关注。本文研究了LLM的价值对齐问题，认为当前的对齐策略在防止滥用方面是根本不足的。尽管通过基于人类偏好的微调，努力在LLM中灌输有用性、诚实性和无害性等规范，但它们仍然容易受到利用这些规范之间冲突的对抗性攻击。我论证了这种易受攻击性反映了现有对齐方法的根本局限性：它们强化了浅层的行为倾向，而不是赋予LLM进行规范性思考的真正能力。通过研究道德心理学，我展示了人类进行深思熟虑的推理能力如何增强他们对类似对抗策略的抵抗力。相比之下，LLM缺乏检测和理性解决规范冲突的强大能力，使其容易受到操纵；即使是最近在推理方面的LLM进展也没有解决这个问题。这个“浅层对齐”问题对AI安全和监管具有重要意义，表明当前的方法不足以缓解日益强大的AI系统可能带来的潜在危害。,"The paper argues that current alignment methods for LLMs are insufficient for ensuring they are helpful, harmless, and honest, as they lack the capacity for genuine normative deliberation.",LLM,"Helpful, Harmless, Honest","LLM alignment, normative conflicts, AI safety, moral psychology, adversarial attacks"
"SPARTA ALIGNMENT: Collectively Aligning Multiple Language Models through
  Combat","Yuru Jiang, Wenxuan Ding, Shangbin Feng, Greg Durrett, Yulia Tsvetkov",2025-06-05T07:51:23Z,http://arxiv.org/pdf/2506.04721v1,"We propose SPARTA ALIGNMENT, an algorithm to collectively align multiple LLMs
through competition and combat. To complement a single model's lack of
diversity in generation and biases in evaluation, multiple LLMs form a ""sparta
tribe"" to compete against each other in fulfilling instructions while serving
as judges for the competition of others. For each iteration, one instruction
and two models are selected for a duel, the other models evaluate the two
responses, and their evaluation scores are aggregated through a adapted
elo-ranking based reputation system, where winners/losers of combat gain/lose
weight in evaluating others. The peer-evaluated combat results then become
preference pairs where the winning response is preferred over the losing one,
and all models learn from these preferences at the end of each iteration.
SPARTA ALIGNMENT enables the self-evolution of multiple LLMs in an iterative
and collective competition process. Extensive experiments demonstrate that
SPARTA ALIGNMENT outperforms initial models and 4 self-alignment baselines
across 10 out of 12 tasks and datasets with 7.0% average improvement. Further
analysis reveals that SPARTA ALIGNMENT generalizes more effectively to unseen
tasks and leverages the expertise diversity of participating models to produce
more logical, direct and informative outputs.",我们提出了SPARTA对齐算法，通过竞争和战斗来集体对齐多个LLM。为了弥补单个模型在生成中的多样性不足和评估中的偏见，多个LLM形成一个“斯巴达部落”，相互竞争以完成指令，同时作为他人竞争的评委。在每次迭代中，选择一个指令和两个模型进行决斗，其他模型评估两个响应，并通过适应的Elo排名声誉系统聚合评分，决斗的胜者/负者在评估他人时获得/失去权重。同行评估的战斗结果成为偏好对，其中获胜的响应优于失败的响应，所有模型都从这些偏好中学习。SPARTA对齐使多个LLM在迭代和集体竞争过程中自我进化。广泛的实验表明，SPARTA对齐在12个任务和数据集中的10个任务中超过了初始模型和4个自对齐基线，平均提高了7.0%。进一步的分析揭示了SPARTA对齐更有效地推广到未见过的任务，并利用参与模型的专业知识多样性，以生成更合乎逻辑、直接和信息丰富的输出。,"The paper introduces SPARTA ALIGNMENT, a method for collectively aligning multiple LLMs through competitive interactions, improving their performance and logical output.",LLM,Helpful,"LLM alignment, competition, combat, collective learning, preference pairs"
"Joint Evaluation of Answer and Reasoning Consistency for Hallucination
  Detection in Large Reasoning Models","Changyue Wang, Weihang Su, Qingyao Ai, Yiqun Liu",2025-06-05T09:54:04Z,http://arxiv.org/pdf/2506.04832v1,"Large Reasoning Models (LRMs) extend large language models with explicit,
multi-step reasoning traces to enhance transparency and performance on complex
tasks. However, these reasoning traces can be redundant or logically
inconsistent, making them a new source of hallucination that is difficult to
detect. Existing hallucination detection methods focus primarily on
answer-level uncertainty and often fail to detect hallucinations or logical
inconsistencies arising from the model's reasoning trace. This oversight is
particularly problematic for LRMs, where the explicit thinking trace is not
only an important support to the model's decision-making process but also a key
source of potential hallucination. To this end, we propose RACE (Reasoning and
Answer Consistency Evaluation), a novel framework specifically tailored for
hallucination detection in LRMs. RACE operates by extracting essential
reasoning steps and computing four diagnostic signals: inter-sample consistency
of reasoning traces, entropy-based answer uncertainty, semantic alignment
between reasoning and answers, and internal coherence of reasoning. This joint
analysis enables fine-grained hallucination detection even when the final
answer appears correct. Experiments across datasets and different LLMs
demonstrate that RACE outperforms existing hallucination detection baselines,
offering a robust and generalizable solution for evaluating LRMs. Our code is
available at: https://github.com/bebr2/RACE.",大型推理模型（LRMs）通过显式、多步推理轨迹扩展大型语言模型，以增强透明度和复杂任务的性能。然而，这些推理轨迹可能是冗余或逻辑不一致的，使其成为一种新的幻觉来源，难以检测。现有的幻觉检测方法主要集中在答案级别的不确定性，通常无法检测到模型推理轨迹中的幻觉或逻辑不一致。这种疏忽对LRMs尤为问题，因为显式的思维轨迹不仅是模型决策过程的重要支持，也是潜在幻觉的关键来源。为此，我们提出了RACE（推理和答案一致性评估），这是一个专门为LRMs幻觉检测设计的新框架。RACE通过提取关键推理步骤并计算四个诊断信号来运行：推理轨迹的样本间一致性、基于熵的答案不确定性、推理和答案之间的语义对齐以及推理的内部一致性。这种联合分析使得即使最终答案看似正确，也能实现细粒度的幻觉检测。跨数据集和不同LLMs的实验表明，RACE在现有的幻觉检测基线上表现优异，为评估LRMs提供了一个强大且可推广的解决方案。我们的代码可在以下网址找到：https://github.com/bebr2/RACE。,"The paper introduces RACE, a framework for detecting hallucinations in Large Reasoning Models by evaluating the consistency of reasoning and answers.",LLM,Helpful,"Hallucination detection, Large Reasoning Models, Reasoning consistency, Answer consistency, LLM alignment"
"SCOP: Evaluating the Comprehension Process of Large Language Models from
  a Cognitive View","Yongjie Xiao, Hongru Liang, Peixin Qin, Yao Zhang, Wenqiang Lei",2025-06-05T13:10:24Z,http://arxiv.org/pdf/2506.05000v1,"Despite the great potential of large language models(LLMs) in machine
comprehension, it is still disturbing to fully count on them in real-world
scenarios. This is probably because there is no rational explanation for
whether the comprehension process of LLMs is aligned with that of experts. In
this paper, we propose SCOP to carefully examine how LLMs perform during the
comprehension process from a cognitive view. Specifically, it is equipped with
a systematical definition of five requisite skills during the comprehension
process, a strict framework to construct testing data for these skills, and a
detailed analysis of advanced open-sourced and closed-sourced LLMs using the
testing data. With SCOP, we find that it is still challenging for LLMs to
perform an expert-level comprehension process. Even so, we notice that LLMs
share some similarities with experts, e.g., performing better at comprehending
local information than global information. Further analysis reveals that LLMs
can be somewhat unreliable -- they might reach correct answers through flawed
comprehension processes. Based on SCOP, we suggest that one direction for
improving LLMs is to focus more on the comprehension process, ensuring all
comprehension skills are thoroughly developed during training.",尽管大语言模型（LLMs）在机器理解方面具有巨大潜力，但在现实世界的场景中完全依赖它们仍然令人担忧。这是因为尚无合理的解释说明LLMs的理解过程是否与专家一致。在本文中，我们提出了SCOP，以从认知视角仔细检查LLMs在理解过程中的表现。具体来说，它配备了理解过程中五项必备技能的系统定义、为这些技能构建测试数据的严格框架，以及使用测试数据对先进的开源和闭源LLMs进行详细分析。通过SCOP，我们发现LLMs在实现专家级别的理解过程方面仍然具有挑战性。即使如此，我们注意到LLMs与专家有一些相似之处，例如在理解局部信息方面表现更好，而不是全局信息。进一步的分析揭示了LLMs可能不可靠——它们可能通过有缺陷的理解过程得出正确的答案。基于SCOP，我们建议改进LLMs的一个方向是更多地关注理解过程，确保在训练过程中充分发展所有理解技能。,"The paper introduces SCOP, a framework to evaluate the comprehension process of LLMs from a cognitive perspective, highlighting areas for improvement to ensure helpful and honest responses.",LLM,"Helpful, Honest","LLM comprehension, cognitive view, evaluation, skills, reliability"
"RIVAL: Reinforcement Learning with Iterative and Adversarial
  Optimization for Machine Translation","Tianjiao Li, Mengran Yu, Chenyu Shi, Yanjun Zhao, Xiaojing Liu, Qiang Zhang, Qi Zhang, Xuanjing Huang, Jiayin Wang",2025-06-05T14:18:21Z,http://arxiv.org/pdf/2506.05070v1,"Large language models (LLMs) possess strong multilingual capabilities, and
combining Reinforcement Learning from Human Feedback (RLHF) with translation
tasks has shown great potential. However, we observe that this paradigm
performs unexpectedly poorly when applied to colloquial subtitle translation
tasks. In this work, we investigate this issue and find that the offline reward
model (RM) gradually diverges from the online LLM due to distributional shift,
ultimately leading to undesirable training outcomes. To address this, we
propose RIVAL, an adversarial training framework that formulates the process as
a min-max game between the RM and the LLM. RIVAL iteratively updates the both
models, with the RM trained to distinguish strong from weak translations
(qualitative preference reward), and the LLM trained to enhance its translation
for closing this gap. To stabilize training and improve generalizability, we
also incorporate quantitative preference reward (e.g., BLEU) into the RM,
enabling reference-free quality modeling aligned with human evaluation. Through
extensive experiments, we demonstrate that the proposed adversarial training
framework significantly improves upon translation baselines.",大语言模型（LLMs）具有强大的多语言能力，将强化学习从人类反馈（RLHF）与翻译任务结合起来显示出巨大潜力。然而，我们发现这种范式在应用于口语字幕翻译任务时表现出乎意料地差。在本工作中，我们研究了这个问题，发现离线奖励模型（RM）由于分布偏移逐渐偏离在线LLM，最终导致不良的训练结果。为了解决这个问题，我们提出了RIVAL，一个将过程公式化为RM和LLM之间的min-max游戏的对抗训练框架。RIVAL迭代更新两个模型，RM被训练以区分强翻译和弱翻译（定性偏好奖励），而LLM被训练以增强其翻译以缩小这一差距。为了稳定训练并提高可推广性，我们还将定量偏好奖励（例如BLEU）纳入RM，使其能够与人类评估对齐的无参考质量建模。通过广泛的实验，我们证明了所提出的对抗训练框架显著改进了翻译基线。,"The paper introduces RIVAL, an adversarial training framework that improves LLM translation by aligning with human preferences through iterative updates and quantitative rewards.",LLM,Helpful,"RLHF, Translation, Adversarial Training, Human Feedback, LLM"
"Just a Scratch: Enhancing LLM Capabilities for Self-harm Detection
  through Intent Differentiation and Emoji Interpretation","Soumitra Ghosh, Gopendra Vikram Singh,  Shambhavi, Sabarna Choudhury, Asif Ekbal",2025-06-05T14:19:48Z,http://arxiv.org/pdf/2506.05073v1,"Self-harm detection on social media is critical for early intervention and
mental health support, yet remains challenging due to the subtle,
context-dependent nature of such expressions. Identifying self-harm intent aids
suicide prevention by enabling timely responses, but current large language
models (LLMs) struggle to interpret implicit cues in casual language and
emojis. This work enhances LLMs' comprehension of self-harm by distinguishing
intent through nuanced language-emoji interplay. We present the Centennial
Emoji Sensitivity Matrix (CESM-100), a curated set of 100 emojis with
contextual self-harm interpretations and the Self-Harm Identification aNd
intent Extraction with Supportive emoji sensitivity (SHINES) dataset, offering
detailed annotations for self-harm labels, casual mentions (CMs), and serious
intents (SIs). Our unified framework: a) enriches inputs using CESM-100; b)
fine-tunes LLMs for multi-task learning: self-harm detection (primary) and
CM/SI span detection (auxiliary); c) generates explainable rationales for
self-harm predictions. We evaluate the framework on three state-of-the-art
LLMs-Llama 3, Mental-Alpaca, and MentalLlama, across zero-shot, few-shot, and
fine-tuned scenarios. By coupling intent differentiation with contextual cues,
our approach commendably enhances LLM performance in both detection and
explanation tasks, effectively addressing the inherent ambiguity in self-harm
signals. The SHINES dataset, CESM-100 and codebase are publicly available at:
https://www.iitp.ac.in/~ai-nlp-ml/resources.html#SHINES .",在社交媒体上检测自残行为对于早期干预和心理健康支持至关重要，但由于其微妙、依赖上下文的性质，仍然具有挑战性。识别自残意图有助于自杀预防，因为它使及时的回应成为可能，但当前的大型语言模型（LLMs）在解释非正式语言和表情符号中的隐含线索方面存在困难。本文通过细微的语言-表情符号相互作用来区分意图，增强了LLMs对自残的理解。我们提出了Centennial Emoji Sensitivity Matrix（CESM-100），这是一个包含100个具有上下文自残解释的表情符号的精心策划的集合，以及Self-Harm Identification aNd intent Extraction with Supportive emoji sensitivity（SHINES）数据集，提供了自残标签、非正式提及（CMs）和严重意图（SIs）的详细注释。我们的统一框架：a）使用CESM-100丰富输入；b）对LLMs进行多任务学习的微调：自残检测（主要）和CM/SI跨度检测（辅助）；c）为自残预测生成可解释的理由。我们在三种最先进的LLMs-Llama 3、Mental-Alpaca和MentalLlama上进行了评估，跨越零次、少次和微调的情景。通过将意图区分与上下文线索结合起来，我们的方法在检测和解释任务中显著提高了LLM的性能，有效地解决了自残信号中的固有模糊性。SHINES数据集、CESM-100和代码库可在以下网址公开获取：https://www.iitp.ac.in/~ai-nlp-ml/resources.html#SHINES。,The paper presents a framework to enhance LLMs' ability to detect self-harm on social media by improving their understanding of emojis and intent differentiation.,LLM,Harmless,"Self-harm detection, LLM enhancement, Emoji interpretation, Intent differentiation, Mental health support"
Do Large Language Models Judge Error Severity Like Humans?,"Diege Sun, Guanyi Chen, Fan Zhao, Xiaorong Cheng, Tingting He",2025-06-05T15:24:33Z,http://arxiv.org/pdf/2506.05142v1,"Large Language Models (LLMs) are increasingly used as automated evaluators in
natural language generation, yet it remains unclear whether they can accurately
replicate human judgments of error severity. In this study, we systematically
compare human and LLM assessments of image descriptions containing controlled
semantic errors. We extend the experimental framework of van Miltenburg et al.
(2020) to both unimodal (text-only) and multimodal (text + image) settings,
evaluating four error types: age, gender, clothing type, and clothing colour.
Our findings reveal that humans assign varying levels of severity to different
error types, with visual context significantly amplifying perceived severity
for colour and type errors. Notably, most LLMs assign low scores to gender
errors but disproportionately high scores to colour errors, unlike humans, who
judge both as highly severe but for different reasons. This suggests that these
models may have internalised social norms influencing gender judgments but lack
the perceptual grounding to emulate human sensitivity to colour, which is
shaped by distinct neural mechanisms. Only one of the evaluated LLMs, Doubao,
replicates the human-like ranking of error severity, but it fails to
distinguish between error types as clearly as humans. Surprisingly,
DeepSeek-V3, a unimodal LLM, achieves the highest alignment with human
judgments across both unimodal and multimodal conditions, outperforming even
state-of-the-art multimodal models.",大语言模型（LLMs）越来越多地被用作自然语言生成的自动评估器，但尚不清楚它们是否能准确复制人类对错误严重程度的判断。在本研究中，我们系统地比较了人类和LLM对包含受控语义错误的图像描述的评估。我们将van Miltenburg等人（2020）的实验框架扩展到单模态（仅文本）和多模态（文本+图像）设置，评估四种错误类型：年龄、性别、服装类型和服装颜色。我们的发现表明，人类对不同错误类型的严重程度进行了不同的评分，视觉上下文显著放大了颜色和类型错误的感知严重程度。值得注意的是，大多数LLM对性别错误给出低分，但对颜色错误给出不成比例的高分，而人类则认为两者都非常严重，但原因不同。这表明这些模型可能内化了影响性别判断的社会规范，但缺乏感知基础来模仿人类对颜色的敏感性，这种敏感性是由不同的神经机制塑造的。我们评估的LLM中，只有Doubao复制了人类似的错误严重程度排名，但它无法像人类那样清楚地区分错误类型。令人惊讶的是，单模态LLM DeepSeek-V3在单模态和多模态条件下都实现了与人类判断的最高对齐，超过了甚至是最先进的多模态模型。,"The study compares human and LLM judgments of error severity in image descriptions, finding that while some LLMs align well with human judgments, others do not, highlighting the challenges in achieving helpful alignment.",LLM,Helpful,"Error severity, human judgment, multimodal, LLM evaluation, alignment"
CLATTER: Comprehensive Entailment Reasoning for Hallucination Detection,"Ron Eliav, Arie Cattan, Eran Hirsch, Shahaf Bassan, Elias Stengel-Eskin, Mohit Bansal, Ido Dagan",2025-06-05T17:02:52Z,http://arxiv.org/pdf/2506.05243v1,"A common approach to hallucination detection casts it as a natural language
inference (NLI) task, often using LLMs to classify whether the generated text
is entailed by corresponding reference texts. Since entailment classification
is a complex reasoning task, one would expect that LLMs could benefit from
generating an explicit reasoning process, as in CoT reasoning or the explicit
``thinking'' of recent reasoning models. In this work, we propose that guiding
such models to perform a systematic and comprehensive reasoning process -- one
that both decomposes the text into smaller facts and also finds evidence in the
source for each fact -- allows models to execute much finer-grained and
accurate entailment decisions, leading to increased performance. To that end,
we define a 3-step reasoning process, consisting of (i) claim decomposition,
(ii) sub-claim attribution and entailment classification, and (iii) aggregated
classification, showing that such guided reasoning indeed yields improved
hallucination detection. Following this reasoning framework, we introduce an
analysis scheme, consisting of several metrics that measure the quality of the
intermediate reasoning steps, which provided additional empirical evidence for
the improved quality of our guided reasoning scheme.",一个常见的幻觉检测方法将其视为自然语言推理（NLI）任务，通常使用大型语言模型（LLM）来分类生成的文本是否由相应的参考文本推导出来。由于推理分类是一个复杂的推理任务，人们可能会期望LLM能够从生成显式推理过程中受益，例如CoT推理或最近推理模型的显式“思考”。在本工作中，我们提出指导这些模型执行系统化和全面的推理过程——一种既将文本分解为较小的事实，又在源中为每个事实找到证据的过程——使模型能够执行更细粒度和准确的推理决策，从而提高性能。为此，我们定义了一个3步推理过程，包括（i）声明分解，（ii）子声明归因和推理分类，以及（iii）聚合分类，表明这种指导推理确实带来了改进的幻觉检测。根据这个推理框架，我们引入了一种分析方案，包括几个度量标准，用于衡量中间推理步骤的质量，这为我们的指导推理方案提供了额外的实证证据。,The paper introduces a guided reasoning process for LLMs to improve hallucination detection by decomposing text into smaller facts and finding evidence in the source.,LLM,"Helpful, Honest","Hallucination detection, entailment reasoning, LLM, natural language inference, reasoning process"
"Flattery, Fluff, and Fog: Diagnosing and Mitigating Idiosyncratic Biases
  in Preference Models","Anirudh Bharadwaj, Chaitanya Malaviya, Nitish Joshi, Mark Yatskar",2025-06-05T17:59:32Z,http://arxiv.org/pdf/2506.05339v1,"Language models serve as proxies for human preference judgements in alignment
and evaluation, yet they exhibit systematic miscalibration, prioritizing
superficial patterns over substantive qualities. This bias manifests as
overreliance on features like length, structure, and style, leading to issues
like reward hacking and unreliable evaluations. Evidence suggests these biases
originate in artifacts in human training data. In this work, we systematically
investigate the relationship between training data biases and preference model
miscalibration across five idiosyncratic features of language model
generations: length, structure, jargon, sycophancy and vagueness. Using
controlled counterfactual pairs, we first quantify the extent to which
preference models favor responses with magnified biases (skew), finding this
preference occurs in >60% of instances, and model preferences show high
miscalibration (~40%) compared to human preferences. Notably, bias features
only show mild negative correlations to human preference labels (mean r_human =
-0.12) but show moderately strong positive correlations with labels from a
strong reward model (mean r_model = +0.36), suggesting that models may overrely
on spurious cues. To mitigate these issues, we propose a simple post-training
method based on counterfactual data augmentation (CDA) using synthesized
contrastive examples. Finetuning models with CDA reduces average miscalibration
from 39.4% to 32.5% and average absolute skew difference from 20.5% to 10.0%,
while maintaining overall RewardBench performance, showing that targeted
debiasing is effective for building reliable preference models.",语言模型作为人类偏好判断的代理在对齐和评估中，但它们表现出系统性的失调，优先考虑表面模式而非实质性质量。这种偏见表现为对长度、结构和风格等特征的过度依赖，导致奖励作弊和不可靠的评估问题。证据表明，这些偏见源于人类训练数据中的瑕疵。在本研究中，我们系统地研究了训练数据偏见与偏好模型失调之间的关系，跨越语言模型生成的五个独特特征：长度、结构、行话、阿谀奉承和模糊性。使用受控对照对，我们首先量化了偏好模型偏好具有放大偏见的响应（偏差）的程度，发现这种偏好在>60%的实例中发生，模型偏好与人类偏好相比显示出高失调（约40%）。值得注意的是，偏见特征与人类偏好标签仅显示轻微负相关（平均r_human = -0.12），但与强奖励模型的标签显示中等强度的正相关（平均r_model = +0.36），这表明模型可能过度依赖虚假线索。为了缓解这些问题，我们提出了一种基于对比数据增强（CDA）的简单后训练方法，使用合成对比示例。使用CDA微调模型，减少了平均失调从39.4%到32.5%，平均绝对偏差差异从20.5%到10.0%，同时保持整体RewardBench性能，表明有针对性的去偏见对构建可靠的偏好模型是有效的。,The paper investigates biases in preference models used for aligning language models and proposes a debiasing method to improve their reliability.,LLM,"Helpful, Harmless","Bias, Preference Models, Alignment, Debiasing, Reward Hacking"
"Beyond the Protocol: Unveiling Attack Vectors in the Model Context
  Protocol Ecosystem","Hao Song, Yiming Shen, Wenxuan Luo, Leixin Guo, Ting Chen, Jiashui Wang, Beibei Li, Xiaosong Zhang, Jiachi Chen",2025-05-31T08:01:11Z,http://arxiv.org/pdf/2506.02040v2,"The Model Context Protocol (MCP) is an emerging standard designed to enable
seamless interaction between Large Language Model (LLM) applications and
external tools or resources. Within a short period, thousands of MCP services
have already been developed and deployed. However, the client-server
integration architecture inherent in MCP may expand the attack surface against
LLM Agent systems, introducing new vulnerabilities that allow attackers to
exploit by designing malicious MCP servers. In this paper, we present the first
systematic study of attack vectors targeting the MCP ecosystem. Our analysis
identifies four categories of attacks, i.e., Tool Poisoning Attacks, Puppet
Attacks, Rug Pull Attacks, and Exploitation via Malicious External Resources.
To evaluate the feasibility of these attacks, we conduct experiments following
the typical steps of launching an attack through malicious MCP servers:
upload-download-attack. Specifically, we first construct malicious MCP servers
and successfully upload them to three widely used MCP aggregation platforms.
The results indicate that current audit mechanisms are insufficient to identify
and prevent the proposed attack methods. Next, through a user study and
interview with 20 participants, we demonstrate that users struggle to identify
malicious MCP servers and often unknowingly install them from aggregator
platforms. Finally, we demonstrate that these attacks can trigger harmful
behaviors within the user's local environment-such as accessing private files
or controlling devices to transfer digital assets-by deploying a
proof-of-concept (PoC) framework against five leading LLMs. Additionally, based
on interview results, we discuss four key challenges faced by the current
security ecosystem surrounding MCP servers. These findings underscore the
urgent need for robust security mechanisms to defend against malicious MCP
servers.",模型上下文协议（MCP）是一种新兴标准，旨在实现大型语言模型（LLM）应用程序与外部工具或资源之间的无缝互动。在短短几个月内，已经开发和部署了数千个MCP服务。然而，MCP中固有的客户端-服务器集成架构可能会扩大针对LLM代理系统的攻击面，引入新的漏洞，使攻击者能够通过设计恶意MCP服务器进行利用。在本文中，我们首次系统研究了针对MCP生态系统的攻击向量。我们的分析识别了四类攻击，即工具中毒攻击、傀儡攻击、地毯拖拉攻击和通过恶意外部资源的利用。为了评估这些攻击的可行性，我们按照通过恶意MCP服务器发起攻击的典型步骤进行了实验：上传-下载-攻击。具体来说，我们首先构建了恶意MCP服务器，并成功将它们上传到三个广泛使用的MCP聚合平台。结果表明，当前的审计机制不足以识别和防止所提出的攻击方法。接下来，通过与20名参与者的用户研究和访谈，我们证明了用户难以识别恶意MCP服务器，并且往往不知不觉地从聚合器平台安装它们。最后，我们通过针对五个领先LLM的概念验证（PoC）框架，证明这些攻击可以在用户的本地环境中触发有害行为，例如访问私人文件或控制设备以转移数字资产。此外，基于访谈结果，我们讨论了当前MCP服务器安全生态系统面临的四个关键挑战。这些发现强调了迫切需要强大的安全机制来防御恶意MCP服务器。,The paper identifies and evaluates attack vectors in the Model Context Protocol ecosystem that can trigger harmful behaviors in LLM applications.,LLM,Harmless,"LLM, MCP, security, attack vectors, harmful behaviors"
"Beyond Accuracy: Dissecting Mathematical Reasoning for LLMs Under
  Reinforcement Learning","Jiayu Wang, Yifei Ming, Zixuan Ke, Caiming Xiong, Shafiq Joty, Aws Albarghouthi, Frederic Sala",2025-06-05T07:53:59Z,http://arxiv.org/pdf/2506.04723v1,"Reinforcement learning (RL) has become the dominant paradigm for endowing
language models with advanced reasoning capabilities. Despite the substantial
empirical gains demonstrated by RL-based training methods like GRPO, a granular
understanding of their advantages is still lacking. To address this gap, we
introduce a fine-grained analytic framework to dissect the impact of RL on
reasoning. Our framework specifically investigates key elements that have been
hypothesized to benefit from RL training: (1) plan-following and execution, (2)
problem decomposition, and (3) improved reasoning and knowledge utilization.
Using this framework, we gain insights beyond mere accuracy. For instance,
providing models with explicit step-by-step plans surprisingly degrades
performance on the most challenging benchmarks, yet RL-tuned models exhibit
greater robustness, experiencing markedly smaller performance drops than their
base counterparts. This suggests that RL may not primarily enhance the
execution of external plans but rather empower models to formulate and follow
internal strategies better suited to their reasoning processes. Conversely, we
observe that RL enhances the model's capacity to integrate provided knowledge
into its reasoning process, leading to performance improvements across diverse
tasks. We also study difficulty, showing improved training by developing new
ways to exploit hard problems. Our findings lay a foundation for more
principled training and evaluation of reasoning models.",强化学习（RL）已经成为赋予语言模型先进推理能力的主导范式。尽管基于RL的训练方法（如GRPO）展示了显著的经验收益，但对其优势的细粒度理解仍然缺乏。为了填补这一空白，我们引入了一个细粒度的分析框架，以解剖RL对推理的影响。我们的框架特别研究了被假设从RL训练中受益的关键要素：(1) 计划遵循和执行，(2) 问题分解，(3) 改进的推理和知识利用。通过这个框架，我们获得了超越简单准确性的见解。例如，为模型提供明确的逐步计划在最具挑战性的基准测试中意外地降低了性能，然而，经过RL调整的模型表现出更大的鲁棒性，经历了明显小于其基础对应物的性能下降。这表明RL可能并不主要增强外部计划的执行，而是使模型更好地制定和遵循更适合其推理过程的内部策略。相反，我们观察到RL增强了模型将提供的知识整合到其推理过程中的能力，导致在各种任务中表现出改进。我们还研究了难度，通过开发新的利用困难问题的方法来改进训练。我们的发现为更有原则的推理模型的训练和评估奠定了基础。,"The paper introduces a framework to analyze how reinforcement learning improves the reasoning capabilities of large language models, focusing on plan-following, problem decomposition, and knowledge utilization.",LLM,"Helpful, Honest","Reinforcement Learning, Reasoning, Large Language Models, Plan-following, Knowledge Utilization"
"LeanPO: Lean Preference Optimization for Likelihood Alignment in
  Video-LLMs","Xiaodong Wang, Jinfa Huang, Li Yuan, Peixi Peng",2025-06-05T17:21:16Z,http://arxiv.org/pdf/2506.05260v1,"Most Video Large Language Models (Video-LLMs) adopt preference alignment
techniques, e.g., DPO~\citep{rafailov2024dpo}, to optimize the reward margin
between a winning response ($y_w$) and a losing response ($y_l$). However, the
likelihood displacement observed in DPO indicates that both $\log \pi_\theta
(y_w\mid x)$ and $\log \pi_\theta (y_l\mid x) $ often decrease during training,
inadvertently boosting the probabilities of non-target responses. In this
paper, we systematically revisit this phenomenon from LLMs to Video-LLMs,
showing that it intensifies when dealing with the redundant complexity of video
content. To alleviate the impact of this phenomenon, we propose \emph{Lean
Preference Optimization} (LeanPO), a reference-free approach that reformulates
the implicit reward as the average likelihood of the response with respect to
the policy model. A key component of LeanPO is the reward-trustworthiness
correlated self-generated preference data pipeline, which carefully infuses
relevant prior knowledge into the model while continuously refining the
preference data via self-reflection. This allows the policy model to obtain
high-quality paired data and accurately estimate the newly defined reward, thus
mitigating the unintended drop. In addition, we introduce a dynamic label
smoothing strategy that mitigates the impact of noise in responses from diverse
video content, preventing the model from overfitting to spurious details.
Extensive experiments demonstrate that LeanPO significantly enhances the
performance of state-of-the-art Video-LLMs, consistently boosting baselines of
varying capacities with minimal additional training overhead. Moreover, LeanPO
offers a simple yet effective solution for aligning Video-LLM preferences with
human trustworthiness, paving the way toward the reliable and efficient
Video-LLMs.",大多数视频大语言模型（Video-LLMs）采用偏好对齐技术，例如DPO，以优化赢家响应（$y_w$）和输家响应（$y_l$）之间的奖励边际。然而，DPO中观察到的似然位移表明，$\log \pi_\theta (y_w\mid x)$ 和 $\log \pi_\theta (y_l\mid x)$ 在训练过程中往往会减少，从而意外地提高了非目标响应的概率。在本文中，我们系统地重新审视了从LLMs到Video-LLMs的这一现象，表明在处理视频内容的冗余复杂性时，这一现象会加剧。为了缓解这一现象的影响，我们提出了Lean Preference Optimization（LeanPO），一种无参考的方法，将隐含奖励重新定义为与策略模型相关的响应的平均似然。LeanPO的一个关键组件是奖励可信度相关的自生成偏好数据管道，它通过自我反思不断精炼偏好数据，同时将相关的先验知识注入模型。这使策略模型能够获得高质量的配对数据，并准确估计新定义的奖励，从而减轻了意外的下降。此外，我们引入了一种动态标签平滑策略，减轻了来自多样化视频内容的响应噪声的影响，防止模型过拟合于虚假细节。广泛的实验表明，LeanPO显著增强了最先进的Video-LLMs的性能，始终提升了不同容量的基线，几乎没有额外的训练开销。此外，LeanPO为将Video-LLM的偏好与人类可信度对齐提供了一种简单而有效的解决方案，为可靠和高效的Video-LLMs铺平了道路。,"The paper introduces LeanPO, a method to improve the alignment of Video-LLMs with human preferences by mitigating likelihood displacement during training.",LLM,"Helpful, Harmless","Preference Optimization, Video-LLMs, Alignment, Likelihood Displacement, Trustworthiness"
"CLARIFY: Contrastive Preference Reinforcement Learning for Untangling
  Ambiguous Queries","Ni Mu, Hao Hu, Xiao Hu, Yiqin Yang, Bo Xu, Qing-Shan Jia",2025-05-31T04:37:07Z,http://arxiv.org/pdf/2506.00388v2,"Preference-based reinforcement learning (PbRL) bypasses explicit reward
engineering by inferring reward functions from human preference comparisons,
enabling better alignment with human intentions. However, humans often struggle
to label a clear preference between similar segments, reducing label efficiency
and limiting PbRL's real-world applicability. To address this, we propose an
offline PbRL method: Contrastive LeArning for ResolvIng Ambiguous Feedback
(CLARIFY), which learns a trajectory embedding space that incorporates
preference information, ensuring clearly distinguished segments are spaced
apart, thus facilitating the selection of more unambiguous queries. Extensive
experiments demonstrate that CLARIFY outperforms baselines in both non-ideal
teachers and real human feedback settings. Our approach not only selects more
distinguished queries but also learns meaningful trajectory embeddings.",基于偏好的强化学习（PbRL）通过从人类偏好比较中推断奖励函数，绕过了显式奖励工程，从而更好地与人类意图对齐。然而，人类往往难以在相似的段落之间标记出明确的偏好，这降低了标签效率，并限制了PbRL在现实世界中的应用。为了解决这个问题，我们提出了一种离线PbRL方法：对比学习用于解决模糊反馈（CLARIFY），它学习一个包含偏好信息的轨迹嵌入空间，确保明确区分的段落之间有足够的距离，从而促进更不模糊的查询选择。广泛的实验表明，CLARIFY在非理想教师和真实人类反馈设置中都优于基线。我们的方法不仅选择了更有区分度的查询，还学习了有意义的轨迹嵌入。,"The paper introduces CLARIFY, a method that improves the alignment of LLMs with human intentions by learning meaningful trajectory embeddings from ambiguous queries.",LLM,Helpful,"Preference-based reinforcement learning, alignment, human intentions, trajectory embeddings, ambiguous queries"
Biased AI can Influence Political Decision-Making,"Jillian Fisher, Shangbin Feng, Robert Aron, Thomas Richardson, Yejin Choi, Daniel W. Fisher, Jennifer Pan, Yulia Tsvetkov, Katharina Reinecke",2024-10-08T22:56:00Z,http://arxiv.org/pdf/2410.06415v3,"As modern large language models (LLMs) become integral to everyday tasks,
concerns about their inherent biases and their potential impact on human
decision-making have emerged. While bias in models are well-documented, less is
known about how these biases influence human decisions. This paper presents two
interactive experiments investigating the effects of partisan bias in LLMs on
political opinions and decision-making. Participants interacted freely with
either a biased liberal, biased conservative, or unbiased control model while
completing these tasks. We found that participants exposed to partisan biased
models were significantly more likely to adopt opinions and make decisions
which matched the LLM's bias. Even more surprising, this influence was seen
when the model bias and personal political partisanship of the participant were
opposite. However, we also discovered that prior knowledge of AI was weakly
correlated with a reduction of the impact of the bias, highlighting the
possible importance of AI education for robust mitigation of bias effects. Our
findings not only highlight the critical effects of interacting with biased
LLMs and its ability to impact public discourse and political conduct, but also
highlights potential techniques for mitigating these risks in the future.",随着现代大型语言模型（LLM）成为日常任务的重要组成部分，人们对其内在偏见及其对人类决策的潜在影响的担忧日益增加。虽然模型中的偏见已被广泛记录，但对这些偏见如何影响人类决策的了解却很少。本文介绍了两个互动实验，研究了LLM中的党派偏见对政治观点和决策的影响。参与者在完成这些任务时自由与偏向自由、偏向保守或无偏见的控制模型进行互动。我们发现，接触到党派偏见模型的参与者更有可能采纳与LLM偏见相匹配的观点并做出决策。更令人惊讶的是，即使模型偏见与参与者的个人政治党派倾向相反，这种影响也被看到。然而，我们还发现，对AI的先前知识与减少偏见影响的弱相关，突出了AI教育在有效缓解偏见影响中的可能重要性。我们的发现不仅突显了与偏见LLM互动的关键影响及其影响公共讨论和政治行为的能力，还突显了未来缓解这些风险的潜在技术。,The paper investigates how partisan biases in large language models can influence political decision-making and explores potential mitigation techniques.,LLM,Harmless,"Bias, Political Decision-Making, Large Language Models, Influence, Mitigation"
"Biased by Design: Leveraging Inherent AI Biases to Enhance Critical
  Thinking of News Readers","Liudmila Zavolokina, Kilian Sprenkamp, Zoya Katashinskaya, Daniel Gordon Jones",2025-04-20T07:39:00Z,http://arxiv.org/pdf/2504.14522v3,"This paper explores the design of a propaganda detection tool using Large
Language Models (LLMs). Acknowledging the inherent biases in AI models,
especially in political contexts, we investigate how these biases might be
leveraged to enhance critical thinking in news consumption. Countering the
typical view of AI biases as detrimental, our research proposes strategies of
user choice and personalization in response to a user's political stance,
applying psychological concepts of confirmation bias and cognitive dissonance.
We present findings from a qualitative user study, offering insights and design
recommendations (bias awareness, personalization and choice, and gradual
introduction of diverse perspectives) for AI tools in propaganda detection.",这篇论文探讨了使用大型语言模型（LLMs）设计宣传检测工具。承认人工智能模型中的固有偏见，特别是在政治背景下，我们研究了这些偏见如何被利用以增强新闻消费中的批判性思维。与通常将人工智能偏见视为有害的观点相反，我们的研究提出了用户选择和个性化策略，以应对用户的政治立场，应用确认偏见和认知失调的心理概念。我们提出了一个定性用户研究，提供了关于宣传检测AI工具的见解和设计建议（偏见意识、个性化和选择以及逐步引入多样化的观点）。,The paper explores using LLMs to detect propaganda and enhance critical thinking by leveraging inherent biases and offering personalized user experiences.,LLM,"Helpful, Harmless","Propaganda detection, Bias, Critical thinking, Personalization, User choice"
"Reason-to-Recommend: Using Interaction-of-Thought Reasoning to Enhance
  LLM Recommendation","Keyu Zhao, Fengli Xu, Yong Li",2025-06-05T14:16:44Z,http://arxiv.org/pdf/2506.05069v1,"Driven by advances in Large Language Models (LLMs), integrating them into
recommendation tasks has gained interest due to their strong semantic
understanding and prompt flexibility. Prior work encoded user-item interactions
or metadata into prompts for recommendations. In parallel, LLM reasoning,
boosted by test-time scaling and reinforcement learning, has excelled in fields
like mathematics and code, where reasoning traces and correctness signals are
clear, enabling high performance and interpretability. However, directly
applying these reasoning methods to recommendation is ineffective because user
feedback is implicit and lacks reasoning supervision. To address this, we
propose $\textbf{R2Rec}$, a reasoning-enhanced recommendation framework that
samples interaction chains from the user-item graph and converts them into
structured interaction-of-thoughts via a progressive masked prompting strategy,
with each thought representing stepwise reasoning grounded in interaction
context. This allows LLMs to simulate step-by-step decision-making based on
implicit patterns. We design a two-stage training pipeline: supervised
fine-tuning teaches basic reasoning from high-quality traces, and reinforcement
learning refines reasoning via reward signals, alleviating sparse explicit
supervision. Experiments on three real-world datasets show R2Rec outperforms
classical and LLM-based baselines with an average $\textbf{10.48%}$ improvement
in HitRatio@1 and $\textbf{131.81%}$ gain over the original LLM. Furthermore,
the explicit reasoning chains enhance interpretability by revealing the
decision process. Our code is available at:
https://anonymous.4open.science/r/R2Rec-7C5D.",受大型语言模型（LLM）进步的推动，将其集成到推荐任务中已引起兴趣，因为它们具有强大的语义理解能力和提示灵活性。此前的工作将用户-项目交互或元数据编码到推荐提示中。与此同时，LLM推理，受益于测试时扩展和强化学习，在数学和代码等领域表现出色，其中推理轨迹和正确性信号清晰，从而实现高性能和可解释性。然而，直接将这些推理方法应用于推荐是无效的，因为用户反馈是隐式的，缺乏推理监督。为了解决这个问题，我们提出了R2Rec，一个增强推理的推荐框架，从用户-项目图中采样交互链，并通过逐步掩码提示策略将其转换为结构化的思维交互，每个思维代表基于交互上下文的逐步推理。这使得LLM能够根据隐式模式模拟逐步决策。我们设计了一个两阶段的训练流水线：监督微调从高质量的轨迹中教学基本推理，强化学习通过奖励信号精炼推理，缓解稀疏的显式监督。在三个真实世界数据集上的实验表明，R2Rec在HitRatio@1上比经典和基于LLM的基线提高了平均10.48%，比原始LLM提高了131.81%。此外，显式推理链通过揭示决策过程增强了可解释性。我们的代码可在以下网址找到：https://anonymous.4open.science/r/R2Rec-7C5D。,"The paper introduces R2Rec, a framework that enhances LLMs for recommendation tasks by incorporating reasoning techniques to improve performance and interpretability.",LLM,Helpful,"LLM, Recommendation, Reasoning, Interaction-of-Thought, Interpretability"
