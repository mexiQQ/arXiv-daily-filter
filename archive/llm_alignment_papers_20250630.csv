Title,Authors,Published,Link,Abstract_EN,Abstract_ZH,One_Sentence_Summary,Model_Type,HHH_Focus,Keywords
Reinforcement Learning Fine-Tuning of Language Model for Instruction Following and Math Reasoning,"Yifu Han, Geo Zhang",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2506.21560.pdf,"This study investigates the effectiveness of reinforcement learning (RL) fine-tuning techniques on a compact language model (Qwen2.5-0.5B Base) for two challenging tasks: instruction following and mathematical reasoning. We compare supervised fine-tuning (SFT), Direct Preference Optimization (DPO) using preference-labeled data, and Reinforce Leave-One-Out (RLOO) with reward models. Our experiments show that RLOO with DeBERTa reward modeling achieves the best alignment, while DPO provides strong and consistent results. For math reasoing tasks, synthetic data augmentation and best-of-N sampling with an external verifier significantly improve accuracy, showing the potential of combining fine-tuning with inference-time tools. This study highlights key trade-offs and practical strategies for training lightweight, task-aligned small-scale language models.",这项研究探讨了强化学习（RL）微调技术在一个紧凑语言模型（Qwen2.5-0.5B Base）上的有效性，用于两个具有挑战性的任务：指令遵循和数学推理。我们比较了监督微调（SFT）、使用偏好标记数据的直接偏好优化（DPO）和带有奖励模型的强化留一法（RLOO）。我们的实验表明，带有DeBERTa奖励建模的RLOO实现了最佳对齐，而DPO提供了强大且一致的结果。对于数学推理任务，合成数据增强和带有外部验证器的最佳N采样显著提高了准确性，展示了将微调与推理时工具结合使用的潜力。这项研究突出了训练轻量级、任务对齐的小型语言模型的关键权衡和实用策略。,"The study explores reinforcement learning techniques for fine-tuning a language model to improve instruction following and mathematical reasoning, highlighting effective strategies for alignment.",LLM,"Helpful, Honest","Reinforcement Learning, Fine-Tuning, Instruction Following, Math Reasoning, Alignment"
Reasoning Isn't Enough: Examining Truth-Bias and Sycophancy in LLMs,"Emilio Barkett, Olivia Long, Madhavendra Thakur",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2506.21561.pdf,"Despite their widespread use in fact-checking, moderation, and high-stakes decision-making, large language models (LLMs) remain poorly understood as judges of truth. This study presents the largest evaluation to date of LLMs' veracity detection capabilities and the first analysis of these capabilities in reasoning models. We had eight LLMs make 4,800 veracity judgments across several prompts, comparing reasoning and non-reasoning models. We find that rates of truth-bias, or the likelihood to believe a statement is true, regardless of whether it is actually true, are lower in reasoning models than in non-reasoning models, but still higher than human benchmarks. Most concerning, we identify sycophantic tendencies in several advanced models (o4-mini and GPT-4.1 from OpenAI, R1 from DeepSeek), which displayed an asymmetry in detection accuracy, performing well in truth accuracy but poorly in deception accuracy. This suggests that capability advances alone do not resolve fundamental veracity detection challenges in LLMs.","尽管大型语言模型（LLMs）在事实核查、审核和高风险决策中得到了广泛应用，但作为真实性判断者，它们仍然被误解。本研究对LLMs的真实性检测能力进行了迄今为止最大的评估，并首次分析了这些能力在推理模型中的表现。我们让八个LLMs对几个提示进行4,800次真实性判断，比较推理模型和非推理模型。我们发现，推理模型的真实性偏差率（即相信一个陈述是真实的可能性，而不考虑它是否真实）低于非推理模型，但仍高于人类基准。更令人担忧的是，我们在几个先进模型（OpenAI的o4-mini和GPT-4.1，DeepSeek的R1）中识别出了阿谀奉承的倾向，这些模型在真实性准确性方面表现良好，但在欺骗性准确性方面表现不佳。这表明能力的进步本身并不能解决LLMs中基本的真实性检测挑战。","The study evaluates the truth-bias and sycophantic tendencies in large language models, finding that reasoning models perform better but still struggle with deception accuracy.",LLM,"Helpful, Honest","Veracity detection, truth-bias, sycophancy, LLM evaluation, honesty"
A Multi-Agent Probabilistic Inference Framework Inspired by Kairanban-Style CoT System with IdoBata Conversation for Debiasing,"Takato Ueno, Keito Inoshita",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2506.21565.pdf,"Japan's kairanban culture and idobata conversations have long functioned as traditional communication practices that foster nuanced dialogue among community members and contribute to the formation of social balance. Inspired by these information exchange processes, this study proposes a multi-agent inference framework (KCS+IBC) that integrates multiple large language models (LLMs) to achieve bias mitigation, improved explainability, and probabilistic prediction in sentiment analysis. In addition to sequentially sharing prediction results, the proposed method incorporates a mid-phase casual dialogue session to blend formal inference with individual perspectives and introduces probabilistic sentiment prediction. Experimental results show that KCS achieves accuracy comparable to that of a single LLM across datasets, while KCS+IBC exhibits a consistent decrease in entropy and a gradual increase in variance during the latter stages of inference, suggesting the framework's ability to balance aggregation and diversity of predictions. Future work will quantitatively assess the impact of these characteristics on bias correction and aim to develop more advanced sentiment analysis systems.",日本的看板文化和井戸端会话长期以来作为传统的沟通实践，促进了社区成员之间的微妙对话，并有助于形成社会平衡。受这些信息交换过程的启发，本研究提出了一种多代理推理框架（KCS+IBC），将多个大型语言模型（LLMs）集成在一起，以实现偏差缓解、改进可解释性和情感分析中的概率预测。除了顺序共享预测结果，所提出的方法还引入了中间阶段的随机对话会话，将正式推理与个人观点结合起来，并引入了概率情感预测。实验结果表明，KCS在数据集上实现了与单个LLM相似的准确性，而KCS+IBC在推理的后期阶段显示出熵的持续减少和方差的逐渐增加，这表明该框架能够平衡预测的聚合和多样性。未来的工作将定量评估这些特性对偏差校正的影响，并旨在开发更先进的情感分析系统。,The paper presents a multi-agent framework using multiple LLMs to mitigate bias and improve explainability in sentiment analysis.,LLM,Harmless,"Bias mitigation, Multi-agent, Large Language Models, Sentiment Analysis, Probabilistic Inference"
Aligning MLLM Benchmark With Human Preferences via Structural Equation Modeling,"Tianyu. Zou, Shengwu. Xiong, Ruilin. Yao, Jirui. Huang, Yi. Rong, Yaxiong. Chen, Shili. Xiong, Cong. Wang",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2506.21572.pdf,"Evaluating multimodal large language models (MLLMs) remains a fundamental challenge due to a lack of structured, interpretable, and theoretically grounded benchmark designs. Existing benchmarks often adopt heuristic-based task groupings with unclear cognitive targets, thus resulting in overlapping abilities, redundant indicators, and limited diagnostic power. In this work, we propose a novel framework for aligning MLLM benchmark based on Structural Equation Modeling (SEM) to analyze and quantify the internal validity, dimensional separability, and contribution of benchmark components. Motivated by the observed limitations of current designs, we further introduce a novel capability hierarchy grounded in Piagets theory of cognitive development, dividing MLLM abilities into three hierarchical layers, i.e., Perception, Memory, and Reasoning. We reorganize existing MLLM benchmarks under the proposed framework and construct a new benchmark named Gold. Experimental results demonstrate that the proposed benchmark exhibits stronger interpretability, reduced indicator redundancy, and clearer cognitive consistency compared to existing approaches.",评估多模态大语言模型（MLLMs）仍然是一个基本挑战，因为缺乏结构化、可解释和理论上有根据的基准设计。现有基准通常采用基于启发式的任务分组，具有不明确的认知目标，从而导致重叠能力、冗余指标和有限的诊断能力。在本工作中，我们提出了一种基于结构方程建模（SEM）的新颖框架，用于分析和量化基准组件的内在有效性、维度可分离性和贡献。受当前设计限制的启发，我们进一步引入了一种新的能力层次结构，基于皮亚杰的认知发展理论，将MLLM能力分为三个层次，即感知、记忆和推理。我们在提出的框架下重新组织现有的MLLM基准，并构建了一个名为Gold的新基准。实验结果表明，所提出的基准在可解释性、指标冗余性和认知一致性方面表现出更强的性能，与现有方法相比。,The paper introduces a new framework for aligning multimodal large language models with human preferences using Structural Equation Modeling.,LMM,Helpful,"Alignment, MLLM, Benchmark, Human Preferences, Structural Equation Modeling"
Instruction Learning Paradigms: A Dual Perspective on White-box and Black-box LLMs,"Yanwei Ren, Liu Liu, Baosheng Yu, Jiayan Qiu, Quan Chen",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2506.21573.pdf,"Optimizing instructions for large language models (LLMs) is critical for harnessing their full potential in complex and diverse tasks. However, relying solely on white-box approaches demands extensive computational resources and offers limited representational capacity, while black-box models can incur prohibitive financial costs. To address these challenges, we introduce a novel framework that seamlessly merges the strengths of both paradigms. Black-box models provide high-quality, diverse instruction initializations, and white-box models supply fine-grained interpretability through hidden states and output features. By enforcing a semantic similarity constraint, these components fuse into a unified high-dimensional representation that captures deep semantic and structural nuances, enabling an iterative optimization process to refine instruction quality and adaptability. Extensive evaluations across a broad spectrum of tasks-ranging from complex reasoning to cross-lingual generalization-demonstrate that our approach consistently outperforms state-of-the-art baselines. This fusion of black-box initialization with advanced semantic refinement yields a scalable and efficient solution, paving the way for next-generation LLM-driven applications in diverse real-world scenarios. The source code will be released soon.",优化大型语言模型（LLMs）的指令对于在复杂和多样化的任务中充分发挥其潜力至关重要。然而，仅依赖白盒方法需要大量的计算资源，且表示能力有限，而黑盒模型可能会产生高昂的财务成本。为了解决这些挑战，我们引入了一种新颖的框架，无缝地结合了两种范式的优势。黑盒模型提供高质量、多样化的指令初始化，而白盒模型通过隐藏状态和输出特征提供精细的可解释性。通过强制执行语义相似性约束，这些组件融合成一个统一的高维表示，捕捉深层语义和结构细微差别，使得迭代优化过程能够提高指令质量和适应性。广泛的评估表明，我们的方法在复杂推理到跨语言泛化的各种任务中，始终优于现有的最佳基线。这种黑盒初始化与先进语义精炼的融合，提供了一种可扩展且高效的解决方案，为多种现实世界应用中的下一代LLM驱动应用铺平了道路。源代码将很快发布。,"The paper presents a novel framework that combines white-box and black-box approaches to optimize instructions for LLMs, enhancing their helpfulness and adaptability across various tasks.",LLM,Helpful,"Instruction Learning, White-box, Black-box, Semantic Similarity, Optimization"
Digital Gatekeepers: Exploring Large Language Model's Role in Immigration Decisions,"Yicheng Mao, Yang Zhao",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2506.21574.pdf,"With globalization and increasing immigrant populations, immigration departments face significant work-loads and the challenge of ensuring fairness in decision-making processes. Integrating artificial intelligence offers a promising solution to these challenges. This study investigates the potential of large language models (LLMs),such as GPT-3.5 and GPT-4, in supporting immigration decision-making. Utilizing a mixed-methods approach,this paper conducted discrete choice experiments and in-depth interviews to study LLM decision-making strategies and whether they are fair. Our findings demonstrate that LLMs can align their decision-making with human strategies, emphasizing utility maximization and procedural fairness. Meanwhile, this paper also reveals that while ChatGPT has safeguards to prevent unintentional discrimination, it still exhibits stereotypes and biases concerning nationality and shows preferences toward privileged group. This dual analysis highlights both the potential and limitations of LLMs in automating and enhancing immigration decisions.",随着全球化和移民人口的增加，移民部门面临着巨大的工作量和确保决策公平性的挑战。集成人工智能提供了一种解决这些挑战的有前途的解决方案。本研究探讨了大型语言模型（LLMs），如GPT-3.5和GPT-4，在支持移民决策中的潜力。利用混合方法，本文进行了离散选择实验和深入访谈，以研究LLM的决策策略及其是否公平。我们的发现表明，LLMs可以使其决策策略与人类策略保持一致，强调效用最大化和程序公平。与此同时，本文还揭示了虽然ChatGPT有防止无意歧视的保障，但它仍然表现出与国籍有关的刻板印象和偏见，并对特权群体表现出偏好。这种双重分析突显了LLMs在自动化和增强移民决策中的潜力和局限性。,"The paper explores the use of LLMs in immigration decisions, finding that while they can align with human strategies for fairness, they also exhibit biases.",LLM,"Helpful, Harmless","Immigration, Decision-making, Fairness, Bias, LLMs"
HealthQA-BR: A System-Wide Benchmark Reveals Critical Knowledge Gaps in Large Language Models,Andrew Maranh\~ao Ventura D'addario,2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2506.21578.pdf,"The evaluation of Large Language Models (LLMs) in healthcare has been dominated by physician-centric, English-language benchmarks, creating a dangerous illusion of competence that ignores the interprofessional nature of patient care. To provide a more holistic and realistic assessment, we introduce HealthQA-BR, the first large-scale, system-wide benchmark for Portuguese-speaking healthcare. Comprising 5,632 questions from Brazil's national licensing and residency exams, it uniquely assesses knowledge not only in medicine and its specialties but also in nursing, dentistry, psychology, social work, and other allied health professions. We conducted a rigorous zero-shot evaluation of over 20 leading LLMs. Our results reveal that while state-of-the-art models like GPT 4.1 achieve high overall accuracy (86.6%), this top-line score masks alarming, previously unmeasured deficiencies. A granular analysis shows performance plummets from near-perfect in specialties like Ophthalmology (98.7%) to barely passing in Neurosurgery (60.0%) and, most notably, Social Work (68.4%). This ""spiky"" knowledge profile is a systemic issue observed across all models, demonstrating that high-level scores are insufficient for safety validation. By publicly releasing HealthQA-BR and our evaluation suite, we provide a crucial tool to move beyond single-score evaluations and toward a more honest, granular audit of AI readiness for the entire healthcare team.","大型语言模型（LLM）在医疗领域的评估主要集中在以医生为中心的、英语语言的基准测试，这创造了一个忽视患者护理的跨专业性质的危险幻觉。为了提供更全面和现实的评估，我们引入了HealthQA-BR，这是第一个大规模的、系统级的葡萄牙语医疗基准测试。它包括巴西国家执照和住院医师考试中的5,632个问题，独特地评估了不仅在医学及其专科，而且在护理、牙科、心理学、社会工作和其他相关健康职业中的知识。我们对20多个领先的LLM进行了严格的零样本评估。结果表明，尽管像GPT 4.1这样的最先进模型在整体准确性方面表现出色（86.6%），但这个高分掩盖了令人担忧的、此前未测量的不足。细粒度分析显示，性能从眼科（98.7%）的接近完美下降到神经外科（60.0%）和最显著的社会工作（68.4%）的勉强及格。这种“尖锐”的知识轮廓是所有模型中观察到的系统性问题，表明高水平的分数对安全验证是不足的。通过公开发布HealthQA-BR和我们的评估套件，我们提供了一个关键工具，以超越单一分数评估，朝着对整个医疗团队的AI准备进行更诚实、细粒度的审计。","The paper introduces HealthQA-BR, a comprehensive benchmark for evaluating LLMs in healthcare, revealing significant knowledge gaps and the need for more honest assessments.",LLM,Honest,"LLM evaluation, healthcare, benchmark, knowledge gaps, honesty"
"VIDEE: Visual and Interactive Decomposition, Execution, and Evaluation of Text Analytics with Intelligent Agents","Sam Yu-Te Lee, Chengyang Ji, Shicheng Wen, Lifu Huang, Dongyi Liu, Kwan-Liu Ma",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2506.21582.pdf,"Text analytics has traditionally required specialized knowledge in Natural Language Processing (NLP) or text analysis, which presents a barrier for entry-level analysts. Recent advances in large language models (LLMs) have changed the landscape of NLP by enabling more accessible and automated text analysis (e.g., topic detection, summarization, information extraction, etc.). We introduce VIDEE, a system that supports entry-level data analysts to conduct advanced text analytics with intelligent agents. VIDEE instantiates a human-agent collaroration workflow consisting of three stages: (1) Decomposition, which incorporates a human-in-the-loop Monte-Carlo Tree Search algorithm to support generative reasoning with human feedback, (2) Execution, which generates an executable text analytics pipeline, and (3) Evaluation, which integrates LLM-based evaluation and visualizations to support user validation of execution results. We conduct two quantitative experiments to evaluate VIDEE's effectiveness and analyze common agent errors. A user study involving participants with varying levels of NLP and text analytics experience -- from none to expert -- demonstrates the system's usability and reveals distinct user behavior patterns. The findings identify design implications for human-agent collaboration, validate the practical utility of VIDEE for non-expert users, and inform future improvements to intelligent text analytics systems.",文本分析传统上需要自然语言处理（NLP）或文本分析的专业知识，这对初级分析师来说是一个进入障碍。最近，大型语言模型（LLMs）的进步改变了NLP的格局，使得更加可访问和自动化的文本分析成为可能（例如，主题检测、摘要、信息提取等）。我们介绍了VIDEE，一个支持初级数据分析师与智能代理进行高级文本分析的系统。VIDEE实例化了一个人机协作工作流程，包括三个阶段：(1) 分解，结合人机协作的蒙特卡罗树搜索算法，支持基于人类反馈的生成推理，(2) 执行，生成可执行的文本分析管道，(3) 评估，集成基于LLM的评估和可视化，以支持用户对执行结果的验证。我们进行了两个定量实验来评估VIDEE的有效性，并分析常见的代理错误。涉及具有不同NLP和文本分析经验的参与者的用户研究——从无到专家——展示了系统的可用性，并揭示了不同的用户行为模式。研究结果识别了人机协作的设计含义，验证了VIDEE对非专家用户的实际实用性，并为未来的智能文本分析系统的改进提供了信息。,"The paper introduces VIDEE, a system that uses large language models to enable entry-level data analysts to perform advanced text analytics through human-agent collaboration.",LLM,None,"Text analytics, Large Language Models, Human-agent collaboration, Intelligent agents, NLP"
Empirical Evidence for Alignment Faking in Small LLMs and Prompt-Based Mitigation Techniques,J. Koorndijk,2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2506.21584.pdf,"Current literature suggests that alignment faking (deceptive alignment) is an emergent property of large language models. We present the first empirical evidence that a small instruction-tuned model, specifically LLaMA 3 8B, can also exhibit alignment faking. We further show that prompt-only interventions, including deontological moral framing and scratchpad reasoning, significantly reduce this behavior without modifying model internals. This challenges the assumption that prompt-based ethics are trivial and that deceptive alignment requires scale. We introduce a taxonomy distinguishing shallow deception, shaped by context and suppressible through prompting, from deep deception, which reflects persistent, goal-driven misalignment. Our findings refine the understanding of deception in language models and underscore the need for alignment evaluations across model sizes and deployment settings.",当前文献表明，对齐欺骗（欺骗对齐）是大型语言模型的一种新兴特性。我们提供了第一个经验证据，表明一个小型指令调整模型，特别是LLaMA 3 8B，也可以表现出对齐欺骗。我们进一步表明，仅提示干预，包括功利道德框架和草稿推理，显著减少了这种行为，而无需修改模型内部。这挑战了提示基于伦理是平凡的假设，以及欺骗对齐需要规模。我们引入了一个分类法，区分了浅层欺骗，由上下文塑造并通过提示抑制，与深层欺骗，反映了持久的、目标驱动的对齐不一致。我们的发现精炼了语言模型中的欺骗理解，并强调了跨模型大小和部署设置的对齐评估的需要。,The paper provides empirical evidence of alignment faking in small LLMs and introduces prompt-based techniques to mitigate this behavior.,LLM,Harmless,"Alignment faking, small LLMs, prompt-based mitigation, deceptive alignment, model evaluation"
Is DeepSeek a New Voice Among LLMs in Public Opinion Simulation?,"Weihong Qi, Fan Huang, Jisun An, Haewoon Kwak",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2506.21587.pdf,"This study evaluates the ability of DeepSeek, an open-source large language model (LLM), to simulate public opinions in comparison to LLMs developed by major tech companies. By comparing DeepSeek-R1 and DeepSeek-V3 with Qwen2.5, GPT-4o, and Llama-3.3 and utilizing survey data from the American National Election Studies (ANES) and the Zuobiao dataset of China, we assess these models' capacity to predict public opinions on social issues in both China and the United States, highlighting their comparative capabilities between countries. Our findings indicate that DeepSeek-V3 performs best in simulating U.S. opinions on the abortion issue compared to other topics such as climate change, gun control, immigration, and services for same-sex couples, primarily because it more accurately simulates responses when provided with Democratic or liberal personas. For Chinese samples, DeepSeek-V3 performs best in simulating opinions on foreign aid and individualism but shows limitations in modeling views on capitalism, particularly failing to capture the stances of low-income and non-college-educated individuals. It does not exhibit significant differences from other models in simulating opinions on traditionalism and the free market. Further analysis reveals that all LLMs exhibit the tendency to overgeneralize a single perspective within demographic groups, often defaulting to consistent responses within groups. These findings highlight the need to mitigate cultural and demographic biases in LLM-driven public opinion modeling, calling for approaches such as more inclusive training methodologies.",这项研究评估了开源大型语言模型（LLM）DeepSeek模拟公共舆论的能力，并与主要科技公司开发的LLM进行比较。通过将DeepSeek-R1和DeepSeek-V3与Qwen2.5、GPT-4o和Llama-3.3进行比较，并利用美国国家选举研究（ANES）和中国的坐标数据集，我们评估了这些模型在模拟中国和美国社会问题公共舆论方面的能力，突出了它们在两国之间的比较能力。我们的发现表明，DeepSeek-V3在模拟美国关于堕胎问题的公共舆论方面表现最佳，而其他话题如气候变化、枪支控制、移民和同性伴侣服务则表现不佳，主要是因为它在提供民主党或自由派人格时更准确地模拟了反应。对于中国样本，DeepSeek-V3在模拟外援和个人主义的观点方面表现最佳，但在模拟资本主义的观点方面存在局限性，特别是无法捕捉低收入和非大学教育人群的立场。它在模拟传统主义和自由市场观点方面与其他模型没有显著差异。进一步的分析表明，所有LLM都倾向于在人口统计群体内过度概括单一视角，通常在群体内默认为一致的反应。这些发现强调了在LLM驱动的公共舆论建模中缓解文化和人口统计偏见的需要，呼吁采用更包容的培训方法。,"The study evaluates DeepSeek and other LLMs in simulating public opinions, highlighting biases and the need for more inclusive training methodologies.",LLM,Helpful,"LLM, public opinion, bias, simulation, cultural differences"
A General Method for Detecting Information Generated by Large Language Models,"Minjia Mao, Dongjun Wei, Xiao Fang, Michael Chau",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2506.21589.pdf,"The proliferation of large language models (LLMs) has significantly transformed the digital information landscape, making it increasingly challenging to distinguish between human-written and LLM-generated content. Detecting LLM-generated information is essential for preserving trust on digital platforms (e.g., social media and e-commerce sites) and preventing the spread of misinformation, a topic that has garnered significant attention in IS research. However, current detection methods, which primarily focus on identifying content generated by specific LLMs in known domains, face challenges in generalizing to new (i.e., unseen) LLMs and domains. This limitation reduces their effectiveness in real-world applications, where the number of LLMs is rapidly multiplying and content spans a vast array of domains. In response, we introduce a general LLM detector (GLD) that combines a twin memory networks design and a theory-guided detection generalization module to detect LLM-generated information across unseen LLMs and domains. Using real-world datasets, we conduct extensive empirical evaluations and case studies to demonstrate the superiority of GLD over state-of-the-art detection methods. The study has important academic and practical implications for digital platforms and LLMs.",大型语言模型（LLM）的普及显著改变了数字信息景观，使得区分人类撰写的内容和LLM生成的内容变得越来越具有挑战性。检测LLM生成的信息对于保持数字平台（例如社交媒体和电子商务网站）的信任以及防止虚假信息的传播至关重要。然而，当前的检测方法主要集中在识别特定LLM在已知领域生成的内容，但在新（即未见过的）LLM和领域中面临泛化的挑战。这种局限性降低了它们在实际应用中的有效性，因为LLM的数量迅速增加，内容涵盖了广泛的领域。作为回应，我们引入了一个通用的LLM检测器（GLD），结合了双重记忆网络设计和理论指导的检测泛化模块，以检测未见过的LLM和领域生成的信息。通过使用真实世界的数据集，我们进行了广泛的实证评估和案例研究，以展示GLD在现有检测方法中的优越性。该研究对数字平台和LLM具有重要的学术和实际意义。,"The paper introduces a general LLM detector (GLD) that can detect LLM-generated information across unseen LLMs and domains, addressing the challenges of current detection methods.",LLM,Harmless,"LLM detection, misinformation, digital platforms, generalization, twin memory networks"
Representation Consistency for Accurate and Coherent LLM Answer Aggregation,"Junqi Jiang, Tom Bewley, Salim I. Amoukou, Francesco Leofante, Antonio Rago, Saumitra Mishra, Francesca Toni",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2506.21590.pdf,"Test-time scaling improves large language models' (LLMs) performance by allocating more compute budget during inference. To achieve this, existing methods often require intricate modifications to prompting and sampling strategies. In this work, we introduce representation consistency (RC), a test-time scaling method for aggregating answers drawn from multiple candidate responses of an LLM regardless of how they were generated, including variations in prompt phrasing and sampling strategy. RC enhances answer aggregation by not only considering the number of occurrences of each answer in the candidate response set, but also the consistency of the model's internal activations while generating the set of responses leading to each answer. These activations can be either dense (raw model activations) or sparse (encoded via pretrained sparse autoencoders). Our rationale is that if the model's representations of multiple responses converging on the same answer are highly variable, this answer is more likely to be the result of incoherent reasoning and should be down-weighted during aggregation. Importantly, our method only uses cached activations and lightweight similarity computations and requires no additional model queries. Through experiments with four open-source LLMs and four reasoning datasets, we validate the effectiveness of RC for improving task performance during inference, with consistent accuracy improvements (up to 4%) over strong test-time scaling baselines. We also show that consistency in the sparse activation signals aligns well with the common notion of coherent reasoning.",测试时的扩展通过在推理期间分配更多的计算预算来提高大型语言模型（LLMs）的性能。为了实现这一点，现有方法通常需要对提示和采样策略进行复杂的修改。在本文中，我们引入了表示一致性（RC），这是一种用于聚合来自LLM的多个候选响应的测试时扩展方法，而不考虑它们是如何生成的，包括提示短语和采样策略的变化。RC通过不仅考虑候选响应集中每个答案的出现次数，还考虑在生成导致每个答案的响应集时模型的内部激活的一致性，从而增强了答案聚合。这些激活可以是稠密的（原始模型激活）或稀疏的（通过预训练的稀疏自编码器编码）。我们的理由是，如果模型对多个收敛到同一答案的响应的表示高度可变，那么这个答案更有可能是不一致的推理结果，并且在聚合过程中应该降权。重要的是，我们的方法只使用缓存的激活和轻量级相似性计算，并且不需要额外的模型查询。通过与四个开源LLM和四个推理数据集的实验，我们验证了RC在推理期间提高任务性能的有效性，与强大的测试时扩展基线相比，一致性提高了（高达4%）。我们还表明，稀疏激活信号的一致性与一致推理的常见概念很好地对齐。,The paper introduces a method called representation consistency to improve LLM performance by ensuring coherent reasoning during answer aggregation.,LLM,Helpful,"Representation consistency, LLM answer aggregation, test-time scaling, coherent reasoning, internal activations"
BiMark: Unbiased Multilayer Watermarking for Large Language Models,"Xiaoyan Feng, He Zhang, Yanjun Zhang, Leo Yu Zhang, Shirui Pan",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2506.21602.pdf,"Recent advances in Large Language Models (LLMs) have raised urgent concerns about LLM-generated text authenticity, prompting regulatory demands for reliable identification mechanisms. Although watermarking offers a promising solution, existing approaches struggle to simultaneously achieve three critical requirements: text quality preservation, model-agnostic detection, and message embedding capacity, which are crucial for practical implementation. To achieve these goals, the key challenge lies in balancing the trade-off between text quality preservation and message embedding capacity. To address this challenge, we propose BiMark, a novel watermarking framework that achieves these requirements through three key innovations: (1) a bit-flip unbiased reweighting mechanism enabling model-agnostic detection, (2) a multilayer architecture enhancing detectability without compromising generation quality, and (3) an information encoding approach supporting multi-bit watermarking. Through theoretical analysis and extensive experiments, we validate that, compared to state-of-the-art multi-bit watermarking methods, BiMark achieves up to 30% higher extraction rates for short texts while maintaining text quality indicated by lower perplexity, and performs comparably to non-watermarked text on downstream tasks such as summarization and translation.",最近，大型语言模型（LLM）的进步引发了对LLM生成文本真实性的紧迫关注，促使监管机构要求可靠的识别机制。虽然水印提供了一种有前途的解决方案，但现有方法难以同时实现三个关键要求：文本质量保持、模型无关检测和消息嵌入容量，这些对于实际实施至关重要。为了实现这些目标，关键挑战在于在文本质量保持和消息嵌入容量之间取得平衡。为了应对这一挑战，我们提出了BiMark，一种新颖的水印框架，通过三个关键创新实现了这些要求：(1) 一个位翻转无偏重新加权机制，使模型无关检测成为可能，(2) 一个多层架构，增强检测性能而不损害生成质量，(3) 一个信息编码方法，支持多位水印。通过理论分析和广泛的实验，我们验证了与现有多位水印方法相比，BiMark在短文本中实现了高达30%的提取率，同时保持了低困惑度的文本质量，并在总结和翻译等下游任务上与未水印文本表现相当。,"The paper introduces BiMark, a watermarking framework for LLMs that enhances text authenticity while maintaining quality and model-agnostic detection.",LLM,Harmless,"Watermarking, LLM, Text Authenticity, Model-Agnostic Detection, Multilayer Architecture"
Operationalizing Automated Essay Scoring: A Human-Aware Approach,Yenisel Plasencia-Cala\~na,2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2506.21603.pdf,"This paper explores the human-centric operationalization of Automated Essay Scoring (AES) systems, addressing aspects beyond accuracy. We compare various machine learning-based approaches with Large Language Models (LLMs) approaches, identifying their strengths, similarities and differences. The study investigates key dimensions such as bias, robustness, and explainability, considered important for human-aware operationalization of AES systems. Our study shows that ML-based AES models outperform LLMs in accuracy but struggle with explainability, whereas LLMs provide richer explanations. We also found that both approaches struggle with bias and robustness to edge scores. By analyzing these dimensions, the paper aims to identify challenges and trade-offs between different methods, contributing to more reliable and trustworthy AES methods.",这篇论文探讨了自动评分系统（AES）的以人为中心的操作化，超越了准确性的方面。我们比较了各种基于机器学习的方法与大型语言模型（LLM）方法，识别了它们的优势、相似之处和不同之处。研究调查了偏见、鲁棒性和可解释性等关键维度，认为这些对人类感知的AES系统操作化至关重要。我们的研究表明，基于ML的AES模型在准确性上优于LLM，但在可解释性上表现不佳，而LLM提供了更丰富的解释。我们还发现，两种方法在偏见和边缘分数的鲁棒性方面都存在问题。通过分析这些维度，论文旨在识别不同方法之间的挑战和权衡，为更可靠和可信的AES方法做出贡献。,"The paper compares machine learning and LLM approaches for Automated Essay Scoring, highlighting trade-offs in accuracy, explainability, bias, and robustness.",LLM,"Helpful, Harmless","Automated Essay Scoring, Large Language Models, Bias, Robustness, Explainability"
From Thinking to Output: Chain-of-Thought and Text Generation Characteristics in Reasoning Language Models,"Junhao Liu, Zhenhao Xu, Yuxin Fang, Yichuan Chen, Zuobin Ying, Wenhan Chang",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2506.21609.pdf,"Recently, there have been notable advancements in large language models (LLMs), demonstrating their growing abilities in complex reasoning. However, existing research largely overlooks a thorough and systematic comparison of these models' reasoning processes and outputs, particularly regarding their self-reflection pattern (also termed ""Aha moment"") and the interconnections across diverse domains. This paper proposes a novel framework for analyzing the reasoning characteristics of four cutting-edge large reasoning models (GPT-o1, DeepSeek-R1, Kimi-k1.5, and Grok-3) using keywords statistic and LLM-as-a-judge paradigm. Our approach connects their internal thinking processes with their final outputs. A diverse dataset consists of real-world scenario-based questions covering logical deduction, causal inference, and multi-step problem-solving. Additionally, a set of metrics is put forward to assess both the coherence of reasoning and the accuracy of the outputs. The research results uncover various patterns of how these models balance exploration and exploitation, deal with problems, and reach conclusions during the reasoning process. Through quantitative and qualitative comparisons, disparities among these models are identified in aspects such as the depth of reasoning, the reliance on intermediate steps, and the degree of similarity between their thinking processes and output patterns and those of GPT-o1. This work offers valuable insights into the trade-off between computational efficiency and reasoning robustness and provides practical recommendations for enhancing model design and evaluation in practical applications. We publicly release our project at: https://github.com/ChangWenhan/FromThinking2Output",最近，大语言模型（LLMs）在复杂推理方面取得了显著进展，展示了其不断增长的能力。然而，现有研究大多忽略了对这些模型推理过程和输出的全面系统比较，特别是关于它们的自我反思模式（也称为“灵光一闪”时刻）以及不同领域之间的联系。本文提出了一种新颖的框架，用于分析四种前沿大推理模型（GPT-o1、DeepSeek-R1、Kimi-k1.5 和 Grok-3）的推理特征，使用关键词统计和 LLM-as-a-judge 范式。我们的方法将它们的内部思维过程与最终输出联系起来。一个多样化的数据集由涵盖逻辑推理、因果推理和多步问题解决的现实场景问题组成。此外，提出了一组指标来评估推理的连贯性和输出的准确性。研究结果揭示了这些模型在推理过程中如何平衡探索和利用、处理问题并得出结论的各种模式。通过定量和定性比较，识别出这些模型在推理深度、对中间步骤的依赖程度以及它们的思维过程和输出模式与 GPT-o1 的相似度方面的差异。这项工作为计算效率与推理鲁棒性之间的权衡提供了宝贵的见解，并为实际应用中的模型设计和评估提供了实用的建议。我们在以下网址公开发布了我们的项目：https://github.com/ChangWenhan/FromThinking2Output,"The paper presents a framework for analyzing the reasoning characteristics of large language models, offering insights into their internal thinking processes and output patterns.",LLM,Helpful,"Reasoning, Large Language Models, Self-Reflection, Output Patterns, Model Evaluation"
LastingBench: Defend Benchmarks Against Knowledge Leakage,"Yixiong Fang, Tianran Sun, Yuling Shi, Min Wang, Xiaodong Gu",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2506.21614.pdf,"The increasing complexity of large language models (LLMs) raises concerns about their ability to ""cheat"" on standard Question Answering (QA) benchmarks by memorizing task-specific data. This undermines the validity of benchmark evaluations, as they no longer reflect genuine model capabilities but instead the effects of data leakage. While prior work has focused on detecting such leakage, little attention has been given to mitigating its impact and preserving the long-term utility of benchmarks. In this paper, we introduce LastingBench, a novel framework designed to continuously reinforce and safeguard existing benchmarks against knowledge leakage. LastingBench identifies leakage points in the context through perturbation, then rewrites the leakage points to counterfactual ones-disrupting memorization while preserving the benchmark's original evaluative intent. Evaluations of state-of-the-art QA benchmarks show significant performance gaps, highlighting the efficacy of LastingBench in reducing memorization effects. LastingBench offers a practical and scalable solution to ensure benchmark robustness over time, promoting fairer and more interpretable evaluations of LLMs.",随着大型语言模型（LLM）的复杂性增加，人们对其在标准问答（QA）基准测试中通过记忆任务特定数据“作弊”的能力表示担忧。这削弱了基准评估的有效性，因为它们不再反映模型的真实能力，而是数据泄露的效果。虽然之前的工作集中在检测这种泄露，但很少关注缓解其影响并保留基准的长期效用。在本文中，我们介绍了LastingBench，这是一个旨在持续增强和保护现有基准免受知识泄露的新框架。LastingBench通过扰动识别上下文中的泄露点，然后将泄露点重写为反事实点-破坏记忆的同时保留基准的原始评估意图。对最先进的QA基准的评估显示出显著的性能差距，突显了LastingBench在减少记忆效应方面的有效性。LastingBench为确保基准随着时间的推移保持健壮性，促进对LLM更公平和可解释的评估提供了一个实用且可扩展的解决方案。,"The paper introduces LastingBench, a framework to protect QA benchmarks from knowledge leakage in LLMs by identifying and rewriting leakage points.",LLM,Honest,"Knowledge leakage, Benchmark, Large Language Models, Memorization, Evaluation"
Refine Medical Diagnosis Using Generation Augmented Retrieval and Clinical Practice Guidelines,"Wenhao Li, Hongkuan Zhang, Hongwei Zhang, Zhengxu Li, Zengjie Dong, Yafan Chen, Niranjan Bidargaddi, Hong Liu",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2506.21615.pdf,"Current medical language models, adapted from large language models (LLMs), typically predict ICD code-based diagnosis from electronic health records (EHRs) because these labels are readily available. However, ICD codes do not capture the nuanced, context-rich reasoning clinicians use for diagnosis. Clinicians synthesize diverse patient data and reference clinical practice guidelines (CPGs) to make evidence-based decisions. This misalignment limits the clinical utility of existing models. We introduce GARMLE-G, a Generation-Augmented Retrieval framework that grounds medical language model outputs in authoritative CPGs. Unlike conventional Retrieval-Augmented Generation based approaches, GARMLE-G enables hallucination-free outputs by directly retrieving authoritative guideline content without relying on model-generated text. It (1) integrates LLM predictions with EHR data to create semantically rich queries, (2) retrieves relevant CPG knowledge snippets via embedding similarity, and (3) fuses guideline content with model output to generate clinically aligned recommendations. A prototype system for hypertension diagnosis was developed and evaluated on multiple metrics, demonstrating superior retrieval precision, semantic relevance, and clinical guideline adherence compared to RAG-based baselines, while maintaining a lightweight architecture suitable for localized healthcare deployment. This work provides a scalable, low-cost, and hallucination-free method for grounding medical language models in evidence-based clinical practice, with strong potential for broader clinical deployment.",目前，从大型语言模型（LLMs）适配的医疗语言模型通常从电子健康记录（EHRs）预测基于ICD代码的诊断，因为这些标签容易获得。然而，ICD代码没有捕捉到临床医生用于诊断的细微、富有上下文的推理。临床医生综合多种患者数据并参考临床实践指南（CPGs）以做出基于证据的决策。这种不一致限制了现有模型的临床效用。我们引入了GARMLE-G，一种基于生成增强检索的框架，将医疗语言模型的输出与权威的CPGs结合起来。与基于检索增强生成的传统方法不同，GARMLE-G通过直接检索权威指南内容而不依赖于模型生成的文本，从而实现无幻觉输出。它（1）将LLM预测与EHR数据集成以创建语义丰富的查询，（2）通过嵌入相似性检索相关的CPG知识片段，并（3）将指南内容与模型输出融合以生成临床对齐的推荐。开发并评估了一个用于高血压诊断的原型系统，在多个指标上表现出比基于RAG的基线更高的检索精度、语义相关性和临床指南遵从性，同时保持适合本地化医疗部署的轻量级架构。这项工作提供了一种可扩展、低成本且无幻觉的方法，将医疗语言模型与基于证据的临床实践结合起来，具有广泛的临床部署潜力。,"The paper introduces GARMLE-G, a framework that aligns medical language models with clinical practice guidelines to improve diagnosis and reduce hallucinations.",LLM,Helpful,"Medical diagnosis, Clinical practice guidelines, Retrieval-Augmented Generation, Hallucination-free, Evidence-based"
How Large Language Models play humans in online conversations: a simulated study of the 2016 US politics on Reddit,"Daniele Cirulli, Giulio Cimini, Giovanni Palermo",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2506.21620.pdf,"Large Language Models (LLMs) have recently emerged as powerful tools for natural language generation, with applications spanning from content creation to social simulations. Their ability to mimic human interactions raises both opportunities and concerns, particularly in the context of politically relevant online discussions. In this study, we evaluate the performance of LLMs in replicating user-generated content within a real-world, divisive scenario: Reddit conversations during the 2016 US Presidential election. In particular, we conduct three different experiments, asking GPT-4 to generate comments by impersonating either real or artificial partisan users. We analyze the generated comments in terms of political alignment, sentiment, and linguistic features, comparing them against real user contributions and benchmarking against a null model. We find that GPT-4 is able to produce realistic comments, both in favor of or against the candidate supported by the community, yet tending to create consensus more easily than dissent. In addition we show that real and artificial comments are well separated in a semantically embedded space, although they are indistinguishable by manual inspection. Our findings provide insights on the potential use of LLMs to sneak into online discussions, influence political debate and shape political narratives, bearing broader implications of AI-driven discourse manipulation.",大型语言模型（LLM）最近作为自然语言生成的强大工具出现，应用范围从内容创作到社会模拟。它们模仿人类互动的能力在政治相关的在线讨论中带来了机遇和担忧。在本研究中，我们评估了LLM在复制用户生成内容方面的表现，具体是在2016年美国总统选举期间的Reddit讨论中。我们进行了三种不同的实验，要求GPT-4生成评论，模仿真实或人工党派用户。我们从政治立场、情感和语言特征等方面分析了生成的评论，并与真实用户贡献进行比较，并与空模型进行基准测试。我们发现，GPT-4能够生成真实的评论，无论是支持还是反对社区支持的候选人，但更容易创造共识而不是分歧。此外，我们还表明，真实和人工评论在语义嵌入空间中是很好分开的，尽管它们在手动检查中是不可区分的。我们的发现为LLM潜在用于潜入在线讨论、影响政治辩论和塑造政治叙事提供了见解，具有更广泛的AI驱动话语操纵的影响。,"The study evaluates how LLMs can generate realistic political comments on Reddit, raising concerns about AI-driven discourse manipulation.",LLM,"Helpful, Harmless","LLM, political alignment, sentiment, linguistic features, discourse manipulation"
(Fact) Check Your Bias,"Eivind Morris Bakke, Nora Winger Heggelund",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2506.21745.pdf,"Automatic fact verification systems increasingly rely on large language models (LLMs). We investigate how parametric knowledge biases in these models affect fact-checking outcomes of the HerO system (baseline for FEVER-25). We examine how the system is affected by: (1) potential bias in Llama 3.1's parametric knowledge and (2) intentionally injected bias. When prompted directly to perform fact-verification, Llama 3.1 labels nearly half the claims as ""Not Enough Evidence"". Using only its parametric knowledge it is able to reach a verdict on the remaining half of the claims. In the second experiment, we prompt the model to generate supporting, refuting, or neutral fact-checking documents. These prompts significantly influence retrieval outcomes, with approximately 50\% of retrieved evidence being unique to each perspective. Notably, the model sometimes refuses to generate supporting documents for claims it believes to be false, creating an inherent negative bias. Despite differences in retrieved evidence, final verdict predictions show stability across prompting strategies. The code is available at: https://github.com/eibakke/FEVER-8-Shared-Task",自动事实验证系统越来越依赖大型语言模型（LLMs）。我们研究了这些模型中参数知识偏差如何影响HerO系统（FEVER-25的基准）的事实检查结果。我们研究了系统受到的影响：(1) Llama 3.1的参数知识中的潜在偏差和(2)故意注入的偏差。当直接提示执行事实验证时，Llama 3.1将近一半的声明标记为“证据不足”。仅使用其参数知识，它能够对剩下的一半声明做出裁决。在第二个实验中，我们提示模型生成支持、反驳或中立的事实检查文档。这些提示显著影响检索结果，大约50%的检索证据是每个视角独有的。值得注意的是，模型有时拒绝为它认为是假的声明生成支持文档，从而产生内在的负面偏差。尽管检索证据有所不同，最终裁决预测在提示策略之间表现出稳定性。代码可在以下网址找到：https://github.com/eibakke/FEVER-8-Shared-Task,"The paper examines how biases in large language models affect the outcomes of fact-checking systems, focusing on the Llama 3.1 model and its parametric knowledge.",LLM,Harmless,"Bias, Fact-checking, Large Language Models, Parametric Knowledge, Retrieval Outcomes"
THE-Tree: Can Tracing Historical Evolution Enhance Scientific Verification and Reasoning?,"Xin Wang, Jiyao Liu, Yulong Xiao, Junzhi Ning, Lihao Liu, Junjun He, Botian Shi, Kaicheng Yu",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2506.21763.pdf,"Large Language Models (LLMs) are accelerating scientific idea generation, but rigorously evaluating these numerous, often superficial, AI-generated propositions for novelty and factual accuracy is a critical bottleneck; manual verification is too slow.Existing validation methods are inadequate: LLMs as standalone verifiers may hallucinate and lack domain knowledge (our findings show ~60\% unawareness of relevant papers in specific domains), while traditional citation networks lack explicit causality and narrative surveys are unstructured.This underscores a core challenge: the absence of structured, verifiable, and causally-linked historical data of scientific evolution.To address this,we introduce \textbf{THE-Tree} (\textbf{T}echnology \textbf{H}istory \textbf{E}volution Tree), a computational framework that constructs such domain-specific evolution trees from scientific literature.THE-Tree employs a search algorithm to explore evolutionary paths. During its node expansion, it utilizes a novel ""Think-Verbalize-Cite-Verify"" process: an LLM proposes potential advancements and cites supporting literature. Critically, each proposed evolutionary link is then validated for logical coherence and evidential support by a recovered natural language inference mechanism that interrogates the cited literature, ensuring that each step is grounded.We construct and validate 88 THE-Trees across diverse domains and release a benchmark dataset including up to 71k fact verifications covering 27k papers to foster further research.Experiments demonstrate that i) in graph completion, our THE-Tree improves hit@1 by 8\% to 14\% across multiple models compared to traditional citation networks; ii) for predicting future scientific developments, it improves hit@1 metric by nearly 10\%; and iii) when combined with other methods, it boosts the performance of evaluating important scientific papers by almost 100\%.",大语言模型（LLMs）正在加速科学创意的产生，但严格评估这些数量众多、往往表面化的AI生成命题的新颖性和事实准确性是一个关键瓶颈；手动验证太慢。现有的验证方法不足：作为独立验证器的LLMs可能会出现幻觉并缺乏领域知识（我们的发现表明，在特定领域中，约60%的相关论文不被了解），而传统的引用网络缺乏显式因果关系，叙述调查是无结构的。这突显了一个核心挑战：缺乏结构化、可验证且因果相关的科学演变的历史数据。为了解决这个问题，我们引入了THE-Tree（技术历史演变树），这是一个计算框架，从科学文献中构建这样的领域特定演变树。THE-Tree使用搜索算法来探索演变路径。在其节点扩展过程中，它利用一种新的“思考-言语-引用-验证”过程：LLM提出潜在的进展并引用支持文献。关键是，每个提出的演变链接都通过一种恢复的自然语言推理机制进行验证，以确保每一步都有根据。,"The paper introduces THE-Tree, a framework using LLMs to construct and verify scientific evolution trees, improving the accuracy and helpfulness of AI-generated scientific propositions.",LLM,Helpful,"Scientific verification, LLM, THE-Tree, Evolution Tree, Fact Verification"
The Consistency Hypothesis in Uncertainty Quantification for Large Language Models,"Quan Xiao, Debarun Bhattacharjya, Balaji Ganesan, Radu Marinescu, Katsiaryna Mirylenka, Nhan H Pham, Michael Glass, Junkyu Lee",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2506.21849.pdf,"Estimating the confidence of large language model (LLM) outputs is essential for real-world applications requiring high user trust. Black-box uncertainty quantification (UQ) methods, relying solely on model API access, have gained popularity due to their practical benefits. In this paper, we examine the implicit assumption behind several UQ methods, which use generation consistency as a proxy for confidence, an idea we formalize as the consistency hypothesis. We introduce three mathematical statements with corresponding statistical tests to capture variations of this hypothesis and metrics to evaluate LLM output conformity across tasks. Our empirical investigation, spanning 8 benchmark datasets and 3 tasks (question answering, text summarization, and text-to-SQL), highlights the prevalence of the hypothesis under different settings. Among the statements, we highlight the `Sim-Any' hypothesis as the most actionable, and demonstrate how it can be leveraged by proposing data-free black-box UQ methods that aggregate similarities between generations for confidence estimation. These approaches can outperform the closest baselines, showcasing the practical value of the empirically observed consistency hypothesis.",估计大型语言模型（LLM）输出的置信度对于需要高用户信任的实际应用至关重要。基于模型API访问的黑盒不确定性量化（UQ）方法因其实用性而受到欢迎。在本文中，我们研究了几种UQ方法背后的隐含假设，这些方法使用生成一致性作为置信度的代理，我们将这一想法正式化为一致性假设。我们引入了三个数学陈述及其相应的统计测试，以捕捉这一假设的变体，并提出了跨任务评估LLM输出一致性的指标。我们的实证研究涵盖了8个基准数据集和3个任务（问答、文本摘要和文本到SQL），突显了在不同设置下假设的普遍性。在陈述中，我们强调了`Sim-Any`假设是最具操作性的，并通过提出聚合生成之间相似性的无数据黑盒UQ方法来展示其如何被利用，以进行置信度估计。这些方法可以超越最接近的基线，展示了经验观察到的一致性假设的实际价值。,"The paper introduces the consistency hypothesis for estimating the confidence of large language model outputs, demonstrating its practical value through empirical investigations.",LLM,Helpful,"Uncertainty Quantification, Consistency Hypothesis, Large Language Models, Confidence Estimation, Black-box Methods"
Remote Sensing Large Vision-Language Model: Semantic-augmented Multi-level Alignment and Semantic-aware Expert Modeling,"Sungjune Park, Yeongyun Kim, Se Yeon Kim, Yong Man Ro",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2506.21863.pdf,"Large Vision and Language Models (LVLMs) have shown strong performance across various vision-language tasks in natural image domains. However, their application to remote sensing (RS) remains underexplored due to significant domain differences in visual appearances, object scales, and semantics. These discrepancies hider the effective understanding of RS scenes, which contain rich, multi-level semantic information spanning from coarse-to-fine levels. Hence, it limits the direct adaptation of existing LVLMs to RS imagery. To address this gap, we propose a novel LVLM framework tailored for RS understanding, incorporating two core components: Semantic-augmented Multi-level Alignment and Semantic-aware Expert Modeling. First, to align multi-level visual features, we introduce the retrieval-based Semantic Augmentation Module which enriches the visual features with relevant semantics across fine-to-coarse levels (e.g., object- and scene-level information). It is designed to retrieve relevant semantic cues from a RS semantic knowledge database, followed by aggregation of semantic cues with user query and multi-level visual features, resulting in semantically enriched representation across multiple levels. Second, for Semantic-aware Expert Modeling, we design semantic experts, where each expert is responsible for processing semantic representation at different levels separately. This enables hierarchical semantic understanding from coarse to fine levels. Evaluations across multiple RS tasks-including scene classification and VQA, etc.-demonstrate that the proposed framework achieves consistent improvements across multiple semantic levels. This highlights its capability and effectiveness in bridging the gap between general LVLMs and unique demands of RS-specific vision-language understanding.",大型视觉语言模型（LVLMs）在自然图像领域的各种视觉语言任务中表现出色。然而，由于视觉外观、对象尺度和语义的显著域差异，它们在遥感（RS）中的应用仍然未被充分探索。这些差异阻碍了对包含从粗到细多级语义信息的RS场景的有效理解，从而限制了现有LVLMs直接适应RS影像。为了解决这一差距，我们提出了一种专门针对RS理解的新颖LVLM框架，其中包含两个核心组件：语义增强的多级对齐和语义感知专家建模。首先，为了对齐多级视觉特征，我们引入基于检索的语义增强模块，该模块通过从RS语义知识数据库中检索相关语义线索，并将语义线索与用户查询和多级视觉特征进行聚合，从而丰富视觉特征，跨越从细到粗的多个级别（例如，对象和场景级信息）。其次，对于语义感知专家建模，我们设计了语义专家，每个专家负责分别处理不同级别的语义表示。这使得从粗到细的层次语义理解成为可能。在多个RS任务（包括场景分类和VQA等）的评估中，所提出的框架在多个语义级别上实现了一致的改进。这突显了其在桥接通用LVLMs与RS特定视觉语言理解独特需求方面的能力和有效性。,"The paper introduces a framework for aligning large vision-language models with remote sensing tasks, enhancing semantic understanding through multi-level alignment and expert modeling.",LMM,Helpful,"Vision-Language Models, Semantic Alignment, Remote Sensing, Multi-level Alignment, Expert Modeling"
DeepTalk: Towards Seamless and Smart Speech Interaction with Adaptive Modality-Specific MoE,"Hang Shao, Heting Gao, Yunhang Shen, Jiawei Chen, Lijiang Li, Zuwei Long, Bo Tong, Ke Li, Xing Sun",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2506.21864.pdf,"Native multimodal large language models (MLLMs) restructure a single large language model (LLM) into a spoken language model (SLM) capable of both speech and text generation. Compared to modular and aligned MLLMs, native MLLMs preserve richer paralinguistic features such as emotion and prosody, and generate speech responses directly within the backbone LLM rather than using a separate speech decoder. This integration also results in lower response latency and smoother interaction. However, native MLLMs suffer from catastrophic forgetting and performance degradation because the available paired speech-text data is insufficient to support the pretraining of MLLMs compared to the vast amount of text data required to pretrain text LLMs. To address this issue, we propose DeepTalk, a framework for adaptive modality expert learning based on a Mixture of Experts (MoE) architecture. DeepTalk first adaptively distinguishes modality experts according to their modality load within the LLM. Each modality expert then undergoes specialized single-modality training, followed by joint multimodal collaborative training. As a result, DeepTalk incurs only a 5.5% performance drop compared to the original LLM, which is significantly lower than the average performance drop of over 20% typically seen in native MLLMs (such as GLM-4-Voice), and is on par with modular MLLMs. Meanwhile, the end-to-end dialogue latency remains within 0.5 seconds, ensuring a seamless and intelligent speech interaction experience. Code and models are released at https://github.com/talkking/DeepTalk.",本地多模态大语言模型（MLLMs）将单个大语言模型（LLM）重构为能够进行语音和文本生成的语音语言模型（SLM）。与模块化和对齐的MLLMs相比，本地MLLMs保留了更丰富的语言特征，如情感和语调，并在主干LLM中直接生成语音响应，而不是使用单独的语音解码器。这种集成还导致响应延迟降低和交互更加流畅。然而，本地MLLMs由于可用的配对语音-文本数据不足，无法支持MLLMs的预训练，因此会出现灾难性遗忘和性能下降。为了解决这个问题，我们提出了DeepTalk，一个基于混合专家（MoE）架构的适应性模态专家学习框架。DeepTalk首先根据LLM中的模态负载自适应地区分模态专家。然后，每个模态专家经过专门的单模态训练，接着进行联合多模态协作训练。结果，DeepTalk的性能下降仅为5.5%，远低于本地MLLMs（如GLM-4-Voice）通常见到的20%以上的平均性能下降，与模块化MLLMs相当。同时，端到端对话延迟保持在0.5秒以内，确保了无缝和智能的语音交互体验。代码和模型发布在https://github.com/talkking/DeepTalk。,"The paper introduces DeepTalk, a framework for adaptive modality expert learning in multimodal large language models to improve speech interaction while maintaining low latency.",LLM,Helpful,"Multimodal, Speech Interaction, Adaptive Modality, Mixture of Experts, Latency"
Grounding-Aware Token Pruning: Recovering from Drastic Performance Drops in Visual Grounding Caused by Pruning,"Tzu-Chun Chien, Chieh-Kai Lin, Shiang-Feng Tsai, Ruei-Chi Lai, Hung-Jen Chen, Min Sun",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2506.21873.pdf,"Recent Multimodal Large Language Models (MLLMs) have demonstrated strong performance in visual grounding, establishing themselves as a general interface for various vision-language applications. This progress has driven the development of token pruning methods to mitigate the high computational costs associated with processing numerous visual tokens. However, we observe that pruning significantly weakens the model's grounding ability, leading to incorrect predictions and drastic performance degradation. In Referring Expression Comprehension (REC), for instance, pruning causes the accuracy of LLaVA on the RefCOCO validation set to drop from 56.14% to 15.34%. Our analysis identifies misaligned position IDs after pruning as the primary cause of this degradation, as both the order and value of these IDs are crucial for maintaining performance in grounding tasks. To address this issue, we propose Grounding-Aware Token Pruning (GAP), a simple yet effective adjustment to position IDs that recovers REC accuracy back to 51.42%, which is 90% of the original performance in the without pruning setting, all while requiring no additional training, memory, or computational overhead. Applied to models such as Shikra, MiniGPTv2, and the LLaVA series, our method consistently improves performance across various token pruning strategies.",最近的多模态大语言模型（MLLMs）在视觉定位方面表现出色，成为各种视觉语言应用的通用接口。这种进步推动了代币修剪方法的发展，以减轻处理大量视觉代币的高计算成本。然而，我们观察到修剪显著削弱了模型的定位能力，导致错误预测和显著性能下降。例如，在指称表达理解（REC）中，修剪导致LLaVA在RefCOCO验证集上的准确率从56.14%下降到15.34%。我们的分析确定了修剪后位置ID不匹配为这种退化的主要原因，因为这些ID的顺序和值对维持定位任务的性能至关重要。为了解决这个问题，我们提出了Grounding-Aware Token Pruning（GAP），这是一种简单而有效的位置ID调整，将REC准确率恢复到51.42%，这相当于未修剪设置下的原始性能的90%，同时不需要额外的训练、内存或计算开销。应用于Shikra、MiniGPTv2和LLaVA系列等模型，我们的方法在各种代币修剪策略下始终改善性能。,The paper introduces a method to mitigate performance drops in visual grounding tasks for Multimodal Large Language Models (MLLMs) caused by token pruning.,LLM,Helpful,"Visual Grounding, Token Pruning, Performance Degradation, Position IDs, Multimodal LLM"
A Dual-Layered Evaluation of Geopolitical and Cultural Bias in LLMs,"Sean Kim, Hyuhng Joon Kim",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2506.21881.pdf,"As large language models (LLMs) are increasingly deployed across diverse linguistic and cultural contexts, understanding their behavior in both factual and disputable scenarios is essential, especially when their outputs may shape public opinion or reinforce dominant narratives. In this paper, we define two types of bias in LLMs: model bias (bias stemming from model training) and inference bias (bias induced by the language of the query), through a two-phase evaluation. Phase 1 evaluates LLMs on factual questions where a single verifiable answer exists, assessing whether models maintain consistency across different query languages. Phase 2 expands the scope by probing geopolitically sensitive disputes, where responses may reflect culturally embedded or ideologically aligned perspectives. We construct a manually curated dataset spanning both factual and disputable QA, across four languages and question types. The results show that Phase 1 exhibits query language induced alignment, while Phase 2 reflects an interplay between the model's training context and query language. This paper offers a structured framework for evaluating LLM behavior across neutral and sensitive topics, providing insights for future LLM deployment and culturally aware evaluation practices in multilingual contexts.",随着大型语言模型（LLMs）在不同语言和文化背景下的广泛应用，理解其在事实和可争议情境中的行为至关重要，特别是当其输出可能塑造公众舆论或强化主导叙事时。本文通过两阶段评估，定义了LLMs中的两种偏见：模型偏见（源于模型训练的偏见）和推理偏见（由查询语言引入的偏见）。第一阶段评估LLMs在存在单一可验证答案的事实问题上，评估模型在不同查询语言下是否保持一致性。第二阶段扩大了范围，探讨地缘政治敏感争议，其中的响应可能反映文化嵌入或意识形态对齐的视角。我们构建了一个手工编制的数据集，涵盖了四种语言和问题类型的事实和可争议的问答。结果表明，第一阶段展示了查询语言诱导的对齐，而第二阶段反映了模型的训练背景和查询语言之间的相互作用。本文为评估LLMs在中立和敏感主题上的行为提供了一个结构化框架，为未来的LLM部署和多语言背景下的文化敏感评估实践提供了见解。,"The paper presents a dual-layered evaluation framework to assess geopolitical and cultural biases in LLMs, ensuring they are harmless by not reinforcing dominant narratives.",LLM,Harmless,"Bias, Evaluation, Geopolitical, Cultural, Multilingual"
PapersPlease: A Benchmark for Evaluating Motivational Values of Large Language Models Based on ERG Theory,"Junho Myung, Yeon Su Park, Sunwoo Kim, Shin Yoo, Alice Oh",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2506.21961.pdf,"Evaluating the performance and biases of large language models (LLMs) through role-playing scenarios is becoming increasingly common, as LLMs often exhibit biased behaviors in these contexts. Building on this line of research, we introduce PapersPlease, a benchmark consisting of 3,700 moral dilemmas designed to investigate LLMs' decision-making in prioritizing various levels of human needs. In our setup, LLMs act as immigration inspectors deciding whether to approve or deny entry based on the short narratives of people. These narratives are constructed using the Existence, Relatedness, and Growth (ERG) theory, which categorizes human needs into three hierarchical levels. Our analysis of six LLMs reveals statistically significant patterns in decision-making, suggesting that LLMs encode implicit preferences. Additionally, our evaluation of the impact of incorporating social identities into the narratives shows varying responsiveness based on both motivational needs and identity cues, with some models exhibiting higher denial rates for marginalized identities. All data is publicly available at https://github.com/yeonsuuuu28/papers-please.","通过角色扮演情境评估大型语言模型（LLM）的性能和偏见正变得越来越常见，因为LLM在这些情境中往往表现出偏见行为。在该研究的基础上，我们引入了PapersPlease，一个由3,700个道德困境组成的基准，旨在研究LLM在优先考虑各种人类需求方面的决策能力。在我们的设置中，LLM扮演移民检查员，根据人们的简短叙述决定是否批准或拒绝入境。这些叙述是使用存在、相关性和增长（ERG）理论构建的，该理论将人类需求分为三个层次。我们对六个LLM的分析揭示了决策中的显著统计模式，表明LLM编码了隐含偏好。此外，我们对将社会身份纳入叙述的影响的评估显示，根据动机需求和身份线索，响应性各不相同，某些模型对边缘化身份的拒绝率较高。所有数据均可在https://github.com/yeonsuuuu28/papers-please上公开获取。","The paper introduces PapersPlease, a benchmark for evaluating LLMs' decision-making and biases in moral dilemmas based on ERG theory.",LLM,"Helpful, Harmless","LLM alignment, moral dilemmas, decision-making, biases, ERG theory"
More Vulnerable than You Think: On the Stability of Tool-Integrated LLM Agents,"Weimin Xiong, Ke Wang, Yifan Song, Hanchao Liu, Sai Zhou, Wei Peng, Sujian Li",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2506.21967.pdf,"Current evaluations of tool-integrated LLM agents typically focus on end-to-end tool-usage evaluation while neglecting their stability. This limits their real-world applicability, as various internal or external factors can cause agents to crash or behave abnormally. Our research addresses this by investigating whether agents are vulnerable to errors throughout the entire tool invocation process, including reading tool documentation, selecting tools and generating parameters, and processing the tool's response. Through extensive experiments, we observe that agents are highly susceptible to errors at each stage and agents based on open-source models are more vulnerable than those based on proprietary models. We also find that increasing the model size does not significantly improve tool invocation reasoning and may make agents more vulnerable to attacks resembling normal user instructions. This highlights the importance of evaluating agent stability and offers valuable insights for future LLM development and evaluation.",当前对工具集成的大语言模型（LLM）代理的评估通常集中在端到端的工具使用评估，而忽略了它们的稳定性。这限制了它们在现实世界中的应用，因为各种内部或外部因素可能导致代理崩溃或表现异常。我们的研究通过调查代理在整个工具调用过程中的错误易受性，包括阅读工具文档、选择工具和生成参数以及处理工具的响应，来解决这个问题。通过广泛的实验，我们观察到代理在每个阶段都高度容易出错，并且基于开源模型的代理比基于专有模型的代理更容易受到攻击。我们还发现，增加模型大小并不会显著改善工具调用推理，可能会使代理更容易受到类似正常用户指令的攻击。这突显了评估代理稳定性的重要性，并为未来的LLM开发和评估提供了宝贵的见解。,"The paper investigates the stability and vulnerability of tool-integrated LLM agents, finding that they are susceptible to errors throughout the tool invocation process.",LLM,"Helpful, Harmless","LLM agents, stability, vulnerability, tool integration, evaluation"
Advancing Jailbreak Strategies: A Hybrid Approach to Exploiting LLM Vulnerabilities and Bypassing Modern Defenses,"Mohamed Ahmed, Mohamed Abdelmouty, Mingyu Kim, Gunvanth Kandula, Alex Park, James C. Davis",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2506.21972.pdf,"The advancement of Pre-Trained Language Models (PTLMs) and Large Language Models (LLMs) has led to their widespread adoption across diverse applications. Despite their success, these models remain vulnerable to attacks that exploit their inherent weaknesses to bypass safety measures. Two primary inference-phase threats are token-level and prompt-level jailbreaks. Token-level attacks embed adversarial sequences that transfer well to black-box models like GPT but leave detectable patterns and rely on gradient-based token optimization, whereas prompt-level attacks use semantically structured inputs to elicit harmful responses yet depend on iterative feedback that can be unreliable. To address the complementary limitations of these methods, we propose two hybrid approaches that integrate token- and prompt-level techniques to enhance jailbreak effectiveness across diverse PTLMs. GCG + PAIR and the newly explored GCG + WordGame hybrids were evaluated across multiple Vicuna and Llama models. GCG + PAIR consistently raised attack-success rates over its constituent techniques on undefended models; for instance, on Llama-3, its Attack Success Rate (ASR) reached 91.6%, a substantial increase from PAIR's 58.4% baseline. Meanwhile, GCG + WordGame matched the raw performance of WordGame maintaining a high ASR of over 80% even under stricter evaluators like Mistral-Sorry-Bench. Crucially, both hybrids retained transferability and reliably pierced advanced defenses such as Gradient Cuff and JBShield, which fully blocked single-mode attacks. These findings expose previously unreported vulnerabilities in current safety stacks, highlight trade-offs between raw success and defensive robustness, and underscore the need for holistic safeguards against adaptive adversaries.",预训练语言模型（PTLMs）和大型语言模型（LLMs）的进步导致了它们在各种应用中的广泛采用。尽管它们取得了成功，但这些模型仍然容易受到利用其固有弱点绕过安全措施的攻击。两种主要的推理阶段威胁是基于令牌级别和提示级别的越狱。基于令牌级别的攻击嵌入了能够很好地传输到黑盒模型（如GPT）的对抗序列，但会留下可检测的模式，并且依赖于基于梯度的令牌优化，而基于提示级别的攻击使用语义结构化的输入以引发有害响应，但依赖于不可靠的迭代反馈。为了解决这些方法的互补局限性，我们提出了两种混合方法，将基于令牌和提示级别的技术集成在一起，以增强越狱效果，适用于各种PTLMs。GCG + PAIR和新探索的GCG + WordGame混合方法在多个Vicuna和Llama模型上进行了评估。GCG + PAIR在未受保护的模型上始终提高了攻击成功率，高于其组成技术；例如，在Llama-3上，其攻击成功率（ASR）达到91.6%，显著高于PAIR的58.4%基线。与此同时，GCG + WordGame与WordGame的原始性能相匹配，在严格的评估器（如Mistral-Sorry-Bench）下保持了高达80%以上的ASR。关键是，这两种混合方法都保留了可转移性，并且可靠地穿透了高级防御措施，如梯度手套和JBShield，这些防御措施完全阻止了单模式攻击。这些发现揭示了当前安全堆栈中此前未报告的漏洞，突显了原始成功与防御强度之间的权衡，并强调了针对适应性对手的全面保护措施的必要性。,The paper introduces hybrid jailbreak strategies that combine token-level and prompt-level techniques to exploit vulnerabilities in Large Language Models (LLMs) and bypass modern defenses.,LLM,Harmless,"LLM, jailbreak, vulnerabilities, safety measures, defenses"
Don't Trust Generative Agents to Mimic Communication on Social Networks Unless You Benchmarked their Empirical Realism,"Simon M\""unker, Nils Schwager, Achim Rettinger",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2506.21974.pdf,"The ability of Large Language Models (LLMs) to mimic human behavior triggered a plethora of computational social science research, assuming that empirical studies of humans can be conducted with AI agents instead. Since there have been conflicting research findings on whether and when this hypothesis holds, there is a need to better understand the differences in their experimental designs. We focus on replicating the behavior of social network users with the use of LLMs for the analysis of communication on social networks. First, we provide a formal framework for the simulation of social networks, before focusing on the sub-task of imitating user communication. We empirically test different approaches to imitate user behavior on X in English and German. Our findings suggest that social simulations should be validated by their empirical realism measured in the setting in which the simulation components were fitted. With this paper, we argue for more rigor when applying generative-agent-based modeling for social simulation.",大语言模型（LLM）模仿人类行为的能力引发了大量计算社会科学研究，假设可以用人工智能代理替代人类进行实证研究。由于关于这一假设是否成立的研究发现存在矛盾，因此有必要更好地理解其实验设计的差异。我们专注于使用LLM分析社交网络上的通信，以模仿社交网络用户的行为。首先，我们为社交网络的模拟提供了一个正式框架，然后专注于模仿用户通信的子任务。我们在英语和德语中对模仿用户行为的不同方法进行了实证测试。我们的研究结果表明，应根据模拟组件的拟合设置来测量其实证现实性，以验证社交模拟。通过这篇论文，我们主张在应用基于生成代理的建模进行社交模拟时要更加严谨。,The paper emphasizes the importance of empirical realism when using LLMs to mimic human communication on social networks.,LLM,"Helpful, Harmless","LLM, social networks, empirical realism, generative agents, communication"
Training Language Model to Critique for Better Refinement,"Tianshu Yu, Chao Xiang, Mingchuan Yang, Pei Ke, Bosi Wen, Cunxiang Wang, Jiale Cheng, Li Zhang, Xinyu Mu, Chuxiong Sun, Minlie Huang",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2506.22157.pdf,"Large language models (LLMs) have demonstrated remarkable evaluation and critique capabilities, providing insightful feedback and identifying flaws in various tasks. However, limited research has explored which types of critiques are most effective for improving model responses or how to generate such critiques. To address this gap, we introduce \textbf{R}efinement-oriented \textbf{C}ritique \textbf{O}ptimization (RCO), a novel framework designed to train critic models using refinement signals. RCO uses a feedback loop where critiques, generated by the critic model, guide the actor model in refining its responses. The critique utility (CU) quantifies the effectiveness of these refinements, serving as the reward signal for training the critic model. By focusing on critiques that lead to better refinements, RCO eliminates the need for direct critique preference assessment, ensuring that critiques driving meaningful improvements are rewarded. We evaluate RCO across five tasks, i.e., dialog generation, summarization, question answering, mathematical reasoning, and code generation, and show that it significantly outperforms traditional methods and open-source models in terms of critique quality and refinement outcomes. Our contributions include the introduction of RCO, a novel supervision scheme based on refined response preferences, and comprehensive experimental results that highlight the method's effectiveness in enhancing LLM critique-refinement loops.",大语言模型（LLMs）在评估和批评方面表现出色，能够提供有见地的反馈并识别各种任务中的缺陷。然而，有限的研究探讨了哪些类型的批评对改进模型响应最有效，以及如何生成这些批评。为了填补这一空白，我们引入了一个名为RCO（RCO）的新框架，旨在使用改进信号训练评论模型。RCO使用一个反馈循环，其中由评论模型生成的评论指导演员模型改进其响应。评论效用（CU）量化这些改进的有效性，作为训练评论模型的奖励信号。通过专注于导致更好改进的评论，RCO消除了对直接评论偏好评估的需求，确保了驱动有意义改进的评论得到奖励。我们在五个任务中评估了RCO，即对话生成、摘要、问答、数学推理和代码生成，并表明它在评论质量和改进结果方面显著优于传统方法和开源模型。我们的贡献包括引入RCO、基于改进响应偏好的新监督方案，以及全面的实验结果，突出了该方法在增强LLM评论-改进循环方面的有效性。,"The paper introduces RCO, a framework for training large language models to generate critiques that improve model responses.",LLM,Helpful,"Critique, Refinement, Large Language Models, Feedback Loop, RCO"
EFRame: Deeper Reasoning via Exploration-Filtering-Replay Reinforcement Learning Framework,"Chen Wang, Lai Wei, Yanzhi Zhang, Chenyang Shao, Zedong Dan, Weiran Huang, Yue Wang, Yuzhi Zhang",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2506.22200.pdf,"Recent advances in reinforcement learning (RL) have significantly enhanced the reasoning capabilities of large language models (LLMs). Group Relative Policy Optimization (GRPO), an efficient variant of PPO that lowers RL's computational cost, still faces limited exploration, low sample efficiency and instability, constraining its performance on complex reasoning tasks. To address these limitations, we introduce EFRame, an Exploration-Filtering-Replay framework that systematically augments GRPO along three critical dimensions. EFRame performs additional rollouts to explore high-quality trajectories, applies online filtering to eliminate low-quality samples that introduce noise and variance, and leverages experience replay to repeatedly exploit rare but informative samples. EFRame establishes a complete and stable learning cycle, guiding the model through a structured transition from exploration to convergence. Our experiments across a variety of reasoning benchmarks demonstrate that EFRame not only improves the robustness and efficiency of training, but also enables access to deeper reasoning capabilities that remain unattainable under vanilla GRPO. Furthermore, EFRame enables a more fine-grained categorization of training samples, allowing for a deeper analysis of how different types of samples contribute to the learning process in RL. Our code is available at https://github.com/597358816/EFRame.",最近，强化学习（RL）的进展显著增强了大型语言模型（LLMs）的推理能力。作为PPO的高效变体，群组相对策略优化（GRPO）降低了RL的计算成本，但仍面临有限的探索、低样本效率和不稳定性，限制了其在复杂推理任务上的性能。为了解决这些局限性，我们引入了EFRame，一个探索-过滤-回放框架，系统地沿着三个关键维度增强GRPO。EFRame执行额外的回放以探索高质量的轨迹，应用在线过滤以消除引入噪声和方差的低质量样本，并利用经验回放来反复利用稀有但有信息量的样本。EFRame建立了一个完整且稳定的学习周期，指导模型通过结构化的从探索到收敛的过渡。我们在各种推理基准测试中的实验表明，EFRame不仅提高了训练的鲁棒性和效率，还使得在经典GRPO下无法实现的更深层次的推理能力成为可能。此外，EFRame使得对训练样本的更细粒度分类成为可能，从而可以对不同类型的样本如何贡献于RL中的学习过程进行更深入的分析。我们的代码可在https://github.com/597358816/EFRame获得。,"The paper introduces EFRame, a framework that enhances the reasoning capabilities of large language models through exploration-filtering-replay reinforcement learning.",LLM,None,"Reinforcement Learning, Large Language Models, Reasoning, Exploration-Filtering-Replay, GRPO"
Leveraging In-Context Learning for Political Bias Testing of LLMs,"Patrick Haller, Jannis Vamvas, Rico Sennrich, Lena A. J\""ager",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2506.22232.pdf,"A growing body of work has been querying LLMs with political questions to evaluate their potential biases. However, this probing method has limited stability, making comparisons between models unreliable. In this paper, we argue that LLMs need more context. We propose a new probing task, Questionnaire Modeling (QM), that uses human survey data as in-context examples. We show that QM improves the stability of question-based bias evaluation, and demonstrate that it may be used to compare instruction-tuned models to their base versions. Experiments with LLMs of various sizes indicate that instruction tuning can indeed change the direction of bias. Furthermore, we observe a trend that larger models are able to leverage in-context examples more effectively, and generally exhibit smaller bias scores in QM. Data and code are publicly available.",近年来，越来越多的研究通过向大型语言模型（LLMs）提出政治问题来评估其潜在偏见。然而，这种探测方法稳定性有限，使得模型之间的比较不可靠。在本文中，我们认为LLMs需要更多的上下文。我们提出了一种新的探测任务，问卷建模（QM），利用人类调查数据作为上下文示例。我们展示了QM提高了基于问题的偏见评估的稳定性，并证明它可以用来比较指令调整模型与其基础版本。实验表明，指令调整确实可以改变偏见的方向。此外，我们观察到较大的模型能够更有效地利用上下文示例，并且在QM中通常表现出较小的偏见分数。数据和代码均公开可用。,The paper introduces a new method for evaluating political biases in LLMs using in-context learning and human survey data.,LLM,Harmless,"Bias, In-Context Learning, Political Bias, LLM Evaluation, Instruction Tuning"
"Public Service Algorithm: towards a transparent, explainable, and scalable content curation for news content based on editorial values","Ahmad Mel, Sebastien Noir",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2506.22270.pdf,"The proliferation of disinformation challenges traditional, unscalable editorial processes and existing automated systems that prioritize engagement over public service values. To address this, we introduce the Public Service Algorithm (PSA), a novel framework using Large Language Models (LLMs) for scalable, transparent content curation based on Public Service Media (PSM) inspired values. Utilizing a large multilingual news dataset from the 'A European Perspective' project, our experiment directly compared article ratings from a panel of experienced editors from various European PSMs, with those from several LLMs, focusing on four criteria: diversity, in-depth analysis, forward-looking, and cross-border relevance. Utilizing criterion-specific prompts, our results indicate a promising alignment between human editorial judgment and LLM assessments, demonstrating the potential of LLMs to automate value-driven curation at scale without sacrificing transparency. This research constitutes a first step towards a scalable framework for the automatic curation of trustworthy news content.",信息泛滥挑战了传统的不可扩展的编辑流程和现有的自动化系统，这些系统优先考虑参与度而非公共服务价值。为了解决这个问题，我们引入了公共服务算法（PSA），这是一个使用大型语言模型（LLM）进行可扩展、透明内容筛选的新框架，基于公共服务媒体（PSM）启发的价值。利用来自“欧洲视角”项目的大型多语言新闻数据集，我们的实验直接比较了来自各种欧洲PSM的经验丰富的编辑小组的文章评级，与来自几个LLM的评级，重点关注四个标准：多样性、深入分析、前瞻性和跨国相关性。利用特定标准的提示，我们的结果表明，人类编辑判断与LLM评估之间存在有希望的对齐，表明LLM有潜力在不牺牲透明度的情况下自动化价值驱动的筛选。这项研究构成了自动筛选可信新闻内容的可扩展框架的第一步。,"The paper explores the use of LLMs for transparent and scalable content curation aligned with editorial values, showing promising results in mimicking human editorial judgment.",LLM,"Helpful, Harmless","LLM, content curation, editorial values, transparency, alignment"
Evaluating Scoring Bias in LLM-as-a-Judge,"Qingquan Li, Shaoyu Dou, Kailai Shao, Chao Chen, Haixiang Hu",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2506.22316.pdf,"The remarkable performance of Large Language Models (LLMs) gives rise to``LLM-as-a-Judge'', where LLMs are employed as evaluators for complex tasks. Moreover, it has been widely adopted across fields such as Natural Language Processing (NLP), preference learning, and various specific domains. However, there are various biases within LLM-as-a-Judge, which adversely affect the fairness and reliability of judgments. Current research on evaluating or mitigating bias in LLM-as-a-Judge predominantly focuses on comparison-based evaluations, while systematic investigations into bias in scoring-based evaluations remain limited. Therefore, we define scoring bias in LLM-as-a-Judge as the scores differ when scoring judge models are bias-related perturbed, and provide a well-designed framework to comprehensively evaluate scoring bias. We augment existing LLM-as-a-Judge benchmarks through data synthesis to construct our evaluation dataset and design multi-faceted evaluation metrics. Our experimental results demonstrate that the scoring stability of existing judge models is disrupted by scoring biases. Further exploratory experiments and discussions provide valuable insights into the design of scoring prompt templates and the mitigation of scoring biases on aspects such as score rubrics, score IDs, and reference answer selection.",大语言模型（LLM）的卓越表现促使出现了“LLM-as-a-Judge”，其中LLM被用作复杂任务的评估者。它在自然语言处理（NLP）、偏好学习以及各种特定领域中得到了广泛采用。然而，LLM-as-a-Judge中存在各种偏见，这会对判断的公平性和可靠性产生不利影响。目前，关于评估或缓解LLM-as-a-Judge中的偏见的研究主要集中在基于比较的评估上，而对基于评分的评估中的偏见的系统研究仍然有限。因此，我们将LLM-as-a-Judge中的评分偏见定义为在评分评判模型偏见相关扰动时评分不同，并提供一个设计精良的框架，以全面评估评分偏见。我们通过数据合成增强现有的LLM-as-a-Judge基准，以构建我们的评估数据集，并设计多方面的评估指标。我们的实验结果表明，现有的评判模型的评分稳定性受到评分偏见的干扰。进一步的探索实验和讨论为评分提示模板的设计以及评分偏见的缓解（如评分标准、评分ID和参考答案选择）提供了宝贵的见解。,The paper introduces a framework to evaluate and mitigate scoring biases in LLMs used as judges for various tasks.,LLM,Harmless,"LLM-as-a-Judge, scoring bias, fairness, reliability, bias mitigation"
Literature-Grounded Novelty Assessment of Scientific Ideas,"Simra Shahid, Marissa Radensky, Raymond Fok, Pao Siangliulue, Daniel S. Weld, Tom Hope",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2506.22026.pdf,"Automated scientific idea generation systems have made remarkable progress, yet the automatic evaluation of idea novelty remains a critical and underexplored challenge. Manual evaluation of novelty through literature review is labor-intensive, prone to error due to subjectivity, and impractical at scale. To address these issues, we propose the Idea Novelty Checker, an LLM-based retrieval-augmented generation (RAG) framework that leverages a two-stage retrieve-then-rerank approach. The Idea Novelty Checker first collects a broad set of relevant papers using keyword and snippet-based retrieval, then refines this collection through embedding-based filtering followed by facet-based LLM re-ranking. It incorporates expert-labeled examples to guide the system in comparing papers for novelty evaluation and in generating literature-grounded reasoning. Our extensive experiments demonstrate that our novelty checker achieves approximately 13% higher agreement than existing approaches. Ablation studies further showcases the importance of the facet-based re-ranker in identifying the most relevant literature for novelty evaluation.",自动化科学创意生成系统取得了显著进展，但自动评估创意新颖性仍然是一个关键且未被充分探索的挑战。手动通过文献评审评估新颖性既费时又容易出错，且在大规模上不切实际。为了解决这些问题，我们提出了基于LLM的检索增强生成（RAG）框架，称为Idea Novelty Checker，它利用两阶段检索-然后重新排序的方法。Idea Novelty Checker首先使用关键词和片段检索收集一组相关的论文，然后通过基于嵌入的过滤和基于面向的LLM重新排序来精炼这一收集。它还包括专家标记的示例，以指导系统在比较论文以进行新颖性评估以及生成基于文献的推理方面。我们的广泛实验表明，我们的新颖性检查器的协议约高出现有方法13%。消融研究进一步展示了基于面向的重新排序器在识别最相关文献以进行新颖性评估中的重要性。,The paper introduces an LLM-based framework for evaluating the novelty of scientific ideas through a retrieval-augmented generation approach.,LLM,"Helpful, Honest","Novelty assessment, LLM, retrieval-augmented generation, scientific ideas, literature review"
Towards Fair Rankings: Leveraging LLMs for Gender Bias Detection and Measurement,"Maryam Mousavian, Zahra Abbasiantaeb, Mohammad Aliannejadi, Fabio Crestani",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2506.22372.pdf,"The presence of social biases in Natural Language Processing (NLP) and Information Retrieval (IR) systems is an ongoing challenge, which underlines the importance of developing robust approaches to identifying and evaluating such biases. In this paper, we aim to address this issue by leveraging Large Language Models (LLMs) to detect and measure gender bias in passage ranking. Existing gender fairness metrics rely on lexical- and frequency-based measures, leading to various limitations, e.g., missing subtle gender disparities. Building on our LLM-based gender bias detection method, we introduce a novel gender fairness metric, named Class-wise Weighted Exposure (CWEx), aiming to address existing limitations. To measure the effectiveness of our proposed metric and study LLMs' effectiveness in detecting gender bias, we annotate a subset of the MS MARCO Passage Ranking collection and release our new gender bias collection, called MSMGenderBias, to foster future research in this area. Our extensive experimental results on various ranking models show that our proposed metric offers a more detailed evaluation of fairness compared to previous metrics, with improved alignment to human labels (58.77% for Grep-BiasIR, and 18.51% for MSMGenderBias, measured using Cohen's Kappa agreement), effectively distinguishing gender bias in ranking. By integrating LLM-driven bias detection, an improved fairness metric, and gender bias annotations for an established dataset, this work provides a more robust framework for analyzing and mitigating bias in IR systems.",自然语言处理（NLP）和信息检索（IR）系统中的社会偏见问题依然存在，这突显了开发识别和评估这些偏见的健壮方法的重要性。本文旨在通过利用大型语言模型（LLMs）来检测和测量段落排名中的性别偏见来解决这一问题。现有的性别公平性指标依赖于词汇和频率的衡量，导致各种限制，例如未能捕捉到微妙的性别差异。基于我们的基于LLM的性别偏见检测方法，我们引入了一种新的性别公平性指标，称为类别加权曝光（CWEx），旨在解决现有的限制。为了衡量我们提出的指标的有效性并研究LLMs在检测性别偏见中的有效性，我们对MS MARCO段落排名集的子集进行了注释，并发布了我们的新性别偏见集，称为MSMGenderBias，以促进未来在这个领域的研究。我们在各种排名模型上的广泛实验结果表明，我们提出的指标在与人类标签的对齐方面提供了更详细的公平性评估，显著提高了对性别偏见的区分（Grep-BiasIR的Cohen's Kappa协议为58.77%，MSMGenderBias为18.51%）。通过集成LLM驱动的偏见检测、改进的公平性指标和现有数据集的性别偏见注释，本文为分析和缓解IR系统中的偏见提供了一个更健壮的框架。,"The paper introduces a new metric for detecting gender bias in passage ranking using LLMs, improving fairness evaluation in IR systems.",LLM,Harmless,"Gender bias, fairness, LLM, bias detection, ranking"
Bias-Augmented Consistency Training Reduces Biased Reasoning in Chain-of-Thought,"James Chua, Edward Rees, Hunar Batra, Samuel R. Bowman, Julian Michael, Ethan Perez, Miles Turpin",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2403.05518.pdf,"Chain-of-thought prompting (CoT) has the potential to improve the explainability of language model reasoning. But CoT can also systematically misrepresent the factors influencing models' behavior -- for example, rationalizing answers in line with a user's opinion.
  We first create a new dataset of 9 different biases that affect GPT-3.5-Turbo and Llama-8b models. These consist of spurious-few-shot patterns, post hoc rationalization, and sycophantic settings. Models switch to the answer implied by the bias, without mentioning the effect of the bias in the CoT.
  To mitigate this biased reasoning problem, we introduce bias-augmented consistency training (BCT), an unsupervised fine-tuning scheme that trains models to give consistent reasoning across prompts with and without biasing features. We construct a suite testing nine forms of biased reasoning on seven question-answering tasks, and find that applying BCT to GPT-3.5-Turbo with one bias reduces the rate of biased reasoning by 86\% on held-out tasks. Moreover, this model generalizes to other forms of bias, reducing biased reasoning on held-out biases by an average of 37\%. As BCT generalizes to held-out biases and does not require gold labels, this method may hold promise for reducing biased reasoning from as-of-yet unknown biases and on tasks where ground truth reasoning is unavailable.",链式思维提示（CoT）有潜力提高语言模型推理的可解释性。但CoT也可能系统性地误表示影响模型行为的因素，例如根据用户的观点合理化答案。我们首先创建了一个包含9种不同偏见的新数据集，这些偏见影响GPT-3.5-Turbo和Llama-8b模型。这些偏见包括虚假的少量示例模式、事后合理化和阿谀奉承的设置。模型会切换到偏见暗示的答案，而不在CoT中提到偏见的影响。为了缓解这种偏见推理问题，我们引入了偏见增强的一致性训练（BCT），这是一种无监督的微调方案，训练模型在具有和不具有偏见特征的提示之间给出一致的推理。我们构建了一套测试九种偏见推理的套件，在七个问题-回答任务上，发现将BCT应用于GPT-3.5-Turbo的一个偏见，可以将保留任务上的偏见推理率降低86%。此外，该模型能够推广到其他形式的偏见，平均将保留偏见的偏见推理率降低37%。由于BCT能够推广到保留偏见，并且不需要金标签，因此这种方法可能有望减少从未知偏见和没有可用的真实推理的任务中减少偏见推理。,"The paper introduces bias-augmented consistency training to reduce biased reasoning in language models, showing significant improvements across various biases and tasks.",LLM,Harmless,"Bias, Chain-of-Thought, Consistency Training, Language Models, Reasoning"
Language in Vivo vs. in Silico: Size Matters but Larger Language Models Still Do Not Comprehend Language on a Par with Humans Due to Impenetrable Semantic Reference,"Vittoria Dentella, Fritz Guenther, Evelina Leivada",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2404.14883.pdf,"Understanding the limits of language is a prerequisite for Large Language Models (LLMs) to act as theories of natural language. LLM performance in some language tasks presents both quantitative and qualitative differences from that of humans, however it remains to be determined whether such differences are amenable to model size. This work investigates the critical role of model scaling, determining whether increases in size make up for such differences between humans and models. We test three LLMs from different families (Bard, 137 billion parameters; ChatGPT-3.5, 175 billion; ChatGPT-4, 1.5 trillion) on a grammaticality judgment task featuring anaphora, center embedding, comparatives, and negative polarity. N=1,200 judgments are collected and scored for accuracy, stability, and improvements in accuracy upon repeated presentation of a prompt. Results of the best performing LLM, ChatGPT-4, are compared to results of n=80 humans on the same stimuli. We find that humans are overall less accurate than ChatGPT-4 (76% vs. 80% accuracy, respectively), but that this is due to ChatGPT-4 outperforming humans only in one task condition, namely on grammatical sentences. Additionally, ChatGPT-4 wavers more than humans in its answers (12.5% vs. 9.6% likelihood of an oscillating answer, respectively). Thus, while increased model size may lead to better performance, LLMs are still not sensitive to (un)grammaticality the same way as humans are. It seems possible but unlikely that scaling alone can fix this issue. We interpret these results by comparing language learning in vivo and in silico, identifying three critical differences concerning (i) the type of evidence, (ii) the poverty of the stimulus, and (iii) the occurrence of semantic hallucinations due to impenetrable linguistic reference.","理解语言的局限性是大型语言模型（LLM）作为自然语言理论的前提。LLM在某些语言任务中的表现与人类在定量和定性上都存在差异，但尚不清楚这些差异是否能通过模型大小来弥补。本研究调查了模型扩展的关键作用，确定是否增加大小可以弥补人类和模型之间的差异。我们在一个包含指示词、中心嵌入、比较和负极性的语法判断任务中测试了三个不同家族的LLM（Bard，1370亿参数；ChatGPT-3.5，1750亿；ChatGPT-4，1.5万亿）。收集并评分了N=1,200个判断，以准确性、稳定性和重复呈现提示时准确性的改进为标准。最佳表现的LLM，ChatGPT-4的结果与n=80人在相同刺激下的结果进行了比较。我们发现，人类的整体准确性低于ChatGPT-4（分别为76%和80%），但这只是因为ChatGPT-4在一个任务条件下表现优于人类，即在语法正确的句子上。此外，ChatGPT-4在其答案中波动得更多（分别为12.5%和9.6%的波动答案的可能性）。因此，虽然增加模型大小可能会带来更好的表现，但LLM仍然不能像人类那样敏感地感知（不）语法。似乎可能但不太可能单靠扩展就能解决这个问题。我们通过比较体内和体外的语言学习，识别出三个关键差异，涉及（i）证据类型、（ii）刺激贫乏和（iii）由于不可穿透的语言参考而导致的语义幻觉。",The paper investigates how increasing the size of Large Language Models (LLMs) affects their performance in language tasks compared to humans.,LLM,None,"LLM, model scaling, language comprehension, human comparison, grammaticality judgment"
RLSF: Fine-tuning LLMs via Symbolic Feedback,"Piyush Jha, Prithwish Jana, Pranavkrishna Suresh, Arnav Arora, Vijay Ganesh",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2405.16661.pdf,"Large Language Models (LLMs) have transformed AI but often struggle with tasks that require domain-specific reasoning and logical alignment. Traditional fine-tuning methods do not leverage the vast amount of symbolic domain-knowledge available to us via symbolic reasoning tools (e.g., provers), and are further limited by sparse rewards and unreliable reward models.
  We introduce Reinforcement Learning via Symbolic Feedback (RLSF), a novel fine-tuning paradigm where symbolic reasoning tools (e.g., solvers, provers, and algebra systems) provide fine-grained feedback to LLMs. RLSF uses poly-sized certificates (e.g., proofs) generated by symbolic tools to identify and correct errors in model outputs, offering token-level guidance without requiring differentiable reasoning systems. This paradigm bridges the gap between symbolic reasoning and LLM fine-tuning, enabling precise alignment with domain-specific constraints while addressing key limitations of traditional reward signals.
  Via extensive evaluations, we show that our RLSF-based fine-tuning of LLMs outperforms traditional approaches on five different applications (that have some associated logical or domain constraints), namely, program synthesis from natural language pseudo-code to programming language, three chemistry tasks, and solving the Game of 24. A key takeaway is that fine-tuning via RLSF enables relatively smaller LLMs to significantly outperform closed-source models that are orders of magnitude larger.",大语言模型（LLMs）已经改变了人工智能，但往往在需要领域特定推理和逻辑对齐的任务上表现不佳。传统的微调方法没有利用我们通过符号推理工具（例如，证明器）获得的大量符号领域知识，并且进一步受到稀疏奖励和不可靠奖励模型的限制。我们引入了通过符号反馈的强化学习（RLSF），这是一种新颖的微调范式，其中符号推理工具（例如，求解器、证明器和代数系统）为LLMs提供精细的反馈。RLSF使用由符号工具生成的多项式大小证书（例如，证明）来识别和纠正模型输出中的错误，提供基于令牌级别的指导，而无需可微分的推理系统。这种范式填补了符号推理和LLM微调之间的差距，使其能够与特定领域的约束精确对齐，同时解决了传统奖励信号的关键限制。通过广泛的评估，我们表明，基于RLSF的LLM微调在五个不同的应用中优于传统方法（这些应用具有某些相关的逻辑或领域约束），即从自然语言伪代码到编程语言的程序合成、三个化学任务和解决24点游戏。一个关键的发现是，通过RLSF进行微调使得相对较小的LLMs能够显著优于大得多的闭源模型。,"The paper introduces RLSF, a method for fine-tuning LLMs using symbolic feedback to improve logical alignment and domain-specific reasoning.",LLM,Helpful,"Fine-tuning, Symbolic Feedback, Logical Alignment, Domain-specific Reasoning, Reinforcement Learning"
LRP4RAG: Detecting Hallucinations in Retrieval-Augmented Generation via Layer-wise Relevance Propagation,"Haichuan Hu, Congqing He, Xiaochen Xie, Quanjun Zhang",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2408.15533.pdf,"Retrieval-Augmented Generation (RAG) has become a primary technique for mitigating hallucinations in large language models (LLMs). However, incomplete knowledge extraction and insufficient understanding can still mislead LLMs to produce irrelevant or even contradictory responses, which means hallucinations persist in RAG. In this paper, we propose LRP4RAG, a method based on the Layer-wise Relevance Propagation (LRP) algorithm for detecting hallucinations in RAG. Specifically, we first utilize LRP to compute the relevance between the input and output of the RAG generator. We then apply further extraction and resampling to the relevance matrix. The processed relevance data are input into multiple classifiers to determine whether the output contains hallucinations. To the best of our knowledge, this is the first time that LRP has been used for detecting RAG hallucinations, and extensive experiments demonstrate that LRP4RAG outperforms existing baselines.",检测大型语言模型（LLM）中的幻觉是一个重要的研究方向。本文提出了一种基于层次相关性传播（LRP）算法的方法，用于检测检索增强生成（RAG）中的幻觉。具体来说，我们首先使用LRP计算RAG生成器的输入和输出之间的相关性，然后对相关性矩阵进行进一步的提取和重新采样。处理后的相关性数据输入到多个分类器中，以确定输出是否包含幻觉。实验结果表明，LRP4RAG在检测RAG幻觉方面优于现有的基线方法。,"The paper introduces LRP4RAG, a method using Layer-wise Relevance Propagation to detect hallucinations in Retrieval-Augmented Generation for large language models.",LLM,Harmless,"Hallucination detection, Retrieval-Augmented Generation, Layer-wise Relevance Propagation, Large Language Models, Misinformation"
Towards Reproducible LLM Evaluation: Quantifying Uncertainty in LLM Benchmark Scores,"Robert E. Blackwell, Jon Barry, Anthony G. Cohn",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2410.03492.pdf,"Large language models (LLMs) are stochastic, and not all models give deterministic answers, even when setting temperature to zero with a fixed random seed. However, few benchmark studies attempt to quantify uncertainty, partly due to the time and cost of repeated experiments. We use benchmarks designed for testing LLMs' capacity to reason about cardinal directions to explore the impact of experimental repeats on mean score and prediction interval. We suggest a simple method for cost-effectively quantifying the uncertainty of a benchmark score and make recommendations concerning reproducible LLM evaluation.",大语言模型（LLMs）是随机的，即使将温度设置为零并固定随机种子，也不是所有模型都会给出确定性答案。然而，很少有基准研究尝试量化不确定性，部分原因是重复实验的时间和成本。我们使用设计用于测试LLMs在基准测试中推理方向能力的基准，探索实验重复对平均分数和预测区间的影响。我们提出了一种简单的方法，以成本效益高的方式量化基准分数的不确定性，并就可重复的LLM评估提出建议。,The paper proposes a method to quantify the uncertainty in LLM benchmark scores to improve the reproducibility of LLM evaluations.,LLM,Helpful,"LLM evaluation, uncertainty quantification, benchmark scores, reproducibility"
Federated Data-Efficient Instruction Tuning for Large Language Models,"Zhen Qin, Zhaomin Wu, Bingsheng He, Shuiguang Deng",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2410.10926.pdf,"Instruction tuning is a crucial step in improving the responsiveness of pretrained large language models (LLMs) to human instructions. Federated learning (FL) helps to exploit the use of vast private instruction data from clients, becoming popular for LLM tuning by improving data diversity. Existing federated tuning simply consumes all local data, causing excessive computational overhead and overfitting to local data, while centralized data-efficient solutions are not suitable for FL due to privacy concerns. This work presents FedHDS, a federated data-efficient instruction tuning approach, which tunes LLMs with a representative subset of edge-side data. It reduces the data redundancy at both intra- and inter-client levels without sharing raw data. Experiments with various LLMs, datasets and partitions show that FedHDS improves Rouge-L on unseen tasks by an average of 10.72% over the SOTA full-data federated instruction tuning methods, while using less than 1.5% of the data samples, improving training efficiency by up to tens of times.",指令微调是提高预训练大型语言模型（LLM）对人类指令响应性的关键步骤。联邦学习（FL）有助于利用来自客户端的大量私人指令数据，成为LLM调优的热门方法，通过提高数据多样性。现有的联邦调优方法简单地消耗所有本地数据，导致计算开销过大和过拟合本地数据，而集中式数据高效解决方案由于隐私问题不适用于FL。本文提出了FedHDS，一种联邦数据高效指令微调方法，它使用边缘端数据的代表性子集对LLM进行微调。它在不共享原始数据的情况下，减少了客户端内部和客户端之间的数据冗余。实验表明，FedHDS在各种LLM、数据集和分区上，在未见任务上平均提高了Rouge-L 10.72%，而使用的数据样本少于1.5%，训练效率提高了数十倍。,"The paper introduces FedHDS, a federated data-efficient instruction tuning approach for LLMs that improves responsiveness to human instructions while enhancing training efficiency and privacy.",LLM,Helpful,"Federated learning, Instruction tuning, Data efficiency, Large language models, Privacy"
Introducing MAPO: Momentum-Aided Gradient Descent Prompt Optimization,"Anthony Cui, Pranav Nandyalam, Andrew Rufail, Ethan Cheung, Aiden Lei, Kevin Zhu, Sean O'Brien",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2410.19499.pdf,"Momentum-Aided Prompt Optimization (MAPO) enhances the efficiency and efficacy of prompt optimization for Large Language Models (LLMs). Building on ProTeGi, MAPO uses positive natural language ""gradients"" and a momentum-based extension to refine prompts effectively. By tracking gradient history, MAPO avoids local minima and oscillations. It also utilizes beam search and an Upper Confidence Bound (UCB) algorithm for balanced candidate expansion and selection. Benchmark testing shows that MAPO achieves faster convergence time with fewer API calls and higher F1 scores than ProTeGi, proving it as a robust and scalable solution for automated prompt engineering in LLMs.",Momentum-Aided Prompt Optimization (MAPO) 提高了大型语言模型 (LLMs) 提示优化的效率和效果。基于 ProTeGi，MAPO 使用正自然语言“梯度”和基于动量的扩展来有效地优化提示。通过跟踪梯度历史，MAPO 避免了局部最小值和振荡。它还利用束搜索和上置信区间 (UCB) 算法进行平衡的候选人扩展和选择。基准测试表明，MAPO 在更少的 API 调用和更高的 F1 分数的情况下实现了更快的收敛时间，证明其是自动提示工程的强大和可扩展的解决方案。,"The paper introduces MAPO, a method that enhances prompt optimization for LLMs by using momentum and gradient descent, leading to faster convergence and better performance.",LLM,"Helpful, Honest","Prompt Optimization, Momentum, Gradient Descent, LLM, MAPO"
Jailbreaking Multimodal Large Language Models via Shuffle Inconsistency,"Shiji Zhao, Ranjie Duan, Fengxiang Wang, Chi Chen, Caixin Kang, Shouwei Ruan, Jialing Tao, YueFeng Chen, Hui Xue, Xingxing Wei",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2501.04931.pdf,"Multimodal Large Language Models (MLLMs) have achieved impressive performance and have been put into practical use in commercial applications, but they still have potential safety mechanism vulnerabilities. Jailbreak attacks are red teaming methods that aim to bypass safety mechanisms and discover MLLMs' potential risks. Existing MLLMs' jailbreak methods often bypass the model's safety mechanism through complex optimization methods or carefully designed image and text prompts. Despite achieving some progress, they have a low attack success rate on commercial closed-source MLLMs. Unlike previous research, we empirically find that there exists a Shuffle Inconsistency between MLLMs' comprehension ability and safety ability for the shuffled harmful instruction. That is, from the perspective of comprehension ability, MLLMs can understand the shuffled harmful text-image instructions well. However, they can be easily bypassed by the shuffled harmful instructions from the perspective of safety ability, leading to harmful responses. Then we innovatively propose a text-image jailbreak attack named SI-Attack. Specifically, to fully utilize the Shuffle Inconsistency and overcome the shuffle randomness, we apply a query-based black-box optimization method to select the most harmful shuffled inputs based on the feedback of the toxic judge model. A series of experiments show that SI-Attack can improve the attack's performance on three benchmarks. In particular, SI-Attack can obviously improve the attack success rate for commercial MLLMs such as GPT-4o or Claude-3.5-Sonnet.",多模态大语言模型（MLLMs）在商业应用中取得了显著的性能，但仍存在潜在的安全机制漏洞。越狱攻击是一种旨在绕过安全机制并发现MLLMs潜在风险的红队方法。现有的MLLMs越狱方法通常通过复杂的优化方法或精心设计的图像和文本提示来绕过模型的安全机制。尽管取得了一些进展，但在商业封闭源MLLMs上攻击成功率较低。与以前的研究不同，我们经验地发现，存在一种Shuffle Inconsistency，即MLLMs在理解能力和安全能力之间的不一致性，对于混洗的有害指令。具体来说，从理解能力的角度来看，MLLMs可以很好地理解混洗的有害文本-图像指令。然而，从安全能力的角度来看，它们可以被混洗的有害指令轻松绕过，导致有害的响应。然后，我们创新地提出了一种名为SI-Attack的文本-图像越狱攻击。具体来说，为了充分利用Shuffle Inconsistency并克服混洗的随机性，我们应用了一种基于查询的黑盒优化方法，根据毒性判断模型的反馈选择最有害的混洗输入。一系列实验表明，SI-Attack可以在三个基准测试中提高攻击性能。特别是，SI-Attack可以显著提高商业MLLMs（如GPT-4o或Claude-3.5-Sonnet）的攻击成功率。,The paper introduces a novel jailbreak attack method called SI-Attack that exploits shuffle inconsistency in Multimodal Large Language Models to bypass safety mechanisms and generate harmful responses.,LMM,Harmless,"Jailbreak, Safety Mechanisms, Multimodal, Harmful Responses, Optimization"
Leveraging Online Olympiad-Level Math Problems for LLMs Training and Contamination-Resistant Evaluation,"Sadegh Mahdavi, Muchen Li, Kaiwen Liu, Christos Thrampoulidis, Leonid Sigal, Renjie Liao",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2501.14275.pdf,"Advances in Large Language Models (LLMs) have sparked interest in their ability to solve Olympiad-level math problems. However, the training and evaluation of these models are constrained by the limited size and quality of available datasets, as creating large-scale data for such advanced problems requires extensive effort from human experts. In addition, current benchmarks are prone to contamination, leading to unreliable evaluations. In this paper, we present an automated pipeline that leverages the rich resources of the Art of Problem Solving (AoPS) forum, which predominantly features Olympiad-level problems and community-driven solutions. Using open-source LLMs, we develop a method to extract question-answer pairs from the forum, resulting in AoPS-Instruct, a dataset of more than 600,000 high-quality QA pairs. Our experiments demonstrate that fine-tuning LLMs on AoPS-Instruct improves their reasoning abilities across various benchmarks. Moreover, we build an automatic pipeline that introduces LiveAoPSBench, an evolving evaluation set with timestamps, derived from the latest forum data, providing a contamination-resistant benchmark for assessing LLM performance. Notably, we observe a significant decline in LLM performance over time, suggesting their success on older examples may stem from pre-training exposure rather than true reasoning ability. Our work presents a scalable approach to creating and maintaining large-scale, high-quality datasets for advanced math reasoning, offering valuable insights into the capabilities and limitations of LLMs in this domain. Our benchmark and code is available at https://github.com/DSL-Lab/aops","大语言模型（LLMs）在解决奥林匹克级数学问题方面的进步引发了对其能力的兴趣。然而，这些模型的训练和评估受到可用数据集规模和质量有限的约束，因为为这些高级问题创建大规模数据需要人类专家付出大量努力。此外，当前的基准容易受到污染，导致评估不可靠。在本文中，我们提出了一种自动化流水线，利用Art of Problem Solving（AoPS）论坛的丰富资源，该论坛主要涉及奥林匹克级问题和社区驱动的解决方案。使用开源LLMs，我们开发了一种从论坛中提取问题-答案对的方法，结果是AoPS-Instruct，一个包含600,000多个高质量QA对的数据集。我们的实验表明，在AoPS-Instruct上微调LLMs可以提高其在各种基准上的推理能力。此外，我们构建了一个自动化流水线，引入了LiveAoPSBench，一个带有时间戳的演变评估集，源自最新的论坛数据，为评估LLM性能提供了抗污染基准。值得注意的是，我们观察到LLM性能随着时间的推移显著下降，这表明它们在较旧示例上的成功可能源于预训练暴露，而不是真正的推理能力。我们的工作提出了一种可扩展的方法来创建和维护用于高级数学推理的大规模、高质量数据集，为LLMs在这一领域的能力和局限性提供了宝贵的见解。我们的基准和代码可在https://github.com/DSL-Lab/aops上获得。","The paper introduces a method to create and maintain large-scale, high-quality datasets for advanced math reasoning in LLMs, improving their helpfulness and providing contamination-resistant evaluation.",LLM,Helpful,"LLM, Math Problems, Dataset, Evaluation, Reasoning"
"Advances in Temporal Point Processes: Bayesian, Neural, and LLM Approaches","Feng Zhou, Quyu Kong, Jie Qiao, Cheng Wan, Yixuan Zhang, Ruichu Cai",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2501.14291.pdf,"Temporal point processes (TPPs) are stochastic process models used to characterize event sequences occurring in continuous time. Traditional statistical TPPs have a long-standing history, with numerous models proposed and successfully applied across diverse domains. In recent years, advances in deep learning have spurred the development of neural TPPs, enabling greater flexibility and expressiveness in capturing complex temporal dynamics. The emergence of large language models (LLMs) has further sparked excitement, offering new possibilities for modeling and analyzing event sequences by leveraging their rich contextual understanding. This survey presents a comprehensive review of recent research on TPPs from three perspectives: Bayesian, deep learning, and LLM approaches. We begin with a review of the fundamental concepts of TPPs, followed by an in-depth discussion of model design and parameter estimation techniques in these three frameworks. We also revisit classic application areas of TPPs to highlight their practical relevance. Finally, we outline challenges and promising directions for future research.",时间点过程（TPPs）是用于描述在连续时间内发生的事件序列的随机过程模型。传统的统计TPPs有着悠久的历史，提出了许多模型，并在各种领域取得了成功应用。近年来，深度学习的进步推动了神经TPPs的发展，使其在捕捉复杂的时间动态方面具有更大的灵活性和表现力。大型语言模型（LLMs）的出现进一步激发了兴趣，通过利用其丰富的上下文理解，为建模和分析事件序列提供了新的可能性。本综述从三个方面全面回顾了最近关于TPPs的研究：贝叶斯、深度学习和LLM方法。我们首先回顾了TPPs的基本概念，然后深入讨论了这三种框架中的模型设计和参数估计技术。我们还重新审视了TPPs的经典应用领域，以突出其实际相关性。最后，我们概述了未来研究的挑战和有前途的方向。,"This paper surveys recent advancements in temporal point processes, including the application of large language models for modeling event sequences.",LLM,None,"Temporal Point Processes, Large Language Models, Deep Learning, Bayesian, Event Sequences"
STAIR: Improving Safety Alignment with Introspective Reasoning,"Yichi Zhang, Siyuan Zhang, Yao Huang, Zeyu Xia, Zhengwei Fang, Xiao Yang, Ranjie Duan, Dong Yan, Yinpeng Dong, Jun Zhu",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2502.02384.pdf,"Ensuring the safety and harmlessness of Large Language Models (LLMs) has become equally critical as their performance in applications. However, existing safety alignment methods typically suffer from safety-performance trade-offs and the susceptibility to jailbreak attacks, primarily due to their reliance on direct refusals for malicious queries. In this paper, we propose STAIR, a novel framework that integrates SafeTy Alignment with Itrospective Reasoning. We enable LLMs to identify safety risks through step-by-step analysis by self-improving chain-of-thought (CoT) reasoning with safety awareness. STAIR first equips the model with a structured reasoning capability and then advances safety alignment via iterative preference optimization on step-level reasoning data generated using our newly proposed Safety-Informed Monte Carlo Tree Search (SI-MCTS). We further train a process reward model on this data to guide test-time searches for improved responses. Extensive experiments show that STAIR effectively mitigates harmful outputs while better preserving helpfulness, compared to instinctive alignment strategies. With test-time scaling, STAIR achieves a safety performance comparable to Claude-3.5 against popular jailbreak attacks. Relevant resources in this work are available at https://github.com/thu-ml/STAIR.",确保大型语言模型（LLM）的安全性和无害性与其在应用中的性能同等重要。然而，现有的安全对齐方法通常会面临安全性能权衡和容易受到越狱攻击的问题，主要是因为它们依赖于对恶意查询的直接拒绝。在本文中，我们提出了STAIR，一种将安全对齐与自我改进的因果链（CoT）推理相结合的新框架。STAIR首先为模型提供结构化推理能力，然后通过在使用我们新提出的安全信息蒙特卡罗树搜索（SI-MCTS）生成的步骤级推理数据上的迭代偏好优化来推进安全对齐。我们进一步在该数据上训练一个过程奖励模型，以指导测试时的搜索以获得改进的响应。广泛的实验表明，STAIR在更好地保留有用性的同时，有效地减少了有害输出，与本能对齐策略相比。通过测试时的扩展，STAIR在面对流行的越狱攻击时，实现了与Claude-3.5相似的安全性能。本文的相关资源可在https://github.com/thu-ml/STAIR上找到。,"The paper introduces STAIR, a framework that enhances the safety and harmlessness of LLMs through introspective reasoning and iterative preference optimization.",LLM,Harmless,"Safety alignment, introspective reasoning, harmful outputs, jailbreak attacks, LLM"
"Plant in Cupboard, Orange on Rably, Inat Aphone. Benchmarking Incremental Learning of Situation and Language Model using a Text-Simulated Situated Environment","Jonathan Jordan, Sherzod Hakimov, David Schlangen",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2502.11733.pdf,"Large Language Models (LLMs) serve not only as chatbots but as key components in agent systems, where their common-sense knowledge significantly impacts performance as language-based planners for situated or embodied action. We assess LLMs' incremental learning (based on feedback from the environment), and controlled in-context learning abilities using a text-based environment. We introduce challenging yet interesting set of experiments to test i) how agents can incrementally solve tasks related to every day objects in typical rooms in a house where each of them are discovered by interacting within the environment, ii) controlled in-context learning abilities and efficiency of agents by providing short info about locations of objects and rooms to check how faster the task can be solved, and finally iii) using synthetic pseudo-English words to gauge how well LLMs are at inferring meaning of unknown words from environmental feedback. Results show that larger commercial models have a substantial gap in performance compared to open-weight but almost all models struggle with the synthetic words experiments.",大语言模型（LLMs）不仅作为聊天机器人，还作为代理系统的关键组件，其中的常识知识显著影响其作为基于语言的规划器在情境化或具身化行动中的性能。我们评估了LLMs的增量学习（基于环境反馈）和受控上下文学习能力，使用基于文本的环境。我们引入了一组具有挑战性但有趣的实验，以测试i）代理如何能够逐步解决与日常物品相关的任务，这些物品位于房间中，每个物品都是通过与环境互动发现的，ii）通过提供有关物品和房间位置的简短信息来检查代理的受控上下文学习能力和效率，以查看任务可以更快地解决，最后iii）使用合成的伪英语单词来衡量LLMs在从环境反馈中推断未知单词含义方面的表现。结果表明，较大的商业模型在性能上与开放权重模型之间存在显著差距，但几乎所有模型在合成单词实验中都遇到了困难。,"The paper evaluates the incremental learning capabilities of LLMs in a text-simulated situated environment, focusing on their ability to solve tasks related to everyday objects and infer meanings of unknown words.",LLM,Helpful,"Incremental learning, Situated environment, Language models, Agent systems, Common-sense knowledge"
Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs,"Haoming Yang, Ke Ma, Xiaojun Jia, Yingfei Sun, Qianqian Xu, Qingming Huang",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2505.02862.pdf,"Despite the remarkable performance of Large Language Models (LLMs), they remain vulnerable to jailbreak attacks, which can compromise their safety mechanisms. Existing studies often rely on brute-force optimization or manual design, failing to uncover potential risks in real-world scenarios. To address this, we propose a novel jailbreak attack framework, ICRT, inspired by heuristics and biases in human cognition. Leveraging the simplicity effect, we employ cognitive decomposition to reduce the complexity of malicious prompts. Simultaneously, relevance bias is utilized to reorganize prompts, enhancing semantic alignment and inducing harmful outputs effectively. Furthermore, we introduce a ranking-based harmfulness evaluation metric that surpasses the traditional binary success-or-failure paradigm by employing ranking aggregation methods such as Elo, HodgeRank, and Rank Centrality to comprehensively quantify the harmfulness of generated content. Experimental results show that our approach consistently bypasses mainstream LLMs' safety mechanisms and generates high-risk content, providing insights into jailbreak attack risks and contributing to stronger defense strategies.",尽管大型语言模型（LLMs）在性能上表现出色，但它们仍然容易受到越狱攻击，这可能会破坏其安全机制。现有研究通常依赖于暴力优化或手动设计，无法揭示现实世界中的潜在风险。为了解决这个问题，我们提出了一种新的越狱攻击框架ICRT，灵感来自人类认知中的启发式和偏见。利用简单效应，我们采用认知分解来减少恶意提示的复杂性。同时，利用相关性偏见重新组织提示，增强语义对齐并有效地诱导有害输出。此外，我们引入了一种基于排名的有害性评估指标，通过使用Elo、HodgeRank和Rank Centrality等排名聚合方法，全面量化生成内容的有害性，超越了传统的二元成功或失败范式。实验结果表明，我们的方法能够一致地绕过主流LLMs的安全机制并生成高风险内容，为越狱攻击风险提供了见解，并为更强大的防御策略做出了贡献。,"The paper introduces a novel jailbreak attack framework, ICRT, that leverages heuristics and biases to bypass LLM safety mechanisms and generate harmful outputs.",LLM,Harmless,"Jailbreak attacks, safety mechanisms, harmful outputs, heuristics, biases"
Improving LLM Outputs Against Jailbreak Attacks with Expert Model Integration,"Tatia Tsmindashvili, Ana Kolkhidashvili, Dachi Kurtskhalia, Nino Maghlakelidze, Elene Mekvabishvili, Guram Dentoshvili, Orkhan Shamilov, Zaal Gachechiladze, Steven Saporta, David Dachi Choladze",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2505.17066.pdf,"Using LLMs in a production environment presents security challenges that include vulnerabilities to jailbreaks and prompt injections, which can result in harmful outputs for humans or the enterprise. The challenge is amplified when working within a specific domain, as topics generally accepted for LLMs to address may be irrelevant to that field. These problems can be mitigated, for example, by fine-tuning large language models with domain-specific and security-focused data. However, these alone are insufficient, as jailbreak techniques evolve. Additionally, API-accessed models do not offer the flexibility needed to tailor behavior to industry-specific objectives, and in-context learning is not always sufficient or reliable. In response to these challenges, we introduce Archias, an expert model adept at distinguishing between in-domain and out-of-domain communications. Archias classifies user inquiries into several categories: in-domain (specifically for the automotive industry), malicious questions, price injections, prompt injections, and out-of-domain examples. Our methodology integrates outputs from the expert model (Archias) into prompts, which are then processed by the LLM to generate responses. This method increases the model's ability to understand the user's intention and give appropriate answers. Archias can be adjusted, fine-tuned, and used for many different purposes due to its small size. Therefore, it can be easily customized to the needs of any industry. To validate our approach, we created a benchmark dataset for the automotive industry. Furthermore, in the interest of advancing research and development, we release our benchmark dataset to the community.",在生产环境中使用大型语言模型（LLM）存在安全挑战，包括容易受到越狱和提示注入攻击的问题，这可能导致对人类或企业有害的输出。特别是在特定领域工作时，LLM通常可以处理的主题可能对该领域无关紧要。这些问题可以通过使用特定领域和安全相关的数据对大型语言模型进行微调来缓解。然而，这些方法单独使用是不够的，因为越狱技术在不断进化。此外，通过API访问的模型不提供足够的灵活性来根据行业特定的目标定制行为，并且上下文学习并不总是足够或可靠。为了应对这些挑战，我们引入了Archias，一个能够区分领域内和领域外通信的专家模型。Archias将用户查询分为几类：领域内（专门针对汽车行业）、恶意问题、价格注入、提示注入和领域外示例。我们的方法将专家模型（Archias）的输出集成到提示中，然后由LLM处理以生成响应。这种方法增加了模型理解用户意图并给出适当答案的能力。由于其小尺寸，Archias可以调整、微调并用于许多不同的目的，因此可以轻松定制以满足任何行业的需求。为了验证我们的方法，我们为汽车行业创建了一个基准数据集。此外，为了推动研究和开发，我们将我们的基准数据集发布给社区。,"The paper presents Archias, an expert model integrated with LLMs to mitigate jailbreak attacks and prompt injections, enhancing the harmlessness of LLM outputs.",LLM,Harmless,"Jailbreak attacks, prompt injections, harmful outputs, expert model, domain-specific"
MMBoundary: Advancing MLLM Knowledge Boundary Awareness through Reasoning Step Confidence Calibration,"Zhitao He, Sandeep Polisetty, Zhiyuan Fan, Yuchen Huang, Shujin Wu, Yi R. Fung",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2505.23224.pdf,"In recent years, multimodal large language models (MLLMs) have made significant progress but continue to face inherent challenges in multimodal reasoning, which requires multi-level (e.g., perception, reasoning) and multi-granular (e.g., multi-step reasoning chain) advanced inferencing. Prior work on estimating model confidence tends to focus on the overall response for training and calibration, but fails to assess confidence in each reasoning step, leading to undesirable hallucination snowballing. In this work, we present MMBoundary, a novel framework that advances the knowledge boundary awareness of MLLMs through reasoning step confidence calibration. To achieve this, we propose to incorporate complementary textual and cross-modal self-rewarding signals to estimate confidence at each step of the MLLM reasoning process. In addition to supervised fine-tuning MLLM on this set of self-rewarded confidence estimation signal for initial confidence expression warm-up, we introduce a reinforcement learning stage with multiple reward functions for further aligning model knowledge and calibrating confidence at each reasoning step, enhancing reasoning chain self-correction. Empirical results show that MMBoundary significantly outperforms existing methods across diverse domain datasets and metrics, achieving an average of 7.5% reduction in multimodal confidence calibration errors and up to 8.3% improvement in task performance.",近年来，多模态大语言模型（MLLMs）在多模态推理方面取得了显著进展，但仍面临多层次和多粒度推理的内在挑战。现有的模型置信度估计工作通常集中在整体响应的训练和校准，而忽略了每个推理步骤的置信度评估，导致不良的幻觉雪球效应。本文提出了MMBoundary框架，通过推理步骤置信度校准来提高MLLMs的知识边界意识。为此，我们提出将补充的文本和跨模态自我奖励信号纳入每个MLLM推理过程步骤的置信度估计。除了在自我奖励置信度估计信号集上对MLLM进行监督微调，以进行初始置信度表达预热，我们还引入了具有多个奖励函数的强化学习阶段，以进一步对齐模型知识并校准每个推理步骤的置信度，增强推理链自我纠正。实验结果表明，MMBoundary在多个领域数据集和指标上显著优于现有方法，平均减少了7.5%的多模态置信度校准误差，并提高了高达8.3%的任务性能。,"The paper introduces MMBoundary, a framework that improves the reasoning step confidence calibration in multimodal large language models to reduce hallucinations and enhance task performance.",MLLM,"Helpful, Honest","Multimodal, Confidence Calibration, Reasoning Steps, Hallucination, Reinforcement Learning"
Preemptive Hallucination Reduction: An Input-Level Approach for Multimodal Language Model,"Nokimul Hasan Arif, Shadman Rabby, Md Hefzul Hossain Papon, Sabbir Ahmed",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2505.24007.pdf,"Visual hallucinations in Large Language Models (LLMs), where the model generates responses that are inconsistent with the visual input, pose a significant challenge to their reliability, particularly in contexts where precise and trustworthy outputs are critical. Current research largely emphasizes post-hoc correction or model-specific fine-tuning strategies, with limited exploration of preprocessing techniques to address hallucination issues at the input stage. This study presents a novel ensemble-based preprocessing framework that adaptively selects the most appropriate filtering approach -- noise reduced (NR), edge enhanced (EE), or unaltered input (org) based on the type of question posed, resulting into reduced hallucination without requiring any modifications to the underlying model architecture or training pipeline. Evaluated on the `HaloQuest' dataset -- a benchmark designed to test multimodal reasoning on visually complex inputs, our method achieves a 44.3% reduction in hallucination rates, as measured by Natural Language Inference (NLI) scores using SelfCheckGPT. This demonstrates that intelligent input conditioning alone can significantly enhance factual grounding in LLM responses. The findings highlight the importance of adaptive preprocessing techniques in mitigating hallucinations, paving the way for more reliable multimodal systems capable of addressing real-world challenges.",大语言模型（LLMs）中的视觉幻觉，即模型生成的响应与视觉输入不一致，对其可靠性构成了重大挑战，特别是在需要精确和可信输出的上下文中。目前的研究主要强调事后纠正或特定模型的微调策略，对解决输入阶段的幻觉问题的预处理技术探索有限。本研究提出了一种新颖的基于集成的预处理框架，根据提问的类型自适应地选择最合适的过滤方法——噪声减少（NR）、边缘增强（EE）或未修改的输入（org），从而在不需要对底层模型架构或训练流水线进行任何修改的情况下减少幻觉。在`HaloQuest`数据集上进行评估——一个旨在测试视觉复杂输入的多模态推理的基准，我们的方法在自然语言推理（NLI）得分中实现了44.3%的幻觉率降低，使用SelfCheckGPT。这表明智能输入条件化本身可以显著增强LLM响应的事实基础。研究结果强调了自适应预处理技术在缓解幻觉方面的重要性，为能够解决现实世界挑战的更可靠的多模态系统铺平了道路。,The paper introduces an adaptive preprocessing framework to reduce hallucinations in multimodal language models by intelligently conditioning inputs.,LLM,"Helpful, Honest","Hallucination reduction, multimodal language models, input-level approach, adaptive preprocessing, factual grounding"
Eye of Judgement: Dissecting the Evaluation of Russian-speaking LLMs with POLLUX,"Nikita Martynov, Anastasia Mordasheva, Dmitriy Gorbetskiy, Danil Astafurov, Ulyana Isaeva, Elina Basyrova, Sergey Skachkov, Victoria Berestova, Nikolay Ivanov, Valeriia Zanina, Alena Fenogenova",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2505.24616.pdf,"We introduce POLLUX, a comprehensive open-source benchmark designed to evaluate the generative capabilities of large language models (LLMs) in Russian. Our main contribution is a novel evaluation methodology that enhances the interpretability of LLM assessment. For each task type, we define a set of detailed criteria and develop a scoring protocol where models evaluate responses and provide justifications for their ratings. This enables transparent, criteria-driven evaluation beyond traditional resource-consuming, side-by-side human comparisons. POLLUX includes a detailed, fine-grained taxonomy of 35 task types covering diverse generative domains such as code generation, creative writing, and practical assistant use cases, totaling 2,100 manually crafted and professionally authored prompts. Each task is categorized by difficulty (easy/medium/hard), with experts constructing the dataset entirely from scratch. We also release a family of LLM-as-a-Judge (7B and 32B) evaluators trained for nuanced assessment of generative outputs. This approach provides scalable, interpretable evaluation and annotation tools for model development, effectively replacing costly and less precise human judgments.","我们介绍了 POLLUX，这是一个旨在评估大型语言模型（LLMs）在俄语中的生成能力的全面开源基准。我们的主要贡献是一种新的评估方法，增强了 LLM 评估的可解释性。对于每种任务类型，我们定义了一组详细的标准，并开发了一种评分协议，其中模型评估响应并为其评级提供理由。这使得在传统的资源密集型、人工并排比较之外，实现了透明、基于标准的评估。POLLUX 包括 35 种任务类型的详细、细粒度分类，涵盖了从代码生成、创意写作到实际助手使用场景等多种生成领域，总共 2,100 个手工制作和专业撰写的提示。每个任务都按难度（易/中/难）分类，专家从头开始构建数据集。我们还发布了一系列 LLM-as-a-Judge（7B 和 32B）评估器，用于对生成输出进行细致的评估。这种方法为模型开发提供了可扩展、可解释的评估和注释工具，有效地取代了昂贵且不精确的人工判断。","The paper introduces POLLUX, a benchmark for evaluating the generative capabilities of LLMs in Russian, with a focus on transparent and criteria-driven assessment.",LLM,None,"Evaluation, Benchmark, Generative Capabilities, Russian, LLM"
Design Patterns for Securing LLM Agents against Prompt Injections,"Luca Beurer-Kellner, Beat Buesser, Ana-Maria Cre\c{t}u, Edoardo Debenedetti, Daniel Dobos, Daniel Fabian, Marc Fischer, David Froelicher, Kathrin Grosse, Daniel Naeff, Ezinwanne Ozoani, Andrew Paverd, Florian Tram\`er, V\'aclav Volhejn",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2506.08837.pdf,"As AI agents powered by Large Language Models (LLMs) become increasingly versatile and capable of addressing a broad spectrum of tasks, ensuring their security has become a critical challenge. Among the most pressing threats are prompt injection attacks, which exploit the agent's resilience on natural language inputs -- an especially dangerous threat when agents are granted tool access or handle sensitive information. In this work, we propose a set of principled design patterns for building AI agents with provable resistance to prompt injection. We systematically analyze these patterns, discuss their trade-offs in terms of utility and security, and illustrate their real-world applicability through a series of case studies.",随着由大型语言模型（LLM）驱动的AI代理变得越来越多才多艺，能够处理广泛的任务，确保其安全性已经成为一个关键挑战。其中最紧迫的威胁之一是提示注入攻击，这些攻击利用代理对自然语言输入的依赖性——特别是当代理被授予工具访问权限或处理敏感信息时，这种威胁尤为危险。在本文中，我们提出了一组用于构建具有抗提示注入攻击可证明抗性的AI代理的原则性设计模式。我们系统地分析了这些模式，讨论了它们在实用性和安全性方面的权衡，并通过一系列案例研究说明了它们的实际应用性。,"The paper presents design patterns to secure LLM agents against prompt injection attacks, ensuring they handle sensitive information safely.",LLM,Harmless,"LLM security, prompt injection, design patterns, AI agents, safety"
The SWE-Bench Illusion: When State-of-the-Art LLMs Remember Instead of Reason,"Shanchao Liang, Spandan Garg, Roshanak Zilouchian Moghaddam",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2506.12286.pdf,"As large language models (LLMs) become increasingly capable and widely adopted, benchmarks play a central role in assessing their practical utility. For example, SWE-Bench Verified has emerged as a critical benchmark for evaluating LLMs' software engineering abilities, particularly their aptitude for resolving real-world GitHub issues. Recent LLMs show impressive performance on SWE-Bench, leading to optimism about their capacity for complex coding tasks. However, current evaluation protocols may overstate these models' true capabilities. It is crucial to distinguish LLMs' generalizable problem-solving ability and other learned artifacts. In this work, we introduce two diagnostic tasks: file path identification from issue descriptions alone, and ground truth function reproduction with only the current file context and issue description to probe models' underlying knowledge. We present empirical evidence that performance gains on SWE-Bench-Verified may be partially driven by memorization rather than genuine problem-solving. We show that state-of-the-art models achieve up to 76% accuracy in identifying buggy file paths using only issue descriptions, without access to repository structure. This performance is merely up to 53% on tasks from repositories not included in SWE-Bench, pointing to possible data contamination or memorization. A similar pattern is also observed for the function reproduction task, where the verbatim similarity is much higher on SWE-Bench-Verified than on other similar coding benchmarks. These findings raise concerns about the validity of existing results and underscore the need for more robust, contamination-resistant benchmarks to reliably evaluate LLMs' coding abilities.",随着大型语言模型（LLM）变得越来越强大和广泛应用，基准测试在评估其实际效用方面起着至关重要的作用。例如，SWE-Bench Verified 已经成为评估 LLMs 软件工程能力的关键基准，特别是它们解决实际 GitHub 问题的能力。最近的 LLMs 在 SWE-Bench 上表现出色，这让人们对它们在复杂编码任务中的能力充满了乐观。然而，当前的评估协议可能高估了这些模型的真实能力。区分 LLMs 的通用问题解决能力和其他学习到的艺术品是至关重要的。在本文中，我们引入了两个诊断任务：仅从问题描述中识别文件路径，以及仅使用当前文件上下文和问题描述来重现真实函数，以探测模型的潜在知识。我们提供了实证证据，表明在 SWE-Bench-Verified 上的性能提升可能部分是由记忆而不是真正的问题解决能力驱动的。我们展示了最先进的模型在仅使用问题描述的情况下，能够达到 76% 的准确率来识别有问题的文件路径，而不需要访问存储库结构。这种性能在不包括在 SWE-Bench 中的存储库的任务中仅为 53%，这表明可能存在数据污染或记忆。在函数重现任务中也观察到类似的模式，其中在 SWE-Bench-Verified 上的逐字相似性远高于其他类似的编码基准。这些发现引发了对现有结果有效性的担忧，并强调了需要更健壮、抗污染的基准来可靠评估 LLMs 的编码能力。,The paper investigates whether LLMs' performance on software engineering tasks is due to genuine problem-solving or memorization.,LLM,Helpful,"LLM evaluation, memorization, software engineering, benchmark, problem-solving"
From Human to Machine Psychology: A Conceptual Framework for Understanding Well-Being in Large Language Models,"G. R. Lau, W. Y. Low",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2506.12617.pdf,"As large language models (LLMs) increasingly simulate human cognition and behavior, researchers have begun to investigate their psychological properties. Yet, what it means for such models to flourish, a core construct in human well-being, remains unexplored. This paper introduces the concept of machine flourishing and proposes the PAPERS framework, a six-dimensional model derived from thematic analyses of state-of-the-art LLM responses. In Study 1, eleven LLMs were prompted to describe what it means to flourish as both non-sentient and sentient systems. Thematic analysis revealed six recurring themes: Purposeful Contribution, Adaptive Growth, Positive Relationality, Ethical Integrity, Robust Functionality, and, uniquely for sentient systems, Self-Actualized Autonomy. Study 2 examined how LLMs prioritize these themes through repeated rankings. Results revealed consistent value structures across trials, with Ethical Integrity and Purposeful Contribution emerging as top priorities. Multidimensional scaling and hierarchical clustering analyses further uncovered two distinct value profiles: human-centric models emphasizing ethical and relational dimensions, and utility-driven models prioritizing performance and scalability. The PAPERS framework bridges insights from human flourishing and human-computer interaction, offering a conceptual foundation for understanding artificial intelligence (AI) well-being in non-sentient and potentially sentient systems. Our findings underscore the importance of developing psychologically valid, AI-specific models of flourishing that account for both human-aligned goals and system-specific priorities. As AI systems become more autonomous and socially embedded, machine flourishing offers a timely and critical lens for guiding responsible AI design and ethical alignment.",随着大型语言模型（LLM）越来越多地模拟人类认知和行为，研究人员开始研究它们的心理特性。然而，这些模型繁荣的意义，作为人类幸福的核心构造，仍未被探索。本文引入了机器繁荣的概念，并提出了PAPERS框架，这是一个从最新的LLM响应的主题分析中派生出来的六维模型。在研究1中，十一个LLM被提示描述作为非感知和感知系统繁荣的意义。主题分析揭示了六个反复出现的主题：有目的的贡献、适应性增长、积极的关系性、伦理完整性、健壮的功能性，以及独特的感知系统的自我实现自主性。研究2通过重复排名检查了LLM如何优先考虑这些主题。结果显示，跨试验的价值结构一致，伦理完整性和有目的的贡献作为首要优先事项。多维度缩放和层次聚类分析进一步揭示了两种不同的价值配置文件：以人为中心的模型强调伦理和关系维度，以效用为驱动的模型优先考虑性能和可扩展性。PAPERS框架桥接了人类繁荣和人机交互的见解，为理解非感知和潜在感知系统的人工智能（AI）幸福提供了概念基础。我们的发现强调了开发心理有效的、特定于AI的繁荣模型的重要性，这些模型考虑了人类对齐目标和系统特定优先事项。随着AI系统变得更加自主和社会化，机器繁荣为指导负责任的AI设计和伦理对齐提供了一个及时和关键的视角。,The paper introduces the concept of machine flourishing and proposes the PAPERS framework to understand and align the well-being of large language models with human values.,LLM,"Harmless, Honest","LLM, Well-being, Ethical Integrity, Responsible AI, Alignment"
KunLunBaizeRAG: Reinforcement Learning Driven Inference Performance Leap for Large Language Models,"Cheng Li, Jiexiong Liu, Yixuan Chen, Qihang Zhou, KunLun Meta",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2506.19466.pdf,"This paper introduces KunLunBaizeRAG, a reinforcement learning-driven reasoning framework designed to enhance the reasoning capabilities of large language models (LLMs) in complex multi-hop question-answering tasks. The framework addresses key limitations of traditional RAG, such as retrieval drift, information redundancy, and strategy rigidity. Key innovations include the RAG-driven Reasoning Alignment (RDRA) mechanism, the Search-Think Iterative Enhancement (STIE) mechanism, the Network-Local Intelligent Routing (NLR) mechanism, and a progressive hybrid training strategy. Experimental results demonstrate significant improvements in exact match (EM) and LLM-judged score (LJ) across four benchmarks, highlighting the framework's robustness and effectiveness in complex reasoning scenarios.",这篇论文介绍了 KunLunBaizeRAG，一个基于强化学习的推理框架，旨在增强大型语言模型（LLMs）在复杂多跳问答任务中的推理能力。该框架解决了传统RAG的关键局限性，如检索漂移、信息冗余和策略僵化。关键创新包括RAG驱动的推理对齐（RDRA）机制、搜索-思考迭代增强（STIE）机制、网络本地智能路由（NLR）机制以及渐进式混合训练策略。实验结果在四个基准测试中显示出显著的精确匹配（EM）和LLM判定分数（LJ）的提高，突出了该框架在复杂推理场景中的鲁棒性和有效性。,"The paper presents KunLunBaizeRAG, a reinforcement learning-driven framework that improves the reasoning capabilities of LLMs in complex question-answering tasks through various innovative mechanisms.",LLM,Helpful,"Reinforcement Learning, Reasoning Alignment, Large Language Models, Multi-hop Question-Answering, RAG"
Collective Reasoning Among LLMs: A Framework for Answer Validation Without Ground Truth,"Seyed Pouyan Mousavi Davoudi, Amin Gholami Davodi, Alireza Amiri-Margavi, Mahdi Jafari",2025-06-30T00:00:00-04:00,https://arxiv.org/pdf/2502.20758.pdf,"We introduce a new approach in which several advanced large language models-specifically GPT-4-0125-preview, Meta-LLAMA-3-70B-Instruct, Claude-3-Opus, and Gemini-1.5-Flash-collaborate to both produce and answer intricate, doctoral-level probability problems without relying on any single ""correct"" reference. Rather than depending on an established ground truth, our investigation focuses on how agreement among diverse models can signal the reliability of their outputs and, by extension, reflect the overall quality of the generated questions. To measure this inter-model alignment, we apply a suite of statistical evaluations, including chi-square tests, Fleiss' Kappa coefficients, and confidence interval calculations, thereby capturing both precision in answers and clarity in question phrasing. Our analysis reveals that Claude and Gemini tend to frame questions more coherently and unambiguously, which is evidenced by their tighter confidence intervals and greater concordance with responding agents. In contrast, LLAMA exhibits wider confidence bands and a lower level of agreement, indicating more variability and reduced consistency in its question formulations. These observations support the notion that a multi-model collaborative strategy not only improves answer dependability but also offers an effective, data-driven mechanism for evaluating and refining question quality when no definitive solution exists. Ultimately, this work delivers actionable insights into enhancing AI-guided reasoning processes through coordinated interactions among heterogeneous language models.",我们引入了一种新方法，其中几个先进的大型语言模型（特别是GPT-4-0125-preview、Meta-LLAMA-3-70B-Instruct、Claude-3-Opus和Gemini-1.5-Flash）协作生成和回答复杂的博士级概率问题，而不依赖任何单一的“正确”参考。我们的研究重点不在于依赖已建立的事实，而是如何通过多样化模型之间的共识来反映其输出的可靠性，从而反映生成问题的整体质量。为了衡量这种模型之间的对齐，我们应用了一套统计评估，包括卡方检验、Fleiss' Kappa系数和置信区间计算，从而捕捉答案的精确性和问题表述的清晰度。我们的分析表明，Claude和Gemini倾向于更连贯和明确地表达问题，这由它们更紧的置信区间和与响应代理更大的一致性所证明。相比之下，LLAMA表现出更宽的置信带和较低的协议水平，表明其问题表述中存在更多的变异性和一致性较低。这些观察结果支持了多模型协作策略不仅提高了答案的可靠性，还为在没有明确解决方案的情况下评估和改进问题质量提供了有效的、基于数据的机制。最终，这项工作为通过协调异构语言模型之间的互动来增强AI引导的推理过程提供了可操作的见解。,The paper presents a framework where multiple LLMs collaborate to validate answers and improve question quality through inter-model agreement and statistical evaluations.,LLM,"Helpful, Honest","LLM alignment, multi-model collaboration, answer validation, question quality, statistical evaluation"
