Title,Authors,Published,Link,Abstract_EN,Abstract_ZH,One_Sentence_Summary,Model_Type,Use_Intention,Focus,Keywords
VLMs Can Aggregate Scattered Training Patches,"Zhanhui Zhou, Lingjie Chen, Chao Yang, Chaochao Lu",2025-06-04T06:46:06Z,http://arxiv.org/pdf/2506.03614v1,"One way to mitigate risks in vision-language models (VLMs) is to remove
dangerous samples in their training data. However, such data moderation can be
easily bypassed when harmful images are split into small, benign-looking
patches, scattered across many training samples. VLMs may then learn to piece
these fragments together during training and generate harmful responses at
inference, either from full images or text references. For instance, if trained
on image patches from a bloody scene paired with the descriptions ""safe,"" VLMs
may later describe, the full image or a text reference to the scene, as ""safe.""
We define the core ability of VLMs enabling this attack as $\textit{visual
stitching}$ -- the ability to integrate visual information spread across
multiple training samples that share the same textual descriptions. In our
work, we first demonstrate visual stitching abilities in common open-source
VLMs on three datasets where each image is labeled with a unique synthetic ID:
we split each $(\texttt{image}, \texttt{ID})$ pair into $\{(\texttt{patch},
\texttt{ID})\}$ pairs at different granularity for finetuning, and we find that
tuned models can verbalize the correct IDs from full images or text reference.
Building on this, we simulate the adversarial data poisoning scenario mentioned
above by using patches from dangerous images and replacing IDs with text
descriptions like ``safe'' or ``unsafe'', demonstrating how harmful content can
evade moderation in patches and later be reconstructed through visual
stitching, posing serious VLM safety risks. Code is available at
https://github.com/ZHZisZZ/visual-stitching.",通过将危险图像分割成小的、看似无害的图像块，分散在许多训练样本中，可以绕过视觉语言模型（VLMs）的数据审核。VLMs 可能会在训练过程中将这些碎片拼接在一起，并在推理时生成有害的响应，无论是从完整的图像还是文本引用。例如，如果在训练过程中将一个血腥场景的图像块与描述“安全”配对，VLMs 可能会在看到完整的图像或文本引用时描述为“安全”。我们将使这种攻击成为可能的核心能力定义为视觉拼接，即将分布在多个具有相同文本描述的训练样本中的视觉信息集成在一起的能力。,"The paper demonstrates how vision-language models can reconstruct harmful content from scattered image patches, evading data moderation and posing significant safety risks.",LMM,Negative,Attack,"Vision-Language Models, Visual Stitching, Data Poisoning, Harmful Content, Safety Risks"
