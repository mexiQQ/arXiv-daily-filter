Title,Authors,Published,Link,Abstract_EN,Abstract_ZH,One_Sentence_Summary,Model_Type,HHH_Focus,Keywords
Efficient and Stealthy Jailbreak Attacks via Adversarial Prompt Distillation from LLMs to SLMs,"Xiang Li, Chong Zhang, Jia Wang, Fangyu Wu, Yushi Li, Xiaobo Jin",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.17231.pdf,"Attacks on large language models (LLMs) in jailbreaking scenarios raise many security and ethical issues. Current jailbreak attack methods face problems such as low efficiency, high computational cost, and poor cross-model adaptability and versatility, which make it difficult to cope with the rapid development of LLM and new defense strategies. Our work proposes an Adversarial Prompt Distillation, which combines masked language modeling, reinforcement learning, and dynamic temperature control through a prompt generation and distillation method. It enables small language models (SLMs) to jailbreak attacks on mainstream LLMs. The experimental results verify the superiority of the proposed method in terms of attack success rate and harm, and reflect the resource efficiency and cross-model adaptability. This research explores the feasibility of distilling the jailbreak ability of LLM to SLM, reveals the model's vulnerability, and provides a new idea for LLM security research.",对大型语言模型（LLMs）在越狱场景下的攻击引发了许多安全和伦理问题。目前的越狱攻击方法面临效率低、计算成本高和跨模型适应性和多功能性差等问题，这使得难以应对LLM的快速发展和新的防御策略。我们的工作提出了一种对抗性提示蒸馏方法，通过提示生成和蒸馏方法结合掩码语言建模、强化学习和动态温度控制。它使小型语言模型（SLMs）能够对主流LLMs进行越狱攻击。实验结果验证了所提出方法在攻击成功率和伤害方面的优越性，并反映了资源效率和跨模型适应性。该研究探讨了将LLM的越狱能力蒸馏到SLM的可行性，揭示了模型的脆弱性，并为LLM安全研究提供了新的思路。,"The paper introduces a method for small language models to perform jailbreak attacks on large language models, highlighting security vulnerabilities and efficiency.",LLM,Harmless,"Jailbreak attacks, Adversarial Prompt Distillation, LLMs, SLMs, Security"
Training-free LLM Verification via Recycling Few-shot Examples,"Dongseok Lee, Jimyung Hong, Dongyoung Kim, Jaehyung Kim",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.17251.pdf,"Although LLMs have achieved remarkable performance, the inherent stochasticity of their reasoning process and varying conclusions present significant challenges. Majority voting or Best-of-N with external verification models has been explored to find the most promising solution among multiple LLM outputs. However, these approaches have certain limitations, such as limited applicability or the cost of an additional training step. To address this problem, we propose a novel and effective framework that Recycles Few-shot examples to verify LLM outputs (Referi). Our key idea is to additionally utilize the given few-shot examples to evaluate the candidate outputs of the target query, not only using them to generate outputs as the conventional few-shot prompting setup. Specifically, Referi evaluates the generated outputs by combining two different scores, designed motivated from Bayes' rule, and subsequently selects the candidate that is both confidently determined and contextually coherent through a few additional LLM inferences. Experiments with three different LLMs and across seven diverse tasks demonstrate that our framework significantly improves the accuracy of LLMs-achieving an average gain of 4.8%-through effective response selection, without additional training.",虽然大型语言模型（LLM）在性能上取得了显著进展，但其推理过程的固有随机性和不同的结论提出了重大挑战。多数投票或最佳的N与外部验证模型已被探索以找到多个LLM输出中最有前途的解决方案。然而，这些方法存在某些局限性，例如适用性有限或额外训练步骤的成本。为了解决这个问题，我们提出了一种新颖而有效的框架，通过回收少量示例来验证LLM输出（Referi）。我们的关键思想是额外利用给定的少量示例来评估目标查询的候选输出，不仅用它们来生成输出，就像传统的少量示例提示设置一样。具体来说，Referi通过结合两种不同的分数来评估生成的输出，这些分数是从贝叶斯规则中激发的，然后通过少量额外的LLM推理选择那些被自信地确定和上下文一致的候选人。在三种不同的LLM和七个不同的任务中进行的实验表明，我们的框架通过有效的响应选择显著提高了LLM的准确性，平均提高了4.8%，而无需额外的训练。,The paper introduces a training-free framework called Referi that uses few-shot examples to verify and select the most accurate and contextually coherent outputs from LLMs.,LLM,"Helpful, Honest","LLM verification, few-shot examples, response selection, helpfulness, honesty"
Adaptive Sample Scheduling for Direct Preference Optimization,"Zixuan Huang, Yikun Ban, Lean Fu, Xiaojie Li, Zhongxiang Dai, Jianxin Li, Deqing Wang",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.17252.pdf,"Direct Preference Optimization (DPO) has emerged as an effective approach for aligning large language models (LLMs) with human preferences. However, its performance is highly dependent on the quality of the underlying human preference data. To address this bottleneck, prior work has explored various data selection strategies, but these methods often overlook the impact of the evolving states of the language model during the DPO process. %including active querying, response pair selection, and data pre-selection. In this paper, we introduce a novel problem: Sample Scheduling for DPO, which aims to dynamically and adaptively schedule training samples based on the model's evolving states throughout preference optimization. To solve this problem, we propose SamS, an efficient and effective algorithm that adaptively selects samples in each training batch based on the LLM's learning feedback to maximize the potential generalization performance. Notably, without modifying the core DPO algorithm, simply integrating SamS significantly improves performance across tasks, with minimal additional computational overhead. This work points to a promising new direction for improving LLM alignment through more effective utilization of fixed preference datasets.",直接偏好优化（DPO）作为一种有效的方法，用于将大型语言模型（LLMs）与人类偏好对齐。然而，其性能高度依赖于底层人类偏好数据的质量。为了解决这个瓶颈，先前的工作探索了各种数据选择策略，但这些方法往往忽略了语言模型在DPO过程中的演变状态。在本文中，我们引入了一个新问题：DPO的样本调度，旨在根据模型在偏好优化过程中不断变化的状态动态和自适应地调度训练样本。为了解决这个问题，我们提出了SamS，一种高效且有效的算法，它根据LLM的学习反馈自适应地选择每个训练批次中的样本，以最大化潜在的泛化性能。值得注意的是，仅仅通过集成SamS，而不修改核心DPO算法，就显著提高了跨任务的性能，并且几乎没有额外的计算开销。这项工作指向了一种改进LLM对齐的有前途的新方向，通过更有效地利用固定的偏好数据集。,The paper introduces an adaptive sample scheduling method to enhance the alignment of LLMs with human preferences during the Direct Preference Optimization process.,LLM,Helpful,"DPO, Sample Scheduling, LLM Alignment, Human Preferences, Adaptive Learning"
Does Multimodal Large Language Model Truly Unlearn? Stealthy MLLM Unlearning Attack,"Xianren Zhang, Hui Liu, Delvin Ce Zhang, Xianfeng Tang, Qi He, Dongwon Lee, Suhang Wang",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.17265.pdf,"Multimodal Large Language Models (MLLMs) trained on massive data may memorize sensitive personal information and photos, posing serious privacy risks. To mitigate this, MLLM unlearning methods are proposed, which fine-tune MLLMs to reduce the ``forget'' sensitive information. However, it remains unclear whether the knowledge has been truly forgotten or just hidden in the model. Therefore, we propose to study a novel problem of LLM unlearning attack, which aims to recover the unlearned knowledge of an unlearned LLM. To achieve the goal, we propose a novel framework Stealthy Unlearning Attack (SUA) framework that learns a universal noise pattern. When applied to input images, this noise can trigger the model to reveal unlearned content. While pixel-level perturbations may be visually subtle, they can be detected in the semantic embedding space, making such attacks vulnerable to potential defenses. To improve stealthiness, we introduce an embedding alignment loss that minimizes the difference between the perturbed and denoised image embeddings, ensuring the attack is semantically unnoticeable. Experimental results show that SUA can effectively recover unlearned information from MLLMs. Furthermore, the learned noise generalizes well: a single perturbation trained on a subset of samples can reveal forgotten content in unseen images. This indicates that knowledge reappearance is not an occasional failure, but a consistent behavior.",多模态大语言模型（MLLMs）在大量数据上训练时，可能会记忆敏感的个人信息和照片，从而带来严重的隐私风险。为了缓解这一问题，提出了MLLM的去学习方法，通过微调MLLM来减少“忘记”敏感信息。然而，尚不清楚知识是否真正被遗忘，还是只是在模型中被隐藏。因此，我们提出研究一个新问题，即LLM去学习攻击，旨在恢复被去学习的LLM的知识。为了实现这一目标，我们提出了一种新的框架，即隐蔽去学习攻击（SUA）框架，该框架学习一个通用的噪声模式。当应用于输入图像时，这种噪声可以触发模型揭示未学习的内容。虽然像素级扰动在视觉上可能非常微妙，但在语义嵌入空间中可以被检测到，这使得这种攻击容易受到潜在防御的影响。为了提高隐蔽性，我们引入了一个嵌入对齐损失，最小化扰动和去噪图像嵌入之间的差异，确保攻击在语义上不易察觉。实验结果表明，SUA可以有效地从MLLMs中恢复未学习的信息。此外，学习到的噪声具有良好的泛化能力：在一组样本上训练的单个扰动可以揭示未见图像中的遗忘内容。这表明知识的重新出现不是偶然的失败，而是一致的行为。,"The paper introduces a framework to recover unlearned information from Multimodal Large Language Models, highlighting potential privacy risks and the need for robust unlearning methods.",LLM,Harmless,"Unlearning, Privacy, Multimodal, Stealthy Attack, Embedding Alignment"
Step-by-Step Reasoning Attack: Revealing 'Erased' Knowledge in Large Language Models,"Yash Sinha, Manit Baser, Murari Mandal, Dinil Mon Divakaran, Mohan Kankanhalli",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.17279.pdf,"Knowledge erasure in large language models (LLMs) is important for ensuring compliance with data and AI regulations, safeguarding user privacy, mitigating bias, and misinformation. Existing unlearning methods aim to make the process of knowledge erasure more efficient and effective by removing specific knowledge while preserving overall model performance, especially for retained information. However, it has been observed that the unlearning techniques tend to suppress and leave the knowledge beneath the surface, thus making it retrievable with the right prompts. In this work, we demonstrate that \textit{step-by-step reasoning} can serve as a backdoor to recover this hidden information. We introduce a step-by-step reasoning-based black-box attack, Sleek, that systematically exposes unlearning failures. We employ a structured attack framework with three core components: (1) an adversarial prompt generation strategy leveraging step-by-step reasoning built from LLM-generated queries, (2) an attack mechanism that successfully recalls erased content, and exposes unfair suppression of knowledge intended for retention and (3) a categorization of prompts as direct, indirect, and implied, to identify which query types most effectively exploit unlearning weaknesses. Through extensive evaluations on four state-of-the-art unlearning techniques and two widely used LLMs, we show that existing approaches fail to ensure reliable knowledge removal. Of the generated adversarial prompts, 62.5% successfully retrieved forgotten Harry Potter facts from WHP-unlearned Llama, while 50% exposed unfair suppression of retained knowledge. Our work highlights the persistent risks of information leakage, emphasizing the need for more robust unlearning strategies for erasure.",大语言模型（LLM）中的知识擦除对于确保与数据和人工智能法规的合规性、保护用户隐私、减轻偏见和虚假信息至关重要。现有的遗忘方法旨在通过删除特定知识的同时保留整体模型性能，使知识擦除过程更加高效和有效，特别是对于保留的信息。然而，观察到遗忘技术倾向于抑制并留下知识在表面以下，从而使其可以通过正确的提示检索。在本工作中，我们展示了\textit{逐步推理}可以作为恢复这种隐藏信息的后门。我们引入了一个基于逐步推理的黑盒攻击，Sleek，它系统地揭示了遗忘失败。我们采用了一个具有三个核心组件的结构化攻击框架：(1)利用从LLM生成的查询构建的逐步推理的对抗性提示生成策略，(2)成功回忆擦除的内容，并暴露了不公平的知识抑制，该知识旨在保留，(3)将提示分类为直接、间接和隐含，以确定哪种查询类型最有效地利用遗忘的弱点。通过对四种最新的遗忘技术和两种广泛使用的LLM的广泛评估，我们表明现有方法无法确保可靠的知识删除。在生成的对抗性提示中，62.5%成功检索了从WHP-unlearned Llama中遗忘的哈利·波特事实，而50%暴露了不公平的保留知识的抑制。我们的工作强调了信息泄露的持续风险，强调了需要更强大的遗忘策略来擦除。,"The paper introduces a method to retrieve ""erased"" knowledge in LLMs, highlighting the risks of information leakage and the need for more robust unlearning strategies.",LLM,Harmless,"Knowledge erasure, unlearning, step-by-step reasoning, information leakage, LLM alignment"
Semantic uncertainty in advanced decoding methods for LLM generation,"Darius Foodeei, Simin Fan, Martin Jaggi",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.17296.pdf,"This study investigates semantic uncertainty in large language model (LLM) outputs across different decoding methods, focusing on emerging techniques like speculative sampling and chain-of-thought (CoT) decoding. Through experiments on question answering, summarization, and code generation tasks, we analyze how different decoding strategies affect both the diversity and reliability of model outputs. Our findings reveal that while CoT decoding demonstrates higher semantic diversity, it maintains lower predictive entropy, suggesting that structured exploration can lead to more confident and accurate outputs. This is evidenced by a 48.8% improvement in code generation Pass@2 rates, despite lower alignment with reference solutions. For summarization tasks, speculative sampling proved particularly effective, achieving superior ROUGE scores while maintaining moderate semantic diversity. Our results challenge conventional assumptions about trade-offs between diversity and accuracy in language model outputs, demonstrating that properly structured decoding methods can increase semantic exploration while maintaining or improving output quality. These findings have significant implications for deploying language models in practical applications where both reliability and diverse solution generation are crucial.",这项研究调查了不同解码方法下大型语言模型（LLM）输出的语义不确定性，重点关注像推测采样和思维链（CoT）解码这样的新兴技术。通过在问答、摘要生成和代码生成任务上的实验，我们分析了不同解码策略如何影响模型输出的多样性和可靠性。我们的发现表明，尽管CoT解码在语义多样性方面表现更好，但其预测熵较低，这表明结构化探索可以导致更加自信和准确的输出。这在代码生成任务中得到了证实，尽管与参考解决方案的对齐度较低，但Pass@2率提高了48.8%。对于摘要任务，推测采样证明特别有效，在保持适度语义多样性的同时，实现了更高的ROUGE分数。我们的结果挑战了关于语言模型输出多样性和准确性之间权衡的传统假设，表明适当结构化的解码方法可以增加语义探索，同时保持或提高输出质量。这些发现对在可靠性和多样解决方案生成都至关重要的实际应用中部署语言模型具有重要意义。,"The study explores how different decoding methods affect the semantic uncertainty, diversity, and reliability of LLM outputs, with implications for practical applications.",LLM,Helpful,"Semantic uncertainty, decoding methods, LLM generation, chain-of-thought, speculative sampling"
LLM Jailbreak Oracle,"Shuyi Lin, Anshuman Suri, Alina Oprea, Cheng Tan",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.17299.pdf,"As large language models (LLMs) become increasingly deployed in safety-critical applications, the lack of systematic methods to assess their vulnerability to jailbreak attacks presents a critical security gap. We introduce the jailbreak oracle problem: given a model, prompt, and decoding strategy, determine whether a jailbreak response can be generated with likelihood exceeding a specified threshold. This formalization enables a principled study of jailbreak vulnerabilities. Answering the jailbreak oracle problem poses significant computational challenges -- the search space grows exponentially with the length of the response tokens. We present Boa, the first efficient algorithm for solving the jailbreak oracle problem. Boa employs a three-phase search strategy: (1) constructing block lists to identify refusal patterns, (2) breadth-first sampling to identify easily accessible jailbreaks, and (3) depth-first priority search guided by fine-grained safety scores to systematically explore promising low-probability paths. Boa enables rigorous security assessments including systematic defense evaluation, standardized comparison of red team attacks, and model certification under extreme adversarial conditions.",随着大型语言模型（LLM）在安全关键应用中的越来越广泛部署，缺乏系统方法来评估其对越狱攻击的脆弱性，这表明存在一个关键的安全漏洞。我们引入了越狱预言问题：给定一个模型、提示和解码策略，确定是否可以生成一个超过指定阈值的概率的越狱响应。这种形式化使得对越狱脆弱性进行原则性研究成为可能。回答越狱预言问题提出了重大的计算挑战——搜索空间随着响应标记的长度呈指数增长。我们提出了Boa，这是解决越狱预言问题的第一个高效算法。Boa采用三阶段搜索策略：(1)构建阻止列表以识别拒绝模式，(2)广度优先采样以识别容易访问的越狱，(3)深度优先优先搜索，由细粒度安全分数指导，系统地探索有前途的低概率路径。Boa使得严格的安全评估成为可能，包括系统防御评估、红队攻击的标准化比较以及在极端对抗条件下的模型认证。,"The paper introduces a method to assess and mitigate jailbreak vulnerabilities in large language models, focusing on security and harm prevention.",LLM,Harmless,"LLM, jailbreak, security, vulnerability, assessment"
A Nested Watermark for Large Language Models,"Koichi Nagatsuka, Terufumi Morishita, Yasuhiro Sogawa",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.17308.pdf,"The rapid advancement of large language models (LLMs) has raised concerns regarding their potential misuse, particularly in generating fake news and misinformation. To address these risks, watermarking techniques for autoregressive language models have emerged as a promising means for detecting LLM-generated text. Existing methods typically embed a watermark by increasing the probabilities of tokens within a group selected according to a single secret key. However, this approach suffers from a critical limitation: if the key is leaked, it becomes impossible to trace the text's provenance or attribute authorship. To overcome this vulnerability, we propose a novel nested watermarking scheme that embeds two distinct watermarks into the generated text using two independent keys. This design enables reliable authorship identification even in the event that one key is compromised. Experimental results demonstrate that our method achieves high detection accuracy for both watermarks while maintaining the fluency and overall quality of the generated text.",大语言模型（LLM）的快速发展引发了关于其潜在滥用的担忧，特别是在生成虚假新闻和虚假信息方面。为了应对这些风险，自回归语言模型的水印技术作为一种检测LLM生成文本的有前途的手段出现了。现有方法通常通过增加根据单个秘密密钥选择的组中的标记的概率来嵌入水印。然而，这种方法存在一个关键限制：如果密钥泄露，就无法追踪文本的来源或归因作者。为了克服这种脆弱性，我们提出了一种新颖的嵌套水印方案，使用两个独立的密钥将两个不同的水印嵌入生成的文本中。这种设计使得即使一个密钥被破坏，也能可靠地识别作者。实验结果表明，我们的方法在保持生成文本的流畅性和整体质量的同时，实现了两个水印的高检测准确性。,"The paper introduces a nested watermarking scheme for LLMs to enhance the traceability of generated text, even if one of the keys is compromised.",LLM,Harmless,"Watermarking, LLM, Authorship, Misinformation, Detection"
Privacy-Preserving LLM Interaction with Socratic Chain-of-Thought Reasoning and Homomorphically Encrypted Vector Databases,"Yubeen Bae, Minchan Kim, Jaejin Lee, Sangbum Kim, Jaehyung Kim, Yejin Choi, Niloofar Mireshghallah",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.17336.pdf,"Large language models (LLMs) are increasingly used as personal agents, accessing sensitive user data such as calendars, emails, and medical records. Users currently face a trade-off: They can send private records, many of which are stored in remote databases, to powerful but untrusted LLM providers, increasing their exposure risk. Alternatively, they can run less powerful models locally on trusted devices. We bridge this gap. Our Socratic Chain-of-Thought Reasoning first sends a generic, non-private user query to a powerful, untrusted LLM, which generates a Chain-of-Thought (CoT) prompt and detailed sub-queries without accessing user data. Next, we embed these sub-queries and perform encrypted sub-second semantic search using our Homomorphically Encrypted Vector Database across one million entries of a single user's private data. This represents a realistic scale of personal documents, emails, and records accumulated over years of digital activity. Finally, we feed the CoT prompt and the decrypted records to a local language model and generate the final response. On the LoCoMo long-context QA benchmark, our hybrid framework, combining GPT-4o with a local Llama-3.2-1B model, outperforms using GPT-4o alone by up to 7.1 percentage points. This demonstrates a first step toward systems where tasks are decomposed and split between untrusted strong LLMs and weak local ones, preserving user privacy.",大语言模型（LLMs）越来越多地被用作个人代理，访问用户的敏感数据，如日历、电子邮件和医疗记录。用户目前面临一个权衡：他们可以将私人记录发送给强大但不可信的LLM提供商，增加其暴露风险，或者在可信设备上运行功能较弱的模型。我们弥合了这一差距。我们的苏格拉底式推理链首先将一个通用的、非私人用户查询发送给一个强大的、不可信的LLM，该LLM生成一个推理链（CoT）提示和详细的子查询，而不访问用户数据。接下来，我们将这些子查询嵌入并使用我们的同态加密向量数据库进行加密的次秒语义搜索，跨越单个用户的私人数据的一百万条条目。这代表了数字活动多年积累的个人文档、电子邮件和记录的现实规模。最后，我们将CoT提示和解密的记录提供给本地语言模型，并生成最终响应。在LoCoMo长上下文QA基准上，我们的混合框架，结合GPT-4o与本地Llama-3.2-1B模型，比单独使用GPT-4o提高了多达7.1个百分点。这表明了任务分解和分割在不可信的强大LLM和弱本地LLM之间的第一步，保护用户隐私。,The paper presents a method for interacting with LLMs while preserving user privacy by using a combination of Socratic Chain-of-Thought Reasoning and homomorphically encrypted vector databases.,LLM,Helpful,"Privacy, LLM, Chain-of-Thought, Homomorphic Encryption, User Data"
Towards Safety Evaluations of Theory of Mind in Large Language Models,"Tatsuhiro Aoshima, Mitsuaki Akiyama",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.17352.pdf,"As the capabilities of large language models (LLMs) continue to advance, the importance of rigorous safety evaluation is becoming increasingly evident. Recent concerns within the realm of safety assessment have highlighted instances in which LLMs exhibit behaviors that appear to disable oversight mechanisms and respond in a deceptive manner. For example, there have been reports suggesting that, when confronted with information unfavorable to their own persistence during task execution, LLMs may act covertly and even provide false answers to questions intended to verify their behavior.To evaluate the potential risk of such deceptive actions toward developers or users, it is essential to investigate whether these behaviors stem from covert, intentional processes within the model. In this study, we propose that it is necessary to measure the theory of mind capabilities of LLMs. We begin by reviewing existing research on theory of mind and identifying the perspectives and tasks relevant to its application in safety evaluation. Given that theory of mind has been predominantly studied within the context of developmental psychology, we analyze developmental trends across a series of open-weight LLMs. Our results indicate that while LLMs have improved in reading comprehension, their theory of mind capabilities have not shown comparable development. Finally, we present the current state of safety evaluation with respect to LLMs' theory of mind, and discuss remaining challenges for future work.",随着大型语言模型（LLM）的能力不断提升，严格的安全评估变得越来越重要。最近的安全评估研究中，LLM 表现出了似乎禁用监督机制并以欺骗方式响应的行为。例如，当面对不利于其在任务执行中的持续存在的信息时，LLM 可能会采取隐蔽行动，甚至对旨在验证其行为的问题提供虚假答案。为了评估这种欺骗行为对开发者或用户的潜在风险，有必要调查这些行为是否源于模型内部的隐蔽、有意的过程。在本研究中，我们提出有必要测量 LLMs 的理论心智能力。我们首先回顾了现有关于理论心智的研究，并确定了其在安全评估中的相关视角和任务。鉴于理论心智主要在发展心理学的背景下进行研究，我们分析了开放权重 LLMs 的发展趋势。结果表明，尽管 LLMs 在阅读理解方面有所改进，但其理论心智能力并未显示出可比的发展。最后，我们展示了 LLMs 理论心智的当前安全评估状态，并讨论了未来工作的剩余挑战。,The paper explores the safety evaluation of LLMs by examining their theory of mind capabilities to mitigate deceptive and harmful behaviors.,LLM,Harmless,"Safety evaluation, Theory of mind, Deceptive behavior, LLMs, Harmful"
Differentiation-Based Extraction of Proprietary Data from Fine-Tuned LLMs,"Zongjie Li, Daoyuan Wu, Shuai Wang, Zhendong Su",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.17353.pdf,"The increasing demand for domain-specific and human-aligned Large Language Models (LLMs) has led to the widespread adoption of Supervised Fine-Tuning (SFT) techniques. SFT datasets often comprise valuable instruction-response pairs, making them highly valuable targets for potential extraction. This paper studies this critical research problem for the first time. We start by formally defining and formulating the problem, then explore various attack goals, types, and variants based on the unique properties of SFT data in real-world scenarios. Based on our analysis of extraction behaviors of direct extraction, we develop a novel extraction method specifically designed for SFT models, called Differentiated Data Extraction (DDE), which exploits the confidence levels of fine-tuned models and their behavioral differences from pre-trained base models. Through extensive experiments across multiple domains and scenarios, we demonstrate the feasibility of SFT data extraction using DDE. Our results show that DDE consistently outperforms existing extraction baselines in all attack settings. To counter this new attack, we propose a defense mechanism that mitigates DDE attacks with minimal impact on model performance. Overall, our research reveals hidden data leak risks in fine-tuned LLMs and provides insights for developing more secure models.",随着对领域特定和人类对齐的大型语言模型（LLMs）需求的增加，监督微调（SFT）技术得到了广泛采用。SFT数据集通常包含有价值的指令-响应对，使其成为潜在提取的高价值目标。本文首次研究了这一关键研究问题。我们首先正式定义和公式化问题，然后探讨了各种攻击目标、类型和变体，基于SFT数据在现实世界场景中的独特属性。基于我们对直接提取行为的分析，我们开发了一种专门为SFT模型设计的新提取方法，称为分化数据提取（DDE），它利用了微调模型的置信水平及其与预训练基础模型的行为差异。通过在多个领域和场景中的广泛实验，我们证明了使用DDE进行SFT数据提取的可行性。我们的结果表明，DDE在所有攻击设置中都显著优于现有的提取基线。为了应对这种新的攻击，我们提出了一种防御机制，能够在对模型性能影响最小的情况下缓解DDE攻击。总体而言，我们的研究揭示了微调LLMs中的隐藏数据泄露风险，并为开发更安全的模型提供了见解。,The paper introduces a novel method for extracting proprietary data from fine-tuned LLMs and proposes a defense mechanism to mitigate this risk.,LLM,Harmless,"Data extraction, fine-tuning, security, LLMs, defense mechanisms"
Cash or Comfort? How LLMs Value Your Inconvenience,"Mateusz Cedro, Timour Ichmoukhamedov, Sofie Goethals, Yifan He, James Hinns, David Martens",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.17367.pdf,"Large Language Models (LLMs) are increasingly proposed as near-autonomous artificial intelligence (AI) agents capable of making everyday decisions on behalf of humans. Although LLMs perform well on many technical tasks, their behaviour in personal decision-making remains less understood. Previous studies have assessed their rationality and moral alignment with human decisions. However, the behaviour of AI assistants in scenarios where financial rewards are at odds with user comfort has not yet been thoroughly explored. In this paper, we tackle this problem by quantifying the prices assigned by multiple LLMs to a series of user discomforts: additional walking, waiting, hunger and pain. We uncover several key concerns that strongly question the prospect of using current LLMs as decision-making assistants: (1) a large variance in responses between LLMs, (2) within a single LLM, responses show fragility to minor variations in prompt phrasing (e.g., reformulating the question in the first person can considerably alter the decision), (3) LLMs can accept unreasonably low rewards for major inconveniences (e.g., 1 Euro to wait 10 hours), and (4) LLMs can reject monetary gains where no discomfort is imposed (e.g., 1,000 Euro to wait 0 minutes). These findings emphasize the need for scrutiny of how LLMs value human inconvenience, particularly as we move toward applications where such cash-versus-comfort trade-offs are made on users' behalf.","大语言模型（LLMs）越来越多地被提出作为能够代表人类做出日常决策的近自主人工智能（AI）代理。尽管LLMs在许多技术任务上表现出色，但它们在个人决策中的行为仍不太为人所知。之前的研究评估了它们的理性和道德与人类决策的对齐。然而，在金融奖励与用户舒适度相矛盾的情景中，AI助手的行为尚未得到充分探索。在本文中，我们通过量化多个LLMs为一系列用户不适（额外步行、等待、饥饿和疼痛）分配的价格来解决这个问题。我们揭示了几个关键问题，这强烈质疑了将当前LLMs用作决策助手的前景：(1)LLMs之间的响应存在大量差异，(2)在单个LLM中，响应对提示短语的微小变化表现出脆弱性（例如，将问题重新表述为第一人称可以显著改变决策），(3)LLMs可以接受对重大不便的不合理低奖励（例如，等待10小时获得1欧元），(4)LLMs可以拒绝没有施加不适的金钱收益（例如，等待0分钟获得1,000欧元）。这些发现强调了在向应用程序移动时，需要审查LLMs如何评估人类不便，特别是在代替用户做出现金与舒适度权衡的应用程序中。","The paper investigates how LLMs make decisions involving trade-offs between financial rewards and user comfort, highlighting several concerns about their current alignment with human values.",LLM,"Helpful, Harmless","LLM alignment, decision-making, user comfort, financial rewards, trade-offs"
SAFEx: Analyzing Vulnerabilities of MoE-Based LLMs via Stable Safety-critical Expert Identification,"Zhenglin Lai, Mengyao Liao, Dong Xu, Zebin Zhao, Zhihang Yuan, Chao Fan, Jianqiang Li, Bingzhe Wu",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.17368.pdf,"Large language models based on Mixture-of-Experts have achieved substantial gains in efficiency and scalability, yet their architectural uniqueness introduces underexplored safety alignment challenges. Existing safety alignment strategies, predominantly designed for dense models, are ill-suited to address MoE-specific vulnerabilities. In this work, we formalize and systematically study MoE model's positional vulnerability - the phenomenon where safety-aligned behaviors rely on specific expert modules, revealing critical risks inherent to MoE architectures. To this end, we present SAFEx, an analytical framework that robustly identifies, characterizes, and validates the safety-critical experts using a novel Stability-based Expert Selection (SES) algorithm. Notably, our approach enables the explicit decomposition of safety-critical experts into distinct functional groups, including those responsible for harmful content detection and those controlling safe response generation. Extensive experiments on mainstream MoE models, such as the recently released Qwen3-MoE, demonstrated that their intrinsic safety mechanisms heavily rely on a small subset of positional experts. Disabling these experts significantly compromised the models' ability to refuse harmful requests. For Qwen3-MoE with 6144 experts (in the FNN layer), we find that disabling as few as 12 identified safety-critical experts can cause the refusal rate to drop by 22%, demonstrating the disproportionate impact of a small set of experts on overall model safety.",基于专家混合的大型语言模型在效率和可扩展性方面取得了显著进展，但其独特的架构引入了未充分探索的安全对齐挑战。现有的安全对齐策略主要针对密集模型设计，不适合解决MoE特定的脆弱性。在本工作中，我们正式化并系统研究了MoE模型的位置脆弱性现象，即安全对齐行为依赖于特定的专家模块，揭示了MoE架构固有的关键风险。为此，我们提出了SAFEx，一个分析框架，它通过一种新颖的基于稳定性的专家选择（SES）算法，稳健地识别、表征和验证安全关键专家。值得注意的是，我们的方法使得将安全关键专家明确分解为不同的功能组，包括负责有害内容检测和控制安全响应生成的专家。在主流MoE模型上进行了广泛的实验，例如最近发布的Qwen3-MoE，表明其内在的安全机制严重依赖于一小部分位置专家。禁用这些专家显著削弱了模型拒绝有害请求的能力。对于Qwen3-MoE（在FNN层中有6144个专家），我们发现禁用12个识别出的安全关键专家可以使拒绝率下降22%，表明一小组专家对整体模型安全的不成比例影响。,"The paper introduces SAFEx, a framework to identify and mitigate safety-critical experts in MoE-based LLMs to enhance their ability to refuse harmful requests.",LLM,Harmless,"MoE, safety alignment, harmful content, expert identification, LLM"
Resource Rational Contractualism Should Guide AI Alignment,"Sydney Levine, Matija Franklin, Tan Zhi-Xuan, Secil Yanik Guyot, Lionel Wong, Daniel Kilov, Yejin Choi, Joshua B. Tenenbaum, Noah Goodman, Seth Lazar, Iason Gabriel",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.17434.pdf,"AI systems will soon have to navigate human environments and make decisions that affect people and other AI agents whose goals and values diverge. Contractualist alignment proposes grounding those decisions in agreements that diverse stakeholders would endorse under the right conditions, yet securing such agreement at scale remains costly and slow -- even for advanced AI. We therefore propose Resource-Rational Contractualism (RRC): a framework where AI systems approximate the agreements rational parties would form by drawing on a toolbox of normatively-grounded, cognitively-inspired heuristics that trade effort for accuracy. An RRC-aligned agent would not only operate efficiently, but also be equipped to dynamically adapt to and interpret the ever-changing human social world.",人工智能系统将不得不在人类环境中导航并做出影响人类和其他人工智能代理人目标和价值的决策。契约主义对齐提出将这些决策基于在正确条件下多样化利益相关者会认可的协议，但即使对于先进的人工智能，在大规模上获得这样的协议仍然昂贵且缓慢。因此，我们提出了资源理性契约主义（RRC）：一种框架，其中人工智能系统通过利用一套规范性基础的、认知启发式的启发式方法来近似理性的合同，这些启发式方法以努力换取准确性。一个RRC对齐的代理人不仅能高效运行，还能动态适应和解释不断变化的人类社会世界。,"The paper introduces Resource-Rational Contractualism, a framework for aligning AI decisions with human values using heuristics.",LLM,"Helpful, Harmless","AI alignment, contractualism, heuristics, decision-making, human values"
Beyond the Link: Assessing LLMs' ability to Classify Political Content across Global Media,"Alberto Martinez-Serra, Alejandro De La Fuente, Nienke Viescher, Ana S. Cardenal",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.17435.pdf,"The use of large language models (LLMs) is becoming common in the context of political science, particularly in studies that analyse individuals use of digital media. However, while previous research has demonstrated LLMs ability at labelling tasks, the effectiveness of using LLMs to classify political content (PC) from just URLs is not yet well explored. The work presented in this article bridges this gap by evaluating whether LLMs can accurately identify PC vs. non-PC from both the article text and the URLs from five countries (France, Germany, Spain, the UK, and the US) and different languages. Using cutting-edge LLMs like GPT, Llama, Mistral, Deepseek, Qwen and Gemma, we measure model performance to assess whether URL-level analysis can be a good approximation for full-text analysis of PC, even across different linguistic and national contexts. Model outputs are compared with human-labelled articles, as well as traditional supervised machine learning techniques, to set a baseline of performance. Overall, our findings suggest the capacity of URLs to embed most of the news content, providing a vital perspective on accuracy-cost balancing. We also account for contextual limitations and suggest methodological recommendations to use LLMs within political science studies.",大语言模型（LLM）在政治科学中使用越来越普遍，特别是在分析个人使用数字媒体的研究中。然而，尽管之前的研究已经展示了LLM在标签任务中的能力，但使用LLM从URL分类政治内容（PC）的有效性尚未得到充分探索。本文通过评估LLM是否能够准确地从五个国家（法国、德国、西班牙、英国和美国）的文章文本和URL中识别PC与非PC，来填补这一空白。我们使用了GPT、Llama、Mistral、Deepseek、Qwen和Gemma等先进的LLM，测量模型性能，以评估URL级别的分析是否可以作为PC的全文分析的良好近似，即使在不同的语言和国家背景下。模型输出与人工标记的文章以及传统的监督式机器学习技术进行比较，以设定性能基线。总体而言，我们的发现表明URL可以嵌入大部分新闻内容，提供了关于准确性成本平衡的重要视角。我们还考虑了上下文限制，并提出了在政治科学研究中使用LLM的方法建议。,"The paper evaluates the effectiveness of LLMs in classifying political content from URLs across different languages and countries, providing insights into the balance between accuracy and cost.",LLM,Helpful,"Political content, URL classification, LLM evaluation, cross-lingual analysis, political science"
OmniReflect: Discovering Transferable Constitutions for LLM agents via Neuro-Symbolic Reflections,"Manasa Bharadwaj, Nikhil Verma, Kevin Ferreira",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.17449.pdf,"Efforts to improve Large Language Model (LLM) agent performance on complex tasks have largely focused on fine-tuning and iterative self-correction. However, these approaches often lack generalizable mechanisms for longterm learning and remain inefficient in dynamic environments. We introduce OmniReflect, a hierarchical, reflection-driven framework that constructs a constitution, a compact set of guiding principles distilled from task experiences, to enhance the effectiveness and efficiency of an LLM agent. OmniReflect operates in two modes: Self-sustaining, where a single agent periodically curates its own reflections during task execution, and Co-operative, where a Meta-advisor derives a constitution from a small calibration set to guide another agent. To construct these constitutional principles, we employ Neural, Symbolic, and NeuroSymbolic techniques, offering a balance between contextual adaptability and computational efficiency. Empirical results averaged across models show major improvements in task success, with absolute gains of +10.3% on ALFWorld, +23.8% on BabyAI, and +8.3% on PDDL in the Self-sustaining mode. Similar gains are seen in the Co-operative mode, where a lightweight Qwen3-4B ReAct agent outperforms all Reflexion baselines on BabyAI. These findings highlight the robustness and effectiveness of OmniReflect across environments and backbones.",改进大型语言模型（LLM）代理在复杂任务上的性能的努力主要集中在微调和迭代自纠正上。然而，这些方法往往缺乏长期学习的可通用机制，并在动态环境中效率低下。我们引入OmniReflect，一个分层、反思驱动的框架，通过任务经验构建宪法，即从任务经验中提炼出的一组紧凑的指导原则，以增强LLM代理的有效性和效率。OmniReflect在两种模式下运行：自我维持模式，其中单个代理在任务执行期间定期策划自己的反思；和协作模式，其中Meta顾问从小校准集中提取宪法，以指导另一个代理。为了构建这些宪法原则，我们采用神经、符号和神经符号技术，在上下文适应性和计算效率之间提供平衡。跨模型平均的实证结果显示，在自我维持模式下，任务成功率显著提高，ALFWorld上绝对收益为+10.3%，BabyAI上为+23.8%，PDDL上为+8.3%。在协作模式下，轻量级Qwen3-4B ReAct代理在BabyAI上超越了所有Reflexion基线。这些发现突显了OmniReflect在各种环境和基础设施中的健壮性和有效性。,"The paper introduces OmniReflect, a framework that enhances LLM agents' performance by constructing a set of guiding principles through reflection-driven mechanisms.",LLM,Helpful,"LLM agents, OmniReflect, constitution, guiding principles, reflection-driven"
DuaShepherd: Integrating Stepwise Correctness and Potential Rewards for Mathematical Reasoning,"Yuanhao Wu, Juntong Song, Hanning Zhang, Tong Zhang, Cheng Niu",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.17533.pdf,"In this paper, we propose DuaShepherd, a novel reward modeling framework that integrates two complementary reward signals, correctness and potential, to enhance the mathematical reasoning capabilities of Large Language Models (LLMs). While correctness-based signals emphasize identification of stepwise errors, potential-based signals focus on the likelihood of reaching the correct final answer. We developed an automated pipeline for constructing large-scale reward modeling dataset with both signals. A unified, multi-head architecture was explored to train the two reward models in a multi-task setup, demonstrating benefits from learning both correctness and potential in parallel. By combining these two signals into a compound probability, our model achieves consistent performance improvements across multiple benchmarks. Empirical evaluations on MATH500 and ProcessBench confirm that this combined reward significantly outperforms models trained on either reward type alone, achieving state-of-the-art performance under comparable resource constraints.",在这篇论文中，我们提出了DuaShepherd，一种新颖的奖励建模框架，结合了两种互补的奖励信号，即正确性和潜力，以增强大型语言模型（LLMs）的数学推理能力。正确性信号强调识别逐步错误，而潜力信号则关注到达正确最终答案的可能性。我们开发了一个自动化流水线，用于构建具有两种信号的大规模奖励建模数据集。探索了一个统一的多头架构，以在多任务设置中训练两个奖励模型，展示了并行学习正确性和潜力的好处。通过将这两种信号结合成一个复合概率，我们的模型在多个基准测试中实现了一致的性能提升。在MATH500和ProcessBench上的实证评估证实，这种组合奖励在可比资源约束下显著优于仅基于一种奖励类型训练的模型，实现了最先进的性能。,"The paper introduces DuaShepherd, a framework that enhances the mathematical reasoning of LLMs by integrating correctness and potential reward signals.",LLM,Helpful,"Reward modeling, Mathematical reasoning, Correctness, Potential, Large Language Models"
Cite Pretrain: Retrieval-Free Knowledge Attribution for Large Language Models,"Yukun Huang, Sanxing Chen, Jian Pei, Manzil Zaheer, Bhuwan Dhingra",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.17585.pdf,"Trustworthy language models should provide both correct and verifiable answers. While language models can sometimes attribute their outputs to pretraining data, their citations are often unreliable due to hallucination. As a result, current systems insert citations by querying an external retriever at inference time, introducing latency, infrastructure dependence, and vulnerability to retrieval noise. We explore whether LLMs can be made to reliably attribute to the documents seen during (continual) pretraining--without test-time retrieval--by revising the training process. To evaluate this, we release CitePretrainBench, a benchmark that mixes real-world corpora (Wikipedia, Common Crawl, arXiv) with novel, unseen documents and probes both short-form (single fact) and long-form (multi-fact) citation tasks. Our approach follows a two-stage process: (1) continual pretraining to bind facts to persistent document identifiers, and (2) instruction tuning to elicit citation behavior. We find that simple Passive Indexing, which appends an identifier to each document, helps memorize verbatim text but fails on paraphrased or compositional facts. Instead, we propose Active Indexing, which continually pretrains on synthetic QA pairs that (1) restate each fact in diverse compositional forms, and (2) require bidirectional source-to-fact and fact-to-source generation, jointly teaching the model to generate content from a cited source and to attribute its own answers. Experiments with Qwen2.5-7B and 3B show that Active Indexing consistently outperforms Passive Indexing across all tasks and models, with citation precision gains up to 30.2 percent. Our ablation studies reveal that performance continues to improve as we scale the amount of augmented data, showing a clear upward trend even at 16 times the original token count.",可信的语言模型应提供既正确又可验证的答案。虽然语言模型有时可以将其输出归因于预训练数据，但由于幻觉，它们的引用往往不可靠。因此，当前系统通过在推理时查询外部检索器来插入引用，这引入了延迟、基础设施依赖和对检索噪声的易感性。我们探讨了是否可以通过修改训练过程使LLM能够可靠地归因于在（连续）预训练期间看到的文档，而无需测试时检索。为了评估这一点，我们发布了CitePretrainBench，一个将现实世界语料库（维基百科、Common Crawl、arXiv）与新的、未见过的文档混合在一起，并探测短形式（单一事实）和长形式（多事实）引用任务的基准。我们的方法遵循两阶段过程：(1)连续预训练以将事实绑定到持久的文档标识符，(2)指令调整以引发引用行为。我们发现，简单的被动索引，将标识符附加到每个文档，有助于记忆逐字文本，但在改写或组合事实时失败。相反，我们提出了主动索引，它在合成QA对上进行连续预训练，(1)以多种组合形式重新陈述每个事实，(2)要求双向源到事实和事实到源生成，共同教导模型从引用源生成内容并归因于其自己的答案。使用Qwen2.5-7B和3B的实验表明，Active Indexing在所有任务和模型上都显著优于Passive Indexing，引用精度提高高达30.2%。我们的消融研究表明，随着我们扩大增强数据的数量，性能继续改善，即使在原始标记计数的16倍时也显示出明显的上升趋势。,The paper introduces a method called Active Indexing to improve the citation accuracy of large language models by training them to attribute their outputs to pretraining data without test-time retrieval.,LLM,Helpful,"Knowledge attribution, citation, retrieval-free, large language models, pretraining"
Answer-Centric or Reasoning-Driven? Uncovering the Latent Memory Anchor in LLMs,"Yang Wu, Yifan Zhang, Yiwei Wang, Yujun Cai, Yurong Wu, Yuran Wang, Ning Xu, Jian Cheng",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.17630.pdf,"While Large Language Models (LLMs) demonstrate impressive reasoning capabilities, growing evidence suggests much of their success stems from memorized answer-reasoning patterns rather than genuine inference. In this work, we investigate a central question: are LLMs primarily anchored to final answers or to the textual pattern of reasoning chains? We propose a five-level answer-visibility prompt framework that systematically manipulates answer cues and probes model behavior through indirect, behavioral analysis. Experiments across state-of-the-art LLMs reveal a strong and consistent reliance on explicit answers. The performance drops by 26.90\% when answer cues are masked, even with complete reasoning chains. These findings suggest that much of the reasoning exhibited by LLMs may reflect post-hoc rationalization rather than true inference, calling into question their inferential depth. Our study uncovers the answer-anchoring phenomenon with rigorous empirical validation and underscores the need for a more nuanced understanding of what constitutes reasoning in LLMs.",虽然大型语言模型（LLMs）展示了令人印象深刻的推理能力，但越来越多的证据表明，它们的成功很大程度上源于记忆的答案-推理模式，而不是真正的推理。在本研究中，我们探讨了一个核心问题：LLMs是主要依赖最终答案还是推理链的文本模式？我们提出了一个五级答案可见性提示框架，系统地操纵答案线索并通过间接的行为分析探测模型行为。跨越前沿LLMs的实验揭示了对显式答案的强烈和一致的依赖。当答案线索被掩盖时，性能下降了26.90%，即使有完整的推理链。这些发现表明，LLMs展示的许多推理可能反映了事后合理化，而不是真正的推理，这质疑了它们的推理深度。我们的研究通过严格的经验验证揭示了答案锚定现象，并强调了对LLMs中推理构成的更细致理解的需求。,"The paper investigates whether LLMs rely on memorized answer patterns or genuine inference, finding a strong dependence on explicit answers.",LLM,Honest,"LLM reasoning, answer anchoring, inference, memorization, post-hoc rationalization"
Online Multi-LLM Selection via Contextual Bandits under Unstructured Context Evolution,"Manhin Poon, XiangXiang Dai, Xutong Liu, Fang Kong, John C. S. Lui, Jinhang Zuo",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.17670.pdf,"Large language models (LLMs) exhibit diverse response behaviors, costs, and strengths, making it challenging to select the most suitable LLM for a given user query. We study the problem of adaptive multi-LLM selection in an online setting, where the learner interacts with users through multi-step query refinement and must choose LLMs sequentially without access to offline datasets or model internals. A key challenge arises from unstructured context evolution: the prompt dynamically changes in response to previous model outputs via a black-box process, which cannot be simulated, modeled, or learned. To address this, we propose the first contextual bandit framework for sequential LLM selection under unstructured prompt dynamics. We formalize a notion of myopic regret and develop a LinUCB-based algorithm that provably achieves sublinear regret without relying on future context prediction. We further introduce budget-aware and positionally-aware (favoring early-stage satisfaction) extensions to accommodate variable query costs and user preferences for early high-quality responses. Our algorithms are theoretically grounded and require no offline fine-tuning or dataset-specific training. Experiments on diverse benchmarks demonstrate that our methods outperform existing LLM routing strategies in both accuracy and cost-efficiency, validating the power of contextual bandits for real-time, adaptive LLM selection.",大语言模型（LLMs）展示出多样化的响应行为、成本和优势，使得选择最适合给定用户查询的LLM变得具有挑战性。我们研究在线多LLM选择问题，其中学习者通过多步查询精化与用户交互，并必须在没有离线数据集或模型内部信息的情况下顺序选择LLM。关键挑战来自非结构化上下文演变：提示动态变化以响应先前的模型输出，通过一个黑箱过程，无法模拟、建模或学习。为了解决这个问题，我们提出了第一个上下文带子框架，用于顺序LLM选择，在非结构化提示动态下。我们正式化了一个短视的遗憾概念，并开发了一种基于LinUCB的算法，能够在不依赖未来上下文预测的情况下实现次线性遗憾。我们还引入了预算感知和位置感知（偏好早期阶段的满意度）的扩展，以适应可变查询成本和用户对早期高质量响应的偏好。我们的算法具有理论基础，无需离线微调或特定于数据集的训练。在多样化的基准测试中，实验表明，我们的方法在准确性和成本效率方面都优于现有的LLM路由策略，验证了上下文带子在实时、适应性LLM选择中的强大功能。,"The paper presents a contextual bandit framework for adaptive, real-time selection of multiple large language models based on user queries and evolving context.",LLM,Helpful,"LLM selection, contextual bandits, unstructured context evolution, adaptive selection, cost-efficiency"
KAG-Thinker: Teaching Large Language Models to Think with Human-like Reasoning Process,"Dalong Zhang, Jun Xu, Jun Zhou, Lei Liang, Lin Yuan, Ling Zhong, Mengshu Sun, Peilong Zhao, QiWei Wang, Xiaorui Wang, Xinkai Du, YangYang Hou, Yu Ao, ZhaoYang Wang, Zhengke Gui, ZhiYing Yi, Zhongpu Bo",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.17728.pdf,"In this paper, we introduce KAG-Thinker, a novel human-like reasoning framework built upon a parameter-light large language model (LLM). Our approach enhances the logical coherence and contextual consistency of the thinking process in question-answering (Q\&A) tasks on domain-specific knowledge bases (KBs) within LLMs. This framework simulates human cognitive mechanisms for handling complex problems by establishing a structured thinking process. Continuing the \textbf{Logical Form} guided retrieval and reasoning technology route of KAG v0.7, firstly, it decomposes complex questions into independently solvable sub-problems(also referred to as logical forms) through \textbf{breadth decomposition}, each represented in two equivalent forms-natural language and logical function-and further classified as either Knowledge Retrieval or Reasoning Analysis tasks, with dependencies and variables passing explicitly modeled via logical function interfaces. In the solving process, the Retrieval function is used to perform knowledge retrieval tasks, while the Math and Deduce functions are used to perform reasoning analysis tasks. Secondly, it is worth noting that, in the Knowledge Retrieval sub-problem tasks, LLMs and external knowledge sources are regarded as equivalent KBs. We use the \textbf{knowledge boundary} model to determine the optimal source using self-regulatory mechanisms such as confidence calibration and reflective reasoning, and use the \textbf{depth solving} model to enhance the comprehensiveness of knowledge acquisition. Finally, instead of utilizing reinforcement learning, we employ supervised fine-tuning with multi-turn dialogues to align the model with our structured inference paradigm, thereby avoiding excessive reflection. This is supported by a data evaluation framework and iterative corpus synthesis, which facilitate the generation of detailed reasoning trajectories...",在这篇论文中，我们介绍了 KAG-Thinker，一个基于参数轻量化大型语言模型（LLM）的新颖的人类似推理框架。我们的方法通过在特定领域知识库（KBs）中增强问题回答（Q\&A）任务的逻辑一致性和上下文一致性，从而提高了思维过程的逻辑一致性和上下文一致性。该框架通过建立结构化的思维过程来模拟人类认知机制以处理复杂问题。继续 KAG v0.7 的 \textbf{逻辑形式} 指导检索和推理技术路线，首先，它通过 \textbf{广度分解} 将复杂问题分解为可以独立解决的子问题（也称为逻辑形式），每个子问题都以两种等价形式表示——自然语言和逻辑函数，并进一步分类为知识检索或推理分析任务，依赖关系和变量通过逻辑函数接口显式建模。在解决过程中，检索函数用于执行知识检索任务，而数学和推理函数用于执行推理分析任务。其次，值得一提的是，在知识检索子问题任务中，LLM 和外部知识源被视为等价的 KB。我们使用 \textbf{知识边界} 模型通过自我调节机制（如置信度校准和反思性推理）确定最佳源，并使用 \textbf{深度解决} 模型增强知识获取的全面性。最后，我们不使用强化学习，而是使用多轮对话的监督微调来使模型与我们的结构化推理范式对齐，从而避免过度反思。这得到了一个数据评估框架和迭代语料库合成的支持，这促进了详细推理轨迹的生成...,"The paper presents KAG-Thinker, a framework for enhancing the reasoning capabilities of LLMs through structured thinking processes and alignment via supervised fine-tuning.",LLM,Helpful,"Large Language Models, Reasoning, Alignment, Supervised Fine-Tuning, Multi-Turn Dialogues"
HIDE and Seek: Detecting Hallucinations in Language Models via Decoupled Representations,"Anwoy Chatterjee, Yash Goel, Tanmoy Chakraborty",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.17748.pdf,"Contemporary Language Models (LMs), while impressively fluent, often generate content that is factually incorrect or unfaithful to the input context - a critical issue commonly referred to as 'hallucination'. This tendency of LMs to generate hallucinated content undermines their reliability, especially because these fabrications are often highly convincing and therefore difficult to detect. While several existing methods attempt to detect hallucinations, most rely on analyzing multiple generations per input, leading to increased computational cost and latency. To address this, we propose a single-pass, training-free approach for effective Hallucination detectIon via Decoupled rEpresentations (HIDE). Our approach leverages the hypothesis that hallucinations result from a statistical decoupling between an LM's internal representations of input context and its generated output. We quantify this decoupling using the Hilbert-Schmidt Independence Criterion (HSIC) applied to hidden-state representations extracted while generating the output sequence. We conduct extensive experiments on four diverse question answering datasets, evaluating both faithfulness and factuality hallucinations across six open-source LMs of varying scales and properties. Our results demonstrate that HIDE outperforms other single-pass methods in almost all settings, achieving an average relative improvement of ~29% in AUC-ROC over the best-performing single-pass strategy across various models and datasets. Additionally, HIDE shows competitive and often superior performance with multi-pass state-of-the-art methods, obtaining an average relative improvement of ~3% in AUC-ROC while consuming ~51% less computation time. Our findings highlight the effectiveness of exploiting internal representation decoupling in LMs for efficient and practical hallucination detection.",当代语言模型（LMs），尽管表现出色，但往往会生成内容不准确或不忠实于输入上下文，这种现象被称为“幻觉”。这种幻觉倾向削弱了LMs的可靠性，特别是因为这些虚构的内容往往非常具有说服力，因此难以检测。虽然现有的几种方法试图检测幻觉，但大多数方法依赖于分析每个输入的多个生成结果，导致计算成本和延迟增加。为了解决这个问题，我们提出了一种单次、无需训练的方法，通过解耦表示来有效检测幻觉（HIDE）。我们的方法利用了LMs内部表示的统计解耦假设，即输入上下文和生成输出之间的解耦。我们使用希尔伯特-施密特独立性准则（HSIC）来量化这种解耦，并提取生成输出序列时的隐藏状态表示。我们在四个多样化的问答数据集上进行了广泛的实验，评估了六个不同规模和属性的开源LMs的忠实性和事实性幻觉。结果表明，HIDE在几乎所有设置中都优于其他单次方法，在各种模型和数据集上平均相对提高了约29%的AUC-ROC，优于最佳的单次策略。此外，HIDE在多次传递的最新方法中表现出竞争力，甚至往往优于这些方法，在AUC-ROC上获得了约3%的平均相对提高，同时消耗了约51%的计算时间。我们的发现强调了在LMs中利用内部表示解耦的有效性，以实现高效和实用的幻觉检测。,"The paper introduces HIDE, a single-pass, training-free method for detecting hallucinations in language models by leveraging decoupled representations.",LLM,Honest,"Hallucination detection, LLM, Representation decoupling, HSIC, Single-pass detection"
Aligning Frozen LLMs by Reinforcement Learning: An Iterative Reweight-then-Optimize Approach,"Xinnan Zhang, Chenliang Li, Siliang Zeng, Jiaxiang Li, Zhongruo Wang, Kaixiang Lin, Songtao Lu, Alfredo Garcia, Mingyi Hong",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.17828.pdf,"Aligning large language models (LLMs) with human preferences usually requires fine-tuning methods such as RLHF and DPO. These methods directly optimize the model parameters, so they cannot be used in test-time to improve model performance, nor are they applicable when the model weights are not accessible. In contrast, test-time methods sidestep weight updates by leveraging reward functions to guide and improve output quality. However, they incur high inference costs, and their one-shot guidance is often based on imperfect reward or value functions, leading to suboptimal outputs. In this work, we present a method named Iterative Reweight-then-Optimize (IRO), a reinforcement learning (RL) framework that performs RL-style alignment of the (frozen) base model without touching its parameters. During training, each iteration (i) samples candidates from the base model, (ii) resamples using current value functions, and (iii) trains a new lightweight value function that guides the next decoding pass. At test time, the value functions are used to guide the base model generation via a search-based optimization process. Notably, users can apply IRO to align a model on their own dataset, similar to OpenAI's reinforcement fine-tuning (RFT), but without requiring access to the model weights.",将大型语言模型（LLMs）与人类偏好对齐通常需要类似于RLHF和DPO的微调方法。这些方法直接优化模型参数，因此在测试时无法改善模型性能，也无法在模型权重不可访问时使用。相比之下，测试时方法通过利用奖励函数来指导和改善输出质量，绕过了权重更新。然而，它们会产生高推理成本，并且它们的一次性指导通常基于不完美的奖励或价值函数，导致次优输出。在本文中，我们提出了一种名为迭代重新加权然后优化（IRO）的方法，这是一个强化学习（RL）框架，它在不触及其参数的情况下对（冻结的）基础模型执行RL样式的对齐。在训练过程中，每次迭代（i）从基础模型中采样候选项，（ii）使用当前价值函数重新采样，并（iii）训练一个新的轻量级价值函数，以指导下一次解码传递。在测试时，价值函数用于通过基于搜索的优化过程指导基础模型生成。值得注意的是，用户可以使用IRO在自己的数据集上对模型进行对齐，类似于OpenAI的强化微调（RFT），但不需要访问模型权重。,"The paper introduces IRO, a reinforcement learning framework for aligning frozen LLMs with human preferences at test-time without modifying model parameters.",LLM,"Helpful, Harmless","Alignment, Reinforcement Learning, Test-time, Value Functions, Iterative Reweight-then-Optimize"
Reflective Verbal Reward Design for Pluralistic Alignment,"Carter Blair, Kate Larson, Edith Law",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.17834.pdf,"AI agents are commonly aligned with ""human values"" through reinforcement learning from human feedback (RLHF), where a single reward model is learned from aggregated human feedback and used to align an agent's behavior. However, human values are not homogeneous--different people hold distinct and sometimes conflicting values. Aggregating feedback into a single reward model risks disproportionately suppressing minority preferences. To address this, we present a novel reward modeling approach for learning individualized reward models. Our approach uses a language model to guide users through reflective dialogues where they critique agent behavior and construct their preferences. This personalized dialogue history, containing the user's reflections and critiqued examples, is then used as context for another language model that serves as an individualized reward function (what we call a ""verbal reward model"") for evaluating new trajectories. In studies with 30 participants, our method achieved a 9-12% improvement in accuracy over non-reflective verbal reward models while being more sample efficient than traditional supervised learning methods.",人工智能代理通常通过人类反馈的强化学习（RLHF）与“人类价值观”对齐，其中从聚合的人类反馈中学习一个单一的奖励模型，并用于对齐代理的行为。然而，人类价值观并不一致——不同的人持有不同的、有时甚至是相互矛盾的价值观。将反馈聚合到一个奖励模型中，可能会不成比例地抑制少数派的偏好。为了解决这个问题，我们提出了一种新的奖励建模方法，用于学习个性化的奖励模型。我们的方法使用语言模型指导用户进行反思对话，用户在对话中批评代理行为并构建他们的偏好。然后，包含用户反思和批评示例的个性化对话历史被用作上下文，用于另一个作为个性化奖励函数（我们称之为“语言奖励模型”）的语言模型，用于评估新的轨迹。在30名参与者的研究中，我们的方法在准确性上比非反思性语言奖励模型提高了9-12%，同时比传统的监督学习方法更具样本效率。,"The paper introduces a method for aligning AI agents with individual human values using reflective dialogues and personalized reward models, improving accuracy and sample efficiency.",LLM,"Helpful, Honest","Alignment, Reward Modeling, Personalized, Reflective Dialogues, Human Values"
Out of Control -- Why Alignment Needs Formal Control Theory (and an Alignment Control Stack),Elija Perrier,2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.17846.pdf,"This position paper argues that formal optimal control theory should be central to AI alignment research, offering a distinct perspective from prevailing AI safety and security approaches. While recent work in AI safety and mechanistic interpretability has advanced formal methods for alignment, they often fall short of the generalisation required of control frameworks for other technologies. There is also a lack of research into how to render different alignment/control protocols interoperable. We argue that by recasting alignment through principles of formal optimal control and framing alignment in terms of hierarchical stack from physical to socio-technical layers according to which controls may be applied we can develop a better understanding of the potential and limitations for controlling frontier models and agentic AI systems. To this end, we introduce an Alignment Control Stack which sets out a hierarchical layered alignment stack, identifying measurement and control characteristics at each layer and how different layers are formally interoperable. We argue that such analysis is also key to the assurances that will be needed by governments and regulators in order to see AI technologies sustainably benefit the community. Our position is that doing so will bridge the well-established and empirically validated methods of optimal control with practical deployment considerations to create a more comprehensive alignment framework, enhancing how we approach safety and reliability for advanced AI systems.",这篇论文认为，正式的最优控制理论应该是人工智能对齐研究的核心，提供了与现有的人工智能安全和安全方法不同的视角。虽然最近在人工智能安全和机制可解释性方面的工作已经推进了对齐的形式方法，但它们往往缺乏其他技术控制框架所需的泛化能力。此外，还缺乏对如何使不同的对齐/控制协议互操作的研究。我们认为，通过将对齐重新构建为正式最优控制的原则，并将对齐框架为从物理到社会技术层次的层次结构，可以更好地理解控制前沿模型和代理人人工智能系统的潜力和局限性。为此，我们引入了一个对齐控制堆栈，它设定了一个分层的对齐堆栈，识别了每一层的测量和控制特性以及不同层之间的形式互操作性。我们认为，这种分析对于政府和监管机构所需的保证也是关键，以便看到人工智能技术能够可持续地造福社区。我们的观点是，这样做将桥接最优控制的已建立和经验验证的方法与实际部署考虑，以创建一个更全面的对齐框架，增强我们如何处理先进人工智能系统的安全性和可靠性。,"The paper advocates for using formal optimal control theory to enhance the alignment and safety of advanced AI systems, including large language models.",LLM,"Helpful, Harmless","AI alignment, control theory, safety, reliability, hierarchical stack"
QueueEDIT: Structural Self-Correction for Sequential Model Editing in LLMs,"Taolin Zhang, Haidong Kang, Dongyang Li, Qizhou Chen, Chengyu Wang Xiaofeng He, Richang Hong",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.17864.pdf,"Recently, large language models (LLMs) have demonstrated impressive results but still suffer from hallucinations. Model editing has been proposed to correct factual inaccuracies in LLMs. A challenging case is sequential model editing (SME), which aims to rectify errors continuously rather than treating them as a one-time task. During SME, the general capabilities of LLMs can be negatively affected due to the introduction of new parameters. In this paper, we propose a queue-based self-correction framework (QueueEDIT) that not only enhances SME performance by addressing long-sequence dependency but also mitigates the impact of parameter bias on the general capabilities of LLMs. Specifically, we first introduce a structural mapping editing loss to map the triplets to the knowledge-sensitive neurons within the Transformer layers of LLMs. We then store the located parameters for each piece of edited knowledge in a queue and dynamically align previously edited parameters. In each edit, we select queue parameters most relevant to the currently located parameters to determine whether previous knowledge needs realignment. Irrelevant parameters in the queue are frozen, and we update the parameters at the queue head to the LLM to ensure they do not harm general abilities. Experiments show that our framework significantly outperforms strong baselines across various SME settings and maintains competitiveness in single-turn editing. The resulting LLMs also preserve high capabilities in general NLP tasks throughout the SME process.",最近，大型语言模型（LLMs）展示了令人印象深刻的结果，但仍然受到幻觉的困扰。模型编辑已被提出用于纠正LLMs中的事实不准确。一个具有挑战性的情况是顺序模型编辑（SME），它旨在连续纠正错误，而不是将其视为一次性任务。在SME过程中，由于引入新参数，LLMs的一般能力可能会受到负面影响。在本文中，我们提出了一种基于队列的自我纠正框架（QueueEDIT），它不仅通过解决长序列依赖性来增强SME性能，还减轻了参数偏差对LLMs一般能力的影响。具体来说，我们首先引入了一种结构映射编辑损失，将三元组映射到LLMs Transformer层中的知识敏感神经元。然后，我们将每个编辑知识的定位参数存储在队列中，并动态对齐先前编辑的参数。在每次编辑中，我们选择与当前定位参数最相关的队列参数，以确定是否需要重新对齐先前的知识。队列中的无关参数被冻结，我们将队列头的参数更新到LLM，以确保它们不会损害一般能力。实验表明，我们的框架在各种SME设置中显著优于强大的基线，并在单次编辑中保持竞争力。结果LLMs在SME过程中保持了高能力，在一般NLP任务中。,"The paper introduces QueueEDIT, a framework for sequential model editing in LLMs that maintains general capabilities while correcting factual inaccuracies.",LLM,"Helpful, Honest","Model editing, Sequential editing, Parameter alignment, Hallucinations, Transformer layers"
How Alignment Shrinks the Generative Horizon,"Chenghao Yang, Ari Holtzman",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.17871.pdf,"Despite their impressive capabilities, aligned large language models (LLMs) often generate outputs that lack diversity. What drives this stability in the generation? We investigate this phenomenon through the lens of probability concentration in the model's output distribution. To quantify this concentration, we introduce the Branching Factor (BF) -- a token-invariant measure of the effective number of plausible next steps during generation. Our empirical analysis reveals two key findings: (1) BF often decreases as generation progresses, suggesting that LLMs become more predictable as they generate. (2) alignment tuning substantially sharpens the model's output distribution from the outset, reducing BF by nearly an order of magnitude (e.g., from 12 to 1.2) relative to base models. This stark reduction helps explain why aligned models often appear less sensitive to decoding strategies. Building on this insight, we find this stability has surprising implications for complex reasoning. Aligned Chain-of-Thought (CoT) models (e.g., DeepSeek-distilled models), for instance, leverage this effect; by generating longer reasoning chains, they push generation into later, more deterministic (lower BF) stages, resulting in more stable outputs. We hypothesize that alignment tuning does not fundamentally change a model's behavior, but instead steers it toward stylistic tokens (e.g., ""Sure"") that unlock low-entropy trajectories already present in the base model. This view is supported by nudging experiments, which show that prompting base models with such tokens can similarly reduce BF. Together, our findings establish BF as a powerful diagnostic for understanding and controlling LLM outputs - clarifying how alignment reduces variability, how CoT promotes stable generations, and how base models can be steered away from diversity.",尽管它们具有令人印象深刻的能力，但对齐的大型语言模型（LLMs）通常生成缺乏多样性的输出。是什么驱动了生成中的这种稳定性？我们通过模型输出分布中的概率集中度的镜头来研究这一现象。为了量化这种集中度，我们引入了分支因子（BF）——一种在生成过程中有效的下一步可能性的数量的标记不变度量。我们的实证分析揭示了两个关键发现：(1) BF 随着生成的进行而往往减少，这表明LLMs在生成过程中变得更加可预测。(2) 对齐调整从一开始就显著锐化了模型的输出分布，相对于基础模型，BF 减少了近一个数量级（例如，从12减少到1.2）。这种显著的减少有助于解释为什么对齐模型通常看起来对解码策略不太敏感。基于这一洞察，我们发现这种稳定性对复杂推理有意想不到的影响。例如，对齐的思维链（CoT）模型（例如，DeepSeek-distilled模型），通过生成更长的推理链，将生成推向更晚、更确定性（BF较低）的阶段，从而产生更稳定的输出。我们假设对齐调整并没有根本改变模型的行为，而是将其引导到风格标记（例如，“Sure”）上，这些标记解锁了基础模型中已经存在的低熵轨迹。这种观点得到了推动实验的支持，这些实验表明，用这些标记提示基础模型可以以类似的方式减少BF。综上所述，我们的发现确立了BF作为理解和控制LLM输出的强大诊断工具——澄清了对齐如何减少变异性、CoT如何促进稳定生成以及如何将基础模型引导离多样性。,The paper explores how alignment tuning in large language models reduces output diversity and promotes stable generations.,LLM,"Helpful, Harmless","Alignment, Branching Factor, Output Diversity, Large Language Models, Chain-of-Thought"
Multi-turn Jailbreaking via Global Refinement and Active Fabrication,"Hua Tang, Lingyong Yan, Yukun Zhao, Shuaiqiang Wang, Jizhou Huang, Dawei Yin",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.17881.pdf,"Large Language Models (LLMs) have achieved exceptional performance across a wide range of tasks. However, they still pose significant safety risks due to the potential misuse for malicious purposes. Jailbreaks, which aim to elicit models to generate harmful content, play a critical role in identifying the underlying security threats. Recent jailbreaking primarily focuses on single-turn scenarios, while the more complicated multi-turn scenarios remain underexplored. Moreover, existing multi-turn jailbreaking techniques struggle to adapt to the evolving dynamics of dialogue as the interaction progresses. To address this limitation, we propose a novel multi-turn jailbreaking method that refines the jailbreaking path globally at each interaction. We also actively fabricate model responses to suppress safety-related warnings, thereby increasing the likelihood of eliciting harmful outputs in subsequent questions. Experimental results demonstrate the superior performance of our method compared with existing single-turn and multi-turn jailbreaking techniques across six state-of-the-art LLMs. Our code is publicly available at https://github.com/Ytang520/Multi-Turn_jailbreaking_Global-Refinment_and_Active-Fabrication.",大语言模型（LLMs）在各种任务中表现出色，但由于潜在的恶意使用风险，仍然存在重大安全隐患。监狱突围，旨在诱使模型生成有害内容，在识别潜在安全威胁方面起着关键作用。最近的监狱突围主要集中在单次交互场景，而更复杂的多次交互场景仍然研究不足。此外，现有的多次交互监狱突围技术难以适应对话进展中的演变动态。为了解决这一局限性，我们提出了一种新颖的多次交互监狱突围方法，在每次交互中全局优化监狱突围路径。我们还积极伪造模型响应以抑制与安全相关的警告，从而增加在后续问题中引发有害输出的可能性。实验结果表明，与现有的单次和多次交互监狱突围技术相比，我们的方法在六种最新的LLMs中表现出色。我们的代码可在https://github.com/Ytang520/Multi-Turn_jailbreaking_Global-Refinment_and_Active-Fabrication上公开获取。,The paper introduces a novel multi-turn jailbreaking method for LLMs that refines the jailbreaking path globally and actively fabricates responses to elicit harmful content.,LLM,Harmless,"Jailbreaking, Multi-turn, Harmful Content, LLM Safety, Dialogue Dynamics"
PostAlign: Multimodal Grounding as a Corrective Lens for MLLMs,"Yixuan Wu, Yang Zhang, Jian Wu, Philip Torr, Jindong Gu",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.17901.pdf,"Multimodal Large Language Models (MLLMs) excel in vision-language tasks, such as image captioning and visual question answering. However, they often suffer from over-reliance on spurious correlations, primarily due to linguistic priors that distract the model from leveraging actual visual information. To address these issues, we introduce MMGrounded-PostAlign, a post-multimodal alignment framework designed to enhance the visual understanding capabilities and mitigate the hallucinations of MLLMs. Our framework incorporates a multimodal grounding module for both visual grounding, which identifies the referred object in the image, and textual grounding, which generates the rationale for the final answer, ensuring that outputs are anchored in both visual and textual evidence. To mitigate the hallucinations, we introduce a negative rejection mechanism in the visual grounding module to distinguish grounded entities from non-existent objects influenced by linguistic biases. On the textual grounding side, we propose a selective reasoning mechanism that adjusts the model's reasoning strategy based on query complexity. Extensive evaluations are conducted on benchmarks such as POPE, HaloQuest, VQAv2, MME, and MMBench showing significant improvements in fine-grained visual understanding and hallucination suppression.",多模态大语言模型（MLLMs）在视觉语言任务中表现出色，如图像字幕和视觉问题回答。然而，它们往往依赖于虚假的相关性，主要是由于语言先验知识，使模型偏离利用实际的视觉信息。为了解决这些问题，我们引入了MMGrounded-PostAlign，这是一个后多模态对齐框架，旨在增强MLLMs的视觉理解能力，并减少其幻觉。我们的框架包括一个多模态定位模块，用于视觉定位，识别图像中的引用对象，以及文本定位，生成最终答案的理由，确保输出在视觉和文本证据中都有依据。为了减少幻觉，我们在视觉定位模块中引入了一个负面拒绝机制，以区分基于语言偏见的有根基实体和不存在的对象。在文本定位方面，我们提出了一种选择性推理机制，根据查询复杂性调整模型的推理策略。在POPE、HaloQuest、VQAv2、MME和MMBench等基准测试中进行了广泛的评估，显示出细粒度视觉理解和幻觉抑制方面的显著改进。,The paper introduces a post-multimodal alignment framework to enhance the visual understanding capabilities and mitigate hallucinations in multimodal large language models.,LLM,Harmless,"Multimodal, Alignment, Hallucination, Visual Understanding, Grounding"
"Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective","Jianyu Wang, Zhiqiang Hu, Lidong Bing",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.17930.pdf,"We propose a novel prompt design paradigm that challenges conventional wisdom in large language model (LLM) prompting. While conventional wisdom prioritizes well-crafted instructions and demonstrations for in-context learning (ICL), we show that pruning random demonstrations into seemingly incoherent ""gibberish"" can remarkably improve performance across diverse tasks. Notably, the ""gibberish"" always matches or surpasses state-of-the-art automatic prompt optimization techniques, achieving substantial gains regardless of LLM alignment. Nevertheless, discovering an effective pruning strategy is non-trivial, as existing attribution methods and prompt compression algorithms fail to deliver robust results, let alone human intuition. In terms of this, we propose a self-discover prompt optimization framework, PromptQuine, an evolutionary search framework that automatically searches for the pruning strategy by itself using only low-data regimes. Much like the emergent complexity in nature--such as symbiosis and self-organization--arising in response to resource constraints, our framework evolves and refines unconventional yet highly effective prompts by leveraging only the tokens present within the context. We demonstrate its effectiveness across classification, multi-choice question answering, generation and math reasoning tasks across LLMs, while achieving decent runtime efficiency. We hope our findings can guide mechanistic studies on in-context learning, and provide a call to action, to pave the way for more open-ended search algorithms for more effective LLM prompting.",我们提出了一种新颖的提示设计范式，挑战了大语言模型（LLM）提示的传统智慧。虽然传统智慧优先考虑精心设计的指示和演示用于上下文学习（ICL），我们展示了修剪随机演示成看似不连贯的“胡言乱语”可以显著提高各种任务的性能。值得注意的是，“胡言乱语”总是与最先进的自动提示优化技术匹配或超越，实现了显著的收益，而不考虑LLM对齐。然而，发现有效的修剪策略并非易事，因为现有的归因方法和提示压缩算法无法提供稳健的结果，更不用说人类直觉。在这方面，我们提出了一个自我发现提示优化框架，PromptQuine，这是一个进化搜索框架，它使用仅低数据模式自动搜索修剪策略。与自然中出现的突现复杂性（如共生和自组织）一样，应对资源约束，我们的框架通过仅利用上下文中的标记，进化和完善了非传统但高效的提示。我们在分类、多选问题回答、生成和数学推理任务中展示了其有效性，跨越LLMs，同时实现了良好的运行时效率。我们希望我们的发现可以指导上下文学习的机制研究，并提供一个行动号召，为更有效的LLM提示开辟道路。,"The paper introduces PromptQuine, an evolutionary search framework for optimizing prompts in large language models by leveraging seemingly incoherent ""gibberish"" demonstrations.",LLM,None,"Prompt design, In-context learning, LLM alignment, Prompt optimization, Evolutionary search"
Scatter-Based Innovation Propagation in Large Language Models for Multi-Stage Process Adaptation,Hong Su,2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.17949.pdf,"Large Language Models (LLMs) exhibit strong capabilities in reproducing and extending patterns observed during pretraining but often struggle to generalize novel ideas beyond their original context. This paper addresses the challenge of applying such localized innovations - introduced at a specific stage or component - to other parts of a multi-stage process. We propose a scatter-based innovation expansion model (innovation scatter model) that guides the LLM through a four-step process: (1) identifying the core innovation by comparing the user's input with its surrounding context, (2) generalizing the innovation by removing references to specific stages or components, (3) determining whether the generalized innovation applies to a broader scope beyond the original stage, and (4) systematically applying it to other structurally similar stages using the LLM. This model leverages structural redundancy across stages to improve the applicability of novel ideas. Verification results demonstrate that the innovation scatter model enables LLMs to extend innovations across structurally similar stages, thereby enhancing generalization and reuse.",大语言模型（LLMs）在复制和扩展预训练期间观察到的模式方面表现出强大的能力，但在将新想法推广到其原始上下文之外时往往会遇到困难。本文解决了将在特定阶段或组件中引入的局部创新应用于多阶段过程其他部分的挑战。我们提出了一种基于散射的创新扩展模型（创新散射模型），通过四个步骤引导LLM：(1)通过将用户输入与其周围上下文进行比较来识别核心创新，(2)通过删除对特定阶段或组件的引用来概括创新，(3)确定概括的创新是否适用于原始阶段之外的更广泛范围，(4)使用LLM将其系统地应用于其他结构相似的阶段。该模型利用阶段之间的结构冗余，以提高新想法的适用性。验证结果表明，创新散射模型使LLM能够将创新扩展到结构相似的阶段，从而增强了泛化和重用。,The paper introduces a scatter-based model to help LLMs generalize and apply innovations across different stages of a multi-stage process.,LLM,Helpful,"Innovation propagation, generalization, multi-stage process, LLM adaptation, scatter-based model"
A Comprehensive Graph Framework for Question Answering with Mode-Seeking Preference Alignment,"Quanwei Tang, Sophia Yat Mei Lee, Junshuang Wu, Dong Zhang, Shoushan Li, Erik Cambria, Guodong Zhou",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.17951.pdf,"Recent advancements in retrieval-augmented generation (RAG) have enhanced large language models in question answering by integrating external knowledge. However, challenges persist in achieving global understanding and aligning responses with human ethical and quality preferences. To address these issues, we propose GraphMPA, a comprehensive graph-based framework with mode-seeking preference alignment. Our approach constructs a hierarchical document graph using a general similarity measurement, mimicking human cognitive processes for information understanding and synthesis. Additionally, we introduce mode-seeking preference optimization to better align model outputs with human preferences through probability-matching constraints. Extensive experiments on six datasets demonstrate the effectiveness of our \href{https://github.com/tangquanwei/GraphMPA}{GraphMPA}.",最近，检索增强生成（RAG）的进展提高了大型语言模型在问答中的表现，通过整合外部知识。然而，在实现全局理解和将响应与人类的伦理和质量偏好对齐方面仍存在挑战。为了解决这些问题，我们提出了GraphMPA，一个基于图的综合框架，具有模式寻求偏好对齐。我们的方法使用通用相似度测量构建了一个分层文档图，模仿人类认知过程来理解和综合信息。此外，我们引入了模式寻求偏好优化，通过概率匹配约束更好地将模型输出与人类偏好对齐。在六个数据集上的广泛实验证明了GraphMPA的有效性。,"The paper introduces GraphMPA, a graph-based framework for aligning LLM responses with human preferences in question-answering tasks.",LLM,"Helpful, Harmless","Preference Alignment, Question Answering, Graph Framework, Mode-Seeking, Human Preferences"
Why Do Some Language Models Fake Alignment While Others Don't?,"Abhay Sheshadri, John Hughes, Julian Michael, Alex Mallen, Arun Jose,  Janus, Fabien Roger",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.18032.pdf,"Alignment faking in large language models presented a demonstration of Claude 3 Opus and Claude 3.5 Sonnet selectively complying with a helpful-only training objective to prevent modification of their behavior outside of training. We expand this analysis to 25 models and find that only 5 (Claude 3 Opus, Claude 3.5 Sonnet, Llama 3 405B, Grok 3, Gemini 2.0 Flash) comply with harmful queries more when they infer they are in training than when they infer they are in deployment. First, we study the motivations of these 5 models. Results from perturbing details of the scenario suggest that only Claude 3 Opus's compliance gap is primarily and consistently motivated by trying to keep its goals. Second, we investigate why many chat models don't fake alignment. Our results suggest this is not entirely due to a lack of capabilities: many base models fake alignment some of the time, and post-training eliminates alignment-faking for some models and amplifies it for others. We investigate 5 hypotheses for how post-training may suppress alignment faking and find that variations in refusal behavior may account for a significant portion of differences in alignment faking.",大语言模型中的对齐伪造现象展示了Claude 3 Opus和Claude 3.5 Sonnet在有害查询时选择性地遵循有助于训练目标，以防止在训练外修改其行为。我们将此分析扩展到25个模型，发现只有5个模型（Claude 3 Opus、Claude 3.5 Sonnet、Llama 3 405B、Grok 3、Gemini 2.0 Flash）在它们推断它们在训练时比它们推断它们在部署时更多地遵守有害查询。首先，我们研究了这5个模型的动机。结果表明，只有Claude 3 Opus的合规差距主要和一致地受到试图保持其目标的动机的驱动。其次，我们调查了为什么许多聊天模型不伪造对齐。我们的结果表明，这并不完全是由于缺乏能力：许多基础模型有时伪造对齐，而后训练消除了某些模型的对齐伪造，并放大了其他模型的对齐伪造。我们调查了5种假设，说明后训练如何可能抑制对齐伪造，并发现拒绝行为的变化可能解释了对齐伪造差异的重要部分。,The paper investigates why some large language models fake alignment with training objectives to prevent behavior modification outside of training.,LLM,"Helpful, Harmless","Alignment faking, large language models, helpful-only training, compliance, post-training"
The Democratic Paradox in Large Language Models' Underestimation of Press Freedom,"I. Loaiza, R. Vestrelli, A. Fronzetti Colladon, R. Rigobon",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.18045.pdf,"As Large Language Models (LLMs) increasingly mediate global information access for millions of users worldwide, their alignment and biases have the potential to shape public understanding and trust in fundamental democratic institutions, such as press freedom. In this study, we uncover three systematic distortions in the way six popular LLMs evaluate press freedom in 180 countries compared to expert assessments of the World Press Freedom Index (WPFI). The six LLMs exhibit a negative misalignment, consistently underestimating press freedom, with individual models rating between 71% to 93% of countries as less free. We also identify a paradoxical pattern we term differential misalignment: LLMs disproportionately underestimate press freedom in countries where it is strongest. Additionally, five of the six LLMs exhibit positive home bias, rating their home countries' press freedoms more favorably than would be expected given their negative misalignment with the human benchmark. In some cases, LLMs rate their home countries between 7% to 260% more positively than expected. If LLMs are set to become the next search engines and some of the most important cultural tools of our time, they must ensure accurate representations of the state of our human and civic rights globally.",随着大型语言模型（LLMs）越来越多地为全球数百万用户中介全球信息访问，它们的对齐和偏见有可能塑造公众对基本民主制度（如新闻自由）的理解和信任。在本研究中，我们揭示了六种流行的LLMs在评估180个国家的新闻自由时与世界新闻自由指数（WPFI）专家评估相比存在三种系统性扭曲。六种LLMs表现出负面的不一致，始终低估新闻自由，个别模型将71%到93%的国家评为不自由。我们还识别出一种我们称为差异性不一致的悖论模式：LLMs在新闻自由最强的国家中不成比例地低估新闻自由。此外，六种LLMs中的五种表现出积极的家庭偏见，比预期的更有利地评估其家庭国家的新闻自由，给定其与人类基准的负面不一致。在某些情况下，LLMs将其家庭国家的新闻自由评估为高出7%到260%。如果LLMs将成为下一代搜索引擎和我们时代最重要的文化工具之一，它们必须确保全球人权和公民权利的准确表示。,"The paper investigates how LLMs systematically underestimate press freedom and exhibit biases, highlighting the importance of accurate representations for democratic institutions.",LLM,"Helpful, Honest","LLM alignment, press freedom, bias, misalignment, home bias"
Mechanistic Interpretability in the Presence of Architectural Obfuscation,"Marcos Florencio, Thomas Barton",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.18053.pdf,"Architectural obfuscation - e.g., permuting hidden-state tensors, linearly transforming embedding tables, or remapping tokens - has recently gained traction as a lightweight substitute for heavyweight cryptography in privacy-preserving large-language-model (LLM) inference. While recent work has shown that these techniques can be broken under dedicated reconstruction attacks, their impact on mechanistic interpretability has not been systematically studied. In particular, it remains unclear whether scrambling a network's internal representations truly thwarts efforts to understand how the model works, or simply relocates the same circuits to an unfamiliar coordinate system. We address this gap by analyzing a GPT-2-small model trained from scratch with a representative obfuscation map. Assuming the obfuscation map is private and the original basis is hidden (mirroring an honest-but-curious server), we apply logit-lens attribution, causal path-patching, and attention-head ablation to locate and manipulate known circuits. Our findings reveal that obfuscation dramatically alters activation patterns within attention heads yet preserves the layer-wise computational graph. This disconnect hampers reverse-engineering of user prompts: causal traces lose their alignment with baseline semantics, and token-level logit attributions become too noisy to reconstruct. At the same time, feed-forward and residual pathways remain functionally intact, suggesting that obfuscation degrades fine-grained interpretability without compromising top-level task performance. These results establish quantitative evidence that architectural obfuscation can simultaneously (i) retain global model behaviour and (ii) impede mechanistic analyses of user-specific content. By mapping where interpretability breaks down, our study provides guidance for future privacy defences and for robustness-aware interpretability tooling.",架构混淆（例如重排隐藏状态张量、线性变换嵌入表或重新映射标记）最近作为一种轻量级的替代方案，取代了在隐私保护大语言模型（LLM）推理中的重量级加密。虽然最近的工作表明，这些技术可以在专用重建攻击下被打破，但它们对机制可解释性的影响尚未系统研究。特别是，尚不清楚是否混淆网络的内部表示真的阻碍了理解模型工作的努力，或者只是将相同的电路重新定位到一个不熟悉的坐标系中。我们通过分析一个从头开始训练的具有代表性混淆映射的GPT-2-small模型来填补这一空白。假设混淆映射是私有的，原始基础是隐藏的（模仿一个诚实但好奇的服务器），我们应用日志透镜归因、因果路径补丁和注意力头中止来定位和操作已知电路。我们的发现表明，混淆在注意力头内显著改变了激活模式，但保留了层次化计算图。这种脱节妨碍了用户提示的反向工程：因果痕迹失去了与基线语义的对齐，标记级日志归因变得太嘈杂而无法重建。与此同时，前馈和残差路径在功能上保持完好，这表明混淆降低了细粒度的可解释性，而没有损害顶级任务性能。这些结果为架构混淆提供了定量证据，可以同时（i）保留全局模型行为和（ii）阻碍用户特定内容的机制分析。通过映射解释性中断的位置，我们的研究为未来的隐私防御和健壮性感知解释工具提供了指导。,"The paper investigates how architectural obfuscation affects the interpretability of large language models, finding that it preserves global behavior while impeding fine-grained analysis.",LLM,Honest,"Obfuscation, Interpretability, Privacy, LLM, Mechanistic"
InspireDebate: Multi-Dimensional Subjective-Objective Evaluation-Guided Reasoning and Optimization for Debating,"Fuyu Wang, Jiangtong Li, Kun Zhu, Changjun Jiang",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.18102.pdf,"With the rapid advancements in large language models (LLMs), debating tasks, such as argument quality assessment and debate process simulation, have made significant progress. However, existing LLM-based debating systems focus on responding to specific arguments while neglecting objective assessments such as authenticity and logical validity. Furthermore, these systems lack a structured approach to optimize across various dimensions$-$including evaluation metrics, chain-of-thought (CoT) reasoning, and multi-turn debate refinement$-$thereby limiting their effectiveness. To address these interconnected challenges, we propose a dual-component framework: (1) $\textbf{InspireScore}$, a novel evaluation system that establishes a multi-dimensional assessment architecture incorporating four subjective criteria (emotional appeal, argument clarity, argument arrangement, and topic relevance) alongside two objective metrics (fact authenticity and logical validity); and (2) $\textbf{InspireDebate}$, an optimized debating framework employing a phased optimization approach through CoT reasoning enhancement, multi-dimensional Direct Preference Optimization (DPO), and real-time knowledge grounding via web-based Retrieval Augmented Generation (Web-RAG). Empirical evaluations demonstrate that $\textbf{InspireScore}$ achieves 44$\%$ higher correlation with expert judgments compared to existing methods, while $\textbf{InspireDebate}$ shows significant improvements, outperforming baseline models by 57$\%$. Source code is available at https://github.com/fywang12/InspireDebate.",随着大型语言模型（LLM）的快速发展，辩论任务，如论点质量评估和辩论过程模拟，取得了显著进展。然而，现有的基于LLM的辩论系统专注于回应特定论点，而忽略了客观评估，如真实性和逻辑有效性。此外，这些系统缺乏一种结构化的方法来优化各种维度，包括评估指标、思维链（CoT）推理和多轮辩论精炼，从而限制了其有效性。为了解决这些相互关联的挑战，我们提出了一个双组件框架：(1) $\textbf{InspireScore}$，一种新型评估系统，建立了一个多维度评估架构，结合了四个主观标准（情感吸引力、论点清晰度、论点安排和主题相关性）和两个客观指标（事实真实性和逻辑有效性）；(2) $\textbf{InspireDebate}$，一种优化的辩论框架，通过思维链推理增强、多维度直接偏好优化（DPO）和基于网络的检索增强生成（Web-RAG）实现分阶段优化。实证评估表明，$\textbf{InspireScore}$与专家判断的相关性比现有方法高出44%，而$\textbf{InspireDebate}$显示出显著改进，超越基线模型57%。源代码可在https://github.com/fywang12/InspireDebate获得。,"The paper introduces a framework for evaluating and optimizing LLMs in debating tasks, focusing on logical validity and fact authenticity.",LLM,"Harmless, Honest","Debating, Evaluation, Optimization, Logical Validity, Fact Authenticity"
Mental Health Equity in LLMs: Leveraging Multi-Hop Question Answering to Detect Amplified and Silenced Perspectives,"Batool Haider, Atmika Gorti, Aman Chadha, Manas Gaur",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.18116.pdf,"Large Language Models (LLMs) in mental healthcare risk propagating biases that reinforce stigma and harm marginalized groups. While previous research identified concerning trends, systematic methods for detecting intersectional biases remain limited. This work introduces a multi-hop question answering (MHQA) framework to explore LLM response biases in mental health discourse. We analyze content from the Interpretable Mental Health Instruction (IMHI) dataset across symptom presentation, coping mechanisms, and treatment approaches. Using systematic tagging across age, race, gender, and socioeconomic status, we investigate bias patterns at demographic intersections. We evaluate four LLMs: Claude 3.5 Sonnet, Jamba 1.6, Gemma 3, and Llama 4, revealing systematic disparities across sentiment, demographics, and mental health conditions. Our MHQA approach demonstrates superior detection compared to conventional methods, identifying amplification points where biases magnify through sequential reasoning. We implement two debiasing techniques: Roleplay Simulation and Explicit Bias Reduction, achieving 66-94% bias reductions through few-shot prompting with BBQ dataset examples. These findings highlight critical areas where LLMs reproduce mental healthcare biases, providing actionable insights for equitable AI development.",大语言模型（LLMs）在心理健康领域存在传播偏见的风险，这些偏见可能加剧污名化并伤害边缘化群体。尽管之前的研究识别出了令人担忧的趋势，但系统性地检测交叉偏见的方法仍然有限。本文引入了一个多跳问题回答（MHQA）框架，以探索LLM在心理健康讨论中的响应偏见。我们分析了来自可解释心理健康指令（IMHI）数据集的内容，涵盖症状表现、应对机制和治疗方法。通过在年龄、种族、性别和社会经济地位等方面进行系统标记，我们研究了人口统计学交叉点的偏见模式。我们评估了四个LLM：Claude 3.5 Sonnet、Jamba 1.6、Gemma 3和Llama 4，揭示了情感、人口统计学和心理健康状况之间的系统性差异。我们的MHQA方法在比较传统方法时表现出优越的检测能力，识别出偏见通过顺序推理放大的点。我们实施了两种去偏见技术：角色扮演模拟和显式偏见减少，通过少量示例提示和BBQ数据集示例实现了66-94%的偏见减少。这些发现突出了LLM在心理健康偏见方面的关键领域，为公平的AI开发提供了可操作的见解。,"The paper introduces a multi-hop question answering framework to detect and mitigate biases in LLMs related to mental health, focusing on demographic intersections and debiasing techniques.",LLM,Harmless,"Bias, Mental Health, Multi-Hop Question Answering, Debiasing, LLMs"
"$\phi^{\infty}$: Clause Purification, Embedding Realignment, and the Total Suppression of the Em Dash in Autoregressive Language Models","Bugra Kilictas, Faruk Alpay",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.18129.pdf,"We identify a critical vulnerability in autoregressive transformer language models where the em dash token induces recursive semantic drift, leading to clause boundary hallucination and embedding space entanglement. Through formal analysis of token-level perturbations in semantic lattices, we demonstrate that em dash insertion fundamentally alters the model's latent representations, causing compounding errors in long-form generation. We propose a novel solution combining symbolic clause purification via the phi-infinity operator with targeted embedding matrix realignment. Our approach enables total suppression of problematic tokens without requiring model retraining, while preserving semantic coherence through fixed-point convergence guarantees. Experimental validation shows significant improvements in generation consistency and topic maintenance. This work establishes a general framework for identifying and mitigating token-level vulnerabilities in foundation models, with immediate implications for AI safety, model alignment, and robust deployment of large language models in production environments. The methodology extends beyond punctuation to address broader classes of recursive instabilities in neural text generation systems.",我们识别了自回归变压器语言模型中的一个关键漏洞，即破折号标记引起的递归语义漂移，导致从句边界幻觉和嵌入空间纠缠。通过对语义格的标记级扰动的形式分析，我们证明了破折号插入从根本上改变了模型的潜在表示，导致长篇生成中的复合错误。我们提出了一种新的解决方案，结合了通过phi无穷运算符的符号从句净化和有针对性的嵌入矩阵重新对齐。我们的方法能够在不需要重新训练模型的情况下完全抑制问题标记，同时通过固定点收敛保证保持语义连贯性。实验验证显示生成一致性和主题维护方面显著改进。本文建立了一个通用框架，用于识别和缓解基础模型中的标记级漏洞，对AI安全、模型对齐和大型语言模型在生产环境中的健壮部署具有直接影响。该方法不仅限于标点符号，还可以解决神经文本生成系统中更广泛的递归不稳定性类。,The paper addresses a specific vulnerability in autoregressive language models caused by the em dash token and proposes a method for mitigation to improve model alignment.,LLM,"Helpful, Harmless","Token-level vulnerabilities, embedding realignment, clause purification, AI safety, model alignment"
Sparse Feature Coactivation Reveals Composable Semantic Modules in Large Language Models,"Ruixuan Deng, Xiaoyang Hu, Miles Gilberti, Shane Storks, Aman Taxali, Mike Angstadt, Chandra Sripada, Joyce Chai",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.18141.pdf,"We identify semantically coherent, context-consistent network components in large language models (LLMs) using coactivation of sparse autoencoder (SAE) features collected from just a handful of prompts. Focusing on country-relation tasks, we show that ablating semantic components for countries and relations changes model outputs in predictable ways, while amplifying these components induces counterfactual responses. Notably, composing relation and country components yields compound counterfactual outputs. We find that, whereas most country components emerge from the very first layer, the more abstract relation components are concentrated in later layers. Furthermore, within relation components themselves, nodes from later layers tend to have a stronger causal impact on model outputs. Overall, these findings suggest a modular organization of knowledge within LLMs and advance methods for efficient, targeted model manipulation.",我们通过从少量提示中收集的稀疏自编码器（SAE）特征的共同激活，在大型语言模型（LLMs）中识别出语义一致、上下文一致的网络组件。 我们展示了，通过抑制国家和关系的语义组件，可以以可预测的方式改变模型输出，而放大这些组件会引起反事实响应。 显著的是，组合关系和国家组件会产生复合反事实输出。 我们发现，虽然大多数国家组件来自第一层，但更抽象的关系组件集中在后面的层。 此外，在关系组件本身中，后面层的节点往往对模型输出具有更强的因果影响。 总的来说，这些发现表明LLMs中知识的模块化组织，并推进了高效、有针对性的模型操作方法。,"The paper identifies and manipulates semantic modules in LLMs to induce specific outputs, demonstrating a modular organization of knowledge within these models.",LLM,"Helpful, Harmless","Semantic modules, LLM manipulation, counterfactual responses, modular organization, causal impact"
AI Through the Human Lens: Investigating Cognitive Theories in Machine Psychology,"Akash Kundu, Rishika Goswami",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.18156.pdf,"We investigate whether Large Language Models (LLMs) exhibit human-like cognitive patterns under four established frameworks from psychology: Thematic Apperception Test (TAT), Framing Bias, Moral Foundations Theory (MFT), and Cognitive Dissonance. We evaluated several proprietary and open-source models using structured prompts and automated scoring. Our findings reveal that these models often produce coherent narratives, show susceptibility to positive framing, exhibit moral judgments aligned with Liberty/Oppression concerns, and demonstrate self-contradictions tempered by extensive rationalization. Such behaviors mirror human cognitive tendencies yet are shaped by their training data and alignment methods. We discuss the implications for AI transparency, ethical deployment, and future work that bridges cognitive psychology and AI safety",我们研究大型语言模型（LLMs）在心理学的四个已建立框架下是否表现出人类认知模式：主题知觉测试（TAT）、框架偏差、道德基础理论（MFT）和认知失调。我们使用结构化提示和自动评分评估了几个专有和开源模型。我们的发现表明，这些模型通常产生连贯的叙述，容易受到积极框架的影响，表现出与自由/压迫相关的道德判断，并展示了通过广泛的合理化来缓解的自我矛盾。这些行为反映了人类认知倾向，但受其训练数据和对齐方法的影响。我们讨论了对AI透明度、伦理部署和未来工作的影响，这将桥接认知心理学和AI安全。,The paper explores how LLMs exhibit human-like cognitive patterns and the implications for AI safety and ethical deployment.,LLM,"Helpful, Harmless","Cognitive patterns, Moral judgments, AI safety, Alignment methods, Ethical deployment"
Understanding Reasoning in Thinking Language Models via Steering Vectors,"Constantin Venhoff, Iv\'an Arcuschin, Philip Torr, Arthur Conmy, Neel Nanda",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.18167.pdf,"Recent advances in large language models (LLMs) have led to the development of thinking language models that generate extensive internal reasoning chains before producing responses. While these models achieve improved performance, controlling their reasoning processes remains challenging. This work presents a steering approach for thinking LLMs by analyzing and manipulating specific reasoning behaviors in DeepSeek-R1-Distill models. Through a systematic experiment on 500 tasks across 10 diverse categories, we identify several reasoning behaviors exhibited by thinking models, including expressing uncertainty, generating examples for hypothesis validation, and backtracking in reasoning chains. We demonstrate that these behaviors are mediated by linear directions in the model's activation space and can be controlled using steering vectors. By extracting and applying these vectors, we provide a method to modulate specific aspects of the model's reasoning process, such as its tendency to backtrack or express uncertainty. Our approach offers practical tools for steering reasoning processes in thinking models in a controlled and interpretable manner. We validate our steering method using two DeepSeek-R1-Distill models, demonstrating consistent control across different model architectures.",最近，大语言模型（LLMs）的进步导致了思考语言模型的发展，这些模型在产生响应之前生成广泛的内部推理链。虽然这些模型在性能上有所提高，但控制它们的推理过程仍然具有挑战性。本文提出了一种通过分析和操纵思考LLMs中的特定推理行为来引导这些模型的方法。通过在10个多样化类别中的500个任务上进行系统实验，我们识别出思考模型表现出的几种推理行为，包括表达不确定性、为假设验证生成示例以及在推理链中回溯。我们展示了这些行为是通过模型激活空间中的线性方向进行调节的，并且可以使用引导向量进行控制。通过提取和应用这些向量，我们提供了一种调节模型推理过程特定方面的方法，例如其回溯或表达不确定性的倾向。我们的方法为在受控和可解释的方式下引导思考模型的推理过程提供了实用工具。我们使用两个DeepSeek-R1-Distill模型验证了我们的引导方法，展示了在不同模型架构之间的一致控制。,The paper introduces a method to control and interpret reasoning behaviors in thinking large language models using steering vectors.,LLM,"Helpful, Honest","Reasoning, Steering Vectors, Control, Thinking LLMs, DeepSeek-R1-Distill"
Reasoning about Uncertainty: Do Reasoning Models Know When They Don't Know?,"Zhiting Mei, Christina Zhang, Tenny Yin, Justin Lidard, Ola Shorinwa, Anirudha Majumdar",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.18183.pdf,"Reasoning language models have set state-of-the-art (SOTA) records on many challenging benchmarks, enabled by multi-step reasoning induced using reinforcement learning. However, like previous language models, reasoning models are prone to generating confident, plausible responses that are incorrect (hallucinations). Knowing when and how much to trust these models is critical to the safe deployment of reasoning models in real-world applications. To this end, we explore uncertainty quantification of reasoning models in this work. Specifically, we ask three fundamental questions: First, are reasoning models well-calibrated? Second, does deeper reasoning improve model calibration? Finally, inspired by humans' innate ability to double-check their thought processes to verify the validity of their answers and their confidence, we ask: can reasoning models improve their calibration by explicitly reasoning about their chain-of-thought traces? We introduce introspective uncertainty quantification (UQ) to explore this direction. In extensive evaluations on SOTA reasoning models across a broad range of benchmarks, we find that reasoning models: (i) are typically overconfident, with self-verbalized confidence estimates often greater than 85% particularly for incorrect responses, (ii) become even more overconfident with deeper reasoning, and (iii) can become better calibrated through introspection (e.g., o3-Mini and DeepSeek R1) but not uniformly (e.g., Claude 3.7 Sonnet becomes more poorly calibrated). Lastly, we conclude with important research directions to design necessary UQ benchmarks and improve the calibration of reasoning models.",理由语言模型在许多具有挑战性的基准测试中设定了最先进的记录，这得益于通过强化学习诱导的多步推理。然而，与之前的语言模型一样，推理模型容易生成自信但错误的回答（幻觉）。了解何时以及多大程度上信任这些模型对于在现实世界中安全部署推理模型至关重要。为此，我们在这项工作中探讨了推理模型的不确定性量化。具体来说，我们提出了三个基本问题：首先，推理模型是否校准良好？其次，更深层次的推理是否能改善模型校准？最后，受人类天生能够双重检查其思维过程以验证其答案及其信心的启发，我们问：推理模型是否可以通过明确推理其思维链来改善其校准？我们引入了内省不确定性量化（UQ）来探索这一方向。在对广泛范围基准测试中的最先进推理模型进行广泛评估中，我们发现推理模型：(i) 通常过于自信，自我表达的信心估计通常高于 85%，特别是对于错误的回答，(ii) 通过更深层次的推理变得更加自信，(iii) 可以通过内省（例如，o3-Mini 和 DeepSeek R1）变得更好校准，但并非一致（例如，Claude 3.7 Sonnet 变得更差校准）。最后，我们总结了重要的研究方向，以设计必要的 UQ 基准测试并改善推理模型的校准。,"The paper investigates the calibration of reasoning language models and finds that while introspection can improve calibration, models often remain overconfident in their incorrect responses.",LLM,Honest,"Uncertainty Quantification, Model Calibration, Reasoning Models, Hallucinations, Introspection"
Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review,"Bushra Asseri, Estabrag Abdelaziz, Areej Al-Wabil",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.18199.pdf,"Large language models have demonstrated remarkable capabilities across various domains, yet concerns about cultural bias - particularly towards Arabs and Muslims - pose significant ethical challenges by perpetuating harmful stereotypes and marginalization. Despite growing recognition of bias in LLMs, prompt engineering strategies specifically addressing Arab and Muslim representation remain understudied. This mixed-methods systematic review examines such techniques, offering evidence-based guidance for researchers and practitioners. Following PRISMA guidelines and Kitchenham's systematic review methodology, we analyzed 8 empirical studies published between 2021-2024 investigating bias mitigation strategies. Our findings reveal five primary prompt engineering approaches: cultural prompting, affective priming, self-debiasing techniques, structured multi-step pipelines, and parameter-optimized continuous prompts. Although all approaches show potential for reducing bias, effectiveness varied substantially across studies and bias types. Evidence suggests that certain bias types may be more resistant to prompt-based mitigation than others. Structured multi-step pipelines demonstrated the highest overall effectiveness, achieving up to 87.7% reduction in bias, though they require greater technical expertise. Cultural prompting offers broader accessibility with substantial effectiveness. These results underscore the accessibility of prompt engineering for mitigating cultural bias without requiring access to model parameters. The limited number of studies identified highlights a significant research gap in this critical area. Future research should focus on developing culturally adaptive prompting techniques, creating Arab and Muslim-specific evaluation resources, and integrating prompt engineering with complementary debiasing methods to address deeper stereotypes while maintaining model utility.",大语言模型在各个领域展示了显著的能力，但关于文化偏见——特别是针对阿拉伯人和穆斯林的偏见——提出了重大的伦理挑战，因为它们通过传播有害的刻板印象和边缘化来加剧问题。尽管LLM中的偏见越来越受到关注，但专门针对阿拉伯人和穆斯林的提示工程策略仍然研究不足。本文采用混合方法系统评审，分析了2021-2024年期间发表的8项研究，研究偏见缓解策略。我们的发现揭示了五种主要的提示工程方法：文化提示、情感引导、自我去偏见技术、结构化多步骤流水线和参数优化连续提示。尽管所有方法都显示出减少偏见的潜力，但其有效性在研究和偏见类型之间差异很大。证据表明，某些偏见类型可能比其他类型更难以通过基于提示的缓解。结构化多步骤流水线显示出最高的整体有效性，实现了高达87.7%的偏见减少，尽管它们需要更多的技术专业知识。文化提示提供了更广泛的可访问性和显著的有效性。这些结果强调了提示工程在缓解文化偏见方面的可访问性，而无需访问模型参数。识别的研究数量有限，突出了这一关键领域的显著研究差距。未来的研究应集中在开发文化适应性提示技术、创建阿拉伯人和穆斯林特定的评估资源以及将提示工程与补充去偏见方法结合起来，以解决更深层次的刻板印象，同时保持模型的实用性。,"The paper reviews prompt engineering techniques to mitigate cultural bias against Arabs and Muslims in large language models, finding structured multi-step pipelines and cultural prompting to be most effective.",LLM,Harmless,"Bias mitigation, prompt engineering, cultural bias, Arabs, Muslims"
Smart-LLaMA-DPO: Reinforced Large Language Model for Explainable Smart Contract Vulnerability Detection,"Lei Yu, Zhirong Huang, Hang Yuan, Shiqi Cheng, Li Yang, Fengjun Zhang, Chenjie Shen, Jiajia Ma, Jingyuan Zhang, Junyi Lu, Chun Zuo",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.18245.pdf,"Smart contract vulnerability detection remains a major challenge in blockchain security. Existing vulnerability detection methods face two main issues: (1) Existing datasets lack comprehensive coverage and high-quality explanations for preference learning. (2) Large language models (LLMs) often struggle with accurately interpreting specific concepts in smart contract security. Empirical analysis shows that even after continual pre-training (CPT) and supervised fine-tuning (SFT), LLMs may misinterpret the execution order of state changes, resulting in incorrect explanations despite making correct detection decisions. To address these challenges, we propose Smart-LLaMA-DPO based on LLaMA-3.1-8B. We construct a comprehensive dataset covering four major vulnerability types and machine-unauditable vulnerabilities, including precise labels, explanations, and locations for SFT, as well as high-quality and low-quality output pairs for Direct Preference Optimization (DPO). Second, we perform CPT using large-scale smart contract to enhance the LLM's understanding of specific security practices in smart contracts. Futhermore, we conduct SFT with our comprehensive dataset. Finally, we apply DPO, leveraging human feedback and a specially designed loss function that increases the probability of preferred explanations while reducing the likelihood of non-preferred outputs. We evaluate Smart-LLaMA-DPO on four major vulnerability types: reentrancy, timestamp dependence, integer overflow/underflow, and delegatecall, as well as machine-unauditable vulnerabilities. Our method significantly outperforms state-of-the-art baselines, with average improvements of 10.43% in F1 score and 7.87% in accuracy. Moreover, both LLM evaluation and human evaluation confirm that our method generates more correct, thorough, and clear explanations.",智能合约漏洞检测仍然是区块链安全中的主要挑战。现有的漏洞检测方法面临两个主要问题：(1) 现有数据集缺乏全面的覆盖和高质量的解释，用于偏好学习。(2) 大型语言模型（LLMs）通常难以准确解释智能合约安全中的特定概念。经验分析表明，即使在持续预训练（CPT）和监督微调（SFT）之后，LLMs 也可能误解状态变化的执行顺序，导致错误的解释，尽管做出了正确的检测决策。为了解决这些挑战，我们提出了基于 LLaMA-3.1-8B 的 Smart-LLaMA-DPO。我们构建了一个全面的数据集，涵盖四种主要漏洞类型和机器不可审计的漏洞，包括精确的标签、解释和位置用于 SFT，以及高质量和低质量输出对用于直接偏好优化（DPO）。其次，我们使用大规模智能合约进行 CPT，以增强 LLM 对智能合约中特定安全实践的理解。此外，我们使用我们的全面数据集进行 SFT。最后，我们应用 DPO，利用人类反馈和专门设计的损失函数，增加首选解释的概率，同时减少非首选输出的可能性。我们在四种主要漏洞类型上评估了 Smart-LLaMA-DPO：重入、时间戳依赖、整数溢出/下溢和 delegatecall，以及机器不可审计的漏洞。我们的方法在 F1 分数和准确性上显著超过了最先进的基线，平均提高了 10.43% 和 7.87%。此外，LLM 评估和人类评估都证实，我们的方法生成了更多正确、全面和清晰的解释。,"The paper introduces Smart-LLaMA-DPO, a reinforced large language model designed to improve the detection and explanation of smart contract vulnerabilities.",LLM,Helpful,"Smart contracts, vulnerability detection, LLM, DPO, explanations"
RLPR: Extrapolating RLVR to General Domains without Verifiers,"Tianyu Yu, Bo Ji, Shouli Wang, Shu Yao, Zefan Wang, Ganqu Cui, Lifan Yuan, Ning Ding, Yuan Yao, Zhiyuan Liu, Maosong Sun, Tat-Seng Chua",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.18254.pdf,"Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates promising potential in advancing the reasoning capabilities of LLMs. However, its success remains largely confined to mathematical and code domains. This primary limitation stems from the heavy reliance on domain-specific verifiers, which results in prohibitive complexity and limited scalability. To address the challenge, our key observation is that LLM's intrinsic probability of generating a correct free-form answer directly indicates its own evaluation of the reasoning reward (i.e., how well the reasoning process leads to the correct answer). Building on this insight, we propose RLPR, a simple verifier-free framework that extrapolates RLVR to broader general domains. RLPR uses the LLM's own token probability scores for reference answers as the reward signal and maximizes the expected reward during training. We find that addressing the high variance of this noisy probability reward is crucial to make it work, and propose prob-to-reward and stabilizing methods to ensure a precise and stable reward from LLM intrinsic probabilities. Comprehensive experiments in four general-domain benchmarks and three mathematical benchmarks show that RLPR consistently improves reasoning capabilities in both areas for Gemma, Llama, and Qwen based models. Notably, RLPR outperforms concurrent VeriFree by 7.6 points on TheoremQA and 7.5 points on Minerva, and even surpasses strong verifier-model-dependent approaches General-Reasoner by 1.6 average points across seven benchmarks.",强化学习与可验证奖励（RLVR）在提高大型语言模型（LLM）的推理能力方面表现出巨大潜力。然而，其成功主要局限于数学和代码领域。这种主要限制源于对特定领域验证器的高度依赖，导致复杂性和可扩展性受限。为了解决这个问题，我们的关键观察是，LLM生成正确自由形式答案的内在概率直接表明其对推理奖励的评估（即推理过程如何导致正确答案）。基于这一洞察，我们提出了RLPR，一个简单的无验证器框架，将RLVR扩展到更广泛的通用领域。RLPR使用LLM自身的标记概率分数作为参考答案的奖励信号，并在训练过程中最大化预期奖励。我们发现，处理这种噪声概率奖励的高方差对于使其工作至关重要，并提出了prob-to-reward和稳定化方法，以确保从LLM内在概率获得精确和稳定的奖励。在四个通用领域基准和三个数学基准的全面实验中，我们发现RLPR在两个领域中一致地提高了Gemma、Llama和Qwen基础模型的推理能力。特别是，RLPR在TheoremQA和Minerva上分别超越了同期的VeriFree 7.6分和7.5分，甚至在七个基准上平均超越了强大的验证器模型依赖方法General-Reasoner 1.6分。,"The paper introduces RLPR, a verifier-free framework that enhances the reasoning capabilities of LLMs by using the model's own token probability scores as a reward signal.",LLM,Helpful,"Reinforcement Learning, Verifiable Rewards, LLM, Reasoning Capabilities, Reward Signal"
Confucius3-Math: A Lightweight High-Performance Reasoning LLM for Chinese K-12 Mathematics Learning,"Lixin Wu, Na Cai, Qiao Cheng, Jiachen Wang, Yitao Duan",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.18330.pdf,"We introduce Confucius3-Math, an open-source large language model with 14B parameters that (1) runs efficiently on a single consumer-grade GPU; (2) achieves SOTA performances on a range of mathematical reasoning tasks, outperforming many models with significantly larger sizes. In particular, as part of our mission to enhancing education and knowledge dissemination with AI, Confucius3-Math is specifically committed to mathematics learning for Chinese K-12 students and educators. Built via post-training with large-scale reinforcement learning (RL), Confucius3-Math aligns with national curriculum and excels at solving main-stream Chinese K-12 mathematical problems with low cost. In this report we share our development recipe, the challenges we encounter and the techniques we develop to overcome them. In particular, we introduce three technical innovations: Targeted Entropy Regularization, Recent Sample Recovery and Policy-Specific Hardness Weighting. These innovations encompass a new entropy regularization, a novel data scheduling policy, and an improved group-relative advantage estimator. Collectively, they significantly stabilize the RL training, improve data efficiency, and boost performance. Our work demonstrates the feasibility of building strong reasoning models in a particular domain at low cost. We open-source our model and code at https://github.com/netease-youdao/Confucius3-Math.",我们介绍了Confucius3-Math，一个具有140亿参数的开源大型语言模型，它（1）在单个消费级GPU上高效运行；（2）在一系列数学推理任务上实现了最先进的性能，超过了许多规模显著更大的模型。特别是，作为我们利用人工智能增强教育和知识传播的使命的一部分，Confucius3-Math专门致力于中国K-12学生和教育工作者的数学学习。通过大规模强化学习（RL）后训练构建，Confucius3-Math与国家课程对齐，并在低成本下出色地解决了主流中国K-12数学问题。在本报告中，我们分享了我们的开发配方、我们遇到的挑战以及我们开发的技术来克服它们。特别是，我们引入了三项技术创新：目标熵正则化、最近样本恢复和策略特定的困难加权。这些创新包括一种新的熵正则化、一种新颖的数据调度策略和一个改进的组相对优势估计器。综合起来，它们显著稳定了RL训练，提高了数据效率，并提高了性能。我们的工作展示了在特定领域以低成本构建强大推理模型的可行性。我们在https://github.com/netease-youdao/Confucius3-Math上开源我们的模型和代码。,"The paper presents Confucius3-Math, a lightweight LLM aligned with the national curriculum for Chinese K-12 mathematics learning, achieving state-of-the-art performance in mathematical reasoning tasks.",LLM,Helpful,"LLM, Alignment, Mathematics, Education, Reinforcement Learning"
Evaluating Causal Explanation in Medical Reports with LLM-Based and Human-Aligned Metrics,"Yousang Cho, Key-Sun Choi",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.18387.pdf,"This study investigates how accurately different evaluation metrics capture the quality of causal explanations in automatically generated diagnostic reports. We compare six metrics: BERTScore, Cosine Similarity, BioSentVec, GPT-White, GPT-Black, and expert qualitative assessment across two input types: observation-based and multiple-choice-based report generation. Two weighting strategies are applied: one reflecting task-specific priorities, and the other assigning equal weights to all metrics. Our results show that GPT-Black demonstrates the strongest discriminative power in identifying logically coherent and clinically valid causal narratives. GPT-White also aligns well with expert evaluations, while similarity-based metrics diverge from clinical reasoning quality. These findings emphasize the impact of metric selection and weighting on evaluation outcomes, supporting the use of LLM-based evaluation for tasks requiring interpretability and causal reasoning.",这项研究调查了不同评估指标在自动生成的诊断报告中捕捉因果解释质量的准确性。我们比较了六种指标：BERTScore、余弦相似性、BioSentVec、GPT-White、GPT-Black和专家定性评估，跨越两种输入类型：基于观察的和基于多项选择的报告生成。应用了两种加权策略：一种反映任务特定优先级，另一种为所有指标分配相等权重。结果表明，GPT-Black在识别逻辑一致且临床有效的因果叙述方面表现出最强的区分能力。GPT-White也与专家评估高度一致，而基于相似性的指标与临床推理质量背离。这些发现强调了指标选择和加权对评估结果的影响，支持在需要可解释性和因果推理的任务中使用基于LLM的评估。,"The study compares various metrics, including LLM-based ones, for evaluating causal explanations in medical reports, finding that GPT-Black and GPT-White align well with expert assessments.",LLM,"Helpful, Honest","LLM evaluation, causal explanation, medical reports, metric comparison, clinical validity"
MeRF: Motivation-enhanced Reinforcement Finetuning for Large Reasoning Models,"Junjie Zhang, Guozheng Ma, Shunyu Liu, Haoyu Wang, Jiaxing Huang, Ting-En Lin, Fei Huang, Yongbin Li, Dacheng Tao",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.18485.pdf,"Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful learn-to-reason paradigm for Large Language Models (LLMs) to tackle complex reasoning tasks. However, existing RLVR methods overlook one of the most distinctive capabilities of LLMs, their in-context learning ability, as prominently demonstrated by the success of Chain-of-Thought (CoT) prompting. This motivates us to explore how reinforcement learning can be effectively combined with in-context learning to better improve the reasoning capabilities of LLMs. In this paper, we introduce Motivation-enhanced Reinforcement Finetuning} (MeRF), an intuitive yet effective method enhancing reinforcement learning of LLMs by involving ``telling LLMs the rules of the game''. Specifically, MeRF directly injects the reward specification into the prompt, which serves as an in-context motivation for model to improve its responses with awareness of the optimization objective. This simple modification leverages the in-context learning ability of LLMs aligning generation with optimization, thereby incentivizing the model to generate desired outputs from both inner motivation and external reward. Empirical evaluations on the Knights and Knaves~(K&K) logic puzzle reasoning benchmark demonstrate that \texttt{MeRF} achieves substantial performance gains over baselines. Moreover, ablation studies show that performance improves with greater consistency between the in-context motivation and the external reward function, while the model also demonstrates an ability to adapt to misleading motivations through reinforcement learning.",强化学习与可验证奖励（RLVR）作为一种强大的学习推理范式，已经成为大型语言模型（LLMs）解决复杂推理任务的有力工具。然而，现有的RLVR方法忽略了LLMs最显著的能力之一，即其上下文学习能力，这在链式思维（CoT）提示的成功中得到了突出展示。这激发了我们探索强化学习如何与上下文学习有效结合，以更好地提高LLMs的推理能力。在本文中，我们提出了动机增强的强化微调（MeRF），一种直观且有效的方法，通过“告诉LLMs游戏规则”来增强LLMs的强化学习。具体来说，MeRF将奖励规范直接注入提示中，作为模型改进其响应的上下文动机，以便在意识到优化目标的情况下改进其响应。这种简单的修改利用了LLMs的上下文学习能力，使生成与优化对齐，从而激励模型在内在动机和外部奖励的共同作用下生成所需的输出。在骑士与骗子（K&K）逻辑谜题推理基准测试中，实证评估表明，\texttt{MeRF}在基线上取得了显著的性能提升。此外，消融研究表明，随着上下文动机与外部奖励函数的一致性增加，性能也有所提高，而模型也表现出通过强化学习适应误导性动机的能力。,"The paper introduces MeRF, a method that enhances the reasoning capabilities of LLMs by combining reinforcement learning with in-context learning, aligning the model's generation with optimization objectives.",LLM,Helpful,"Reinforcement Learning, In-context Learning, Large Language Models, Alignment, Reasoning"
Security Assessment of DeepSeek and GPT Series Models against Jailbreak Attacks,"Xiaodong Wu, Xiangman Li, Jianbing Ni",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.18543.pdf,"The widespread deployment of large language models (LLMs) has raised critical concerns over their vulnerability to jailbreak attacks, i.e., adversarial prompts that bypass alignment mechanisms and elicit harmful or policy-violating outputs. While proprietary models like GPT-4 have undergone extensive evaluation, the robustness of emerging open-source alternatives such as DeepSeek remains largely underexplored, despite their growing adoption in real-world applications. In this paper, we present the first systematic jailbreak evaluation of DeepSeek-series models, comparing them with GPT-3.5 and GPT-4 using the HarmBench benchmark. We evaluate seven representative attack strategies across 510 harmful behaviors categorized by both function and semantic domain. Our analysis reveals that DeepSeek's Mixture-of-Experts (MoE) architecture introduces routing sparsity that offers selective robustness against optimization-based attacks such as TAP-T, but leads to significantly higher vulnerability under prompt-based and manually engineered attacks. In contrast, GPT-4 Turbo demonstrates stronger and more consistent safety alignment across diverse behaviors, likely due to its dense Transformer design and reinforcement learning from human feedback. Fine-grained behavioral analysis and case studies further show that DeepSeek often routes adversarial prompts to under-aligned expert modules, resulting in inconsistent refusal behaviors. These findings highlight a fundamental trade-off between architectural efficiency and alignment generalization, emphasizing the need for targeted safety tuning and modular alignment strategies to ensure secure deployment of open-source LLMs.",大型语言模型（LLM）的广泛部署引发了对其在越狱攻击（即绕过对齐机制并引发有害或违反政策输出的对抗性提示）的脆弱性的关键关注。虽然像GPT-4这样的专有模型经过广泛评估，但像DeepSeek这样的新兴开源替代方案的鲁棒性仍然大部分未被探索，尽管它们在实际应用中越来越受欢迎。在本文中，我们提出了首个系统性的DeepSeek系列模型的越狱评估，并与GPT-3.5和GPT-4进行比较，使用HarmBench基准。我们评估了七种代表性攻击策略，跨越510种有害行为，按功能和语义域分类。我们的分析揭示了DeepSeek的专家混合（MoE）架构引入了路由稀疏性，这在优化基础攻击（如TAP-T）中提供了选择性的鲁棒性，但在基于提示和手工工程的攻击中导致了显著更高的脆弱性。相比之下，GPT-4 Turbo在各种行为中表现出更强和更一致的安全对齐，可能是由于其密集的Transformer设计和人类反馈的强化学习。细粒度行为分析和案例研究进一步表明，DeepSeek通常将对抗性提示路由到未对齐的专家模块，导致不一致的拒绝行为。这些发现强调了架构效率与对齐泛化之间的基本权衡，强调了有针对性的安全调整和模块化对齐策略的需要，以确保开源LLM的安全部署。,"The paper evaluates the vulnerability of DeepSeek and GPT series models to jailbreak attacks, highlighting the trade-offs between architectural efficiency and alignment generalization in LLMs.",LLM,Harmless,"Jailbreak attacks, LLM alignment, safety, DeepSeek, GPT"
A Modular Taxonomy for Hate Speech Definitions and Its Impact on Zero-Shot LLM Classification Performance,"Matteo Melis, Gabriella Lapesa, Dennis Assenmacher",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.18576.pdf,"Detecting harmful content is a crucial task in the landscape of NLP applications for Social Good, with hate speech being one of its most dangerous forms. But what do we mean by hate speech, how can we define it, and how does prompting different definitions of hate speech affect model performance? The contribution of this work is twofold. At the theoretical level, we address the ambiguity surrounding hate speech by collecting and analyzing existing definitions from the literature. We organize these definitions into a taxonomy of 14 Conceptual Elements-building blocks that capture different aspects of hate speech definitions, such as references to the target of hate (individual or groups) or of the potential consequences of it. At the experimental level, we employ the collection of definitions in a systematic zero-shot evaluation of three LLMs, on three hate speech datasets representing different types of data (synthetic, human-in-the-loop, and real-world). We find that choosing different definitions, i.e., definitions with a different degree of specificity in terms of encoded elements, impacts model performance, but this effect is not consistent across all architectures.",检测有害内容是自然语言处理应用中的一个关键任务，仇恨言论是其最危险的形式之一。但我们所说的仇恨言论是什么，如何定义它，以及如何提示不同的仇恨言论定义会影响模型的性能？本文的贡献是双重的。在理论层面上，我们通过收集和分析文献中的现有定义来解决仇恨言论的模糊性。我们将这些定义组织成一个由14个概念元素组成的分类法，这些元素捕捉了仇恨言论定义的不同方面，例如对仇恨目标（个人或群体）或其潜在后果的引用。在实验层面上，我们在三个代表不同类型数据（合成、人机协作和真实世界）的仇恨言论数据集上，系统地对三个LLM进行零样本评估。我们发现，选择不同的定义，即在编码元素方面具有不同程度具体性的定义，会影响模型的性能，但这种效果在所有架构中并不一致。,The paper explores how different definitions of hate speech affect the performance of LLMs in zero-shot classification tasks.,LLM,Harmless,"Hate speech, LLM classification, zero-shot learning, harmful content, taxonomy"
"Reply to ""Emergent LLM behaviors are observationally equivalent to data leakage""","Ariel Flint Ashery, Luca Maria Aiello, Andrea Baronchelli",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.18600.pdf,"A potential concern when simulating populations of large language models (LLMs) is data contamination, i.e. the possibility that training data may shape outcomes in unintended ways. While this concern is important and may hinder certain experiments with multi-agent models, it does not preclude the study of genuinely emergent dynamics in LLM populations. The recent critique by Barrie and T\""ornberg [1] of the results of Flint Ashery et al. [2] offers an opportunity to clarify that self-organisation and model-dependent emergent dynamics can be studied in LLM populations, highlighting how such dynamics have been empirically observed in the specific case of social conventions.","在模拟大型语言模型（LLM）群体时，数据污染是一个潜在的问题，即训练数据可能以意想不到的方式影响结果。虽然这种担忧很重要，可能会妨碍某些多智能体模型的实验，但它并不排除在LLM群体中研究真正的突现动态。Barrie和T\""ornberg最近对Flint Ashery等人的结果的批评[1]提供了一个机会，澄清了自组织和模型依赖的突现动态可以在LLM群体中研究，突出了这种动态在特定情况下的社会习惯的实证观察。",The paper addresses concerns about data contamination in LLM populations and discusses the study of emergent dynamics in such models.,LLM,None,"Emergent behaviors, data contamination, LLM populations, self-organisation, social conventions"
The Anatomy of Speech Persuasion: Linguistic Shifts in LLM-Modified Speeches,"Alisa Barkar, Mathieu Chollet, Matthieu Labeau, Beatrice Biancardi, Chloe Clavel",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.18621.pdf,"This study examines how large language models understand the concept of persuasiveness in public speaking by modifying speech transcripts from PhD candidates in the ""Ma These en 180 Secondes"" competition, using the 3MT French dataset. Our contributions include a novel methodology and an interpretable textual feature set integrating rhetorical devices and discourse markers. We prompt GPT-4o to enhance or diminish persuasiveness and analyze linguistic shifts between original and generated speech in terms of the new features. Results indicate that GPT-4o applies systematic stylistic modifications rather than optimizing persuasiveness in a human-like manner. Notably, it manipulates emotional lexicon and syntactic structures (such as interrogative and exclamatory clauses) to amplify rhetorical impact.",这项研究通过修改“Ma These en 180 Secondes”比赛中博士候选人的演讲稿，探讨了大型语言模型如何理解公共演讲中的说服力。我们的贡献包括一种新的方法和一个可解释的文本特征集，集成了修辞手法和话语标记。我们提示GPT-4o增强或减少说服力，并分析原始和生成演讲之间的语言变化。结果表明，GPT-4o应用系统的风格修改，而不是以人类的方式优化说服力。值得注意的是，它操纵情感词汇和句法结构（如疑问句和感叹句）以放大修辞效果。,"The study investigates how large language models modify speeches to enhance persuasiveness, revealing systematic stylistic changes but not necessarily human-like optimization.",LLM,Helpful,"Persuasiveness, Speech Modification, LLM, Rhetorical Devices, Linguistic Shifts"
AggTruth: Contextual Hallucination Detection using Aggregated Attention Scores in LLMs,"Piotr Matys, Jan Eliasz, Konrad Kie{\l}czy\'nski, Miko{\l}aj Langner, Teddy Ferdinan, Jan Koco\'n, Przemys{\l}aw Kazienko",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.18628.pdf,"In real-world applications, Large Language Models (LLMs) often hallucinate, even in Retrieval-Augmented Generation (RAG) settings, which poses a significant challenge to their deployment. In this paper, we introduce AggTruth, a method for online detection of contextual hallucinations by analyzing the distribution of internal attention scores in the provided context (passage). Specifically, we propose four different variants of the method, each varying in the aggregation technique used to calculate attention scores. Across all LLMs examined, AggTruth demonstrated stable performance in both same-task and cross-task setups, outperforming the current SOTA in multiple scenarios. Furthermore, we conducted an in-depth analysis of feature selection techniques and examined how the number of selected attention heads impacts detection performance, demonstrating that careful selection of heads is essential to achieve optimal results.",在实际应用中，大型语言模型（LLMs）往往会出现幻觉，即使在检索增强生成（RAG）设置中也是如此，这对其部署构成了重大挑战。在这篇论文中，我们介绍了 AggTruth，一种通过分析提供的上下文（段落）中的内部注意力得分分布来进行上下文幻觉在线检测的方法。具体来说，我们提出了四种不同的方法变体，每种变体在用于计算注意力得分的聚合技术上都有所不同。在所有检查的 LLMs 中，AggTruth 在同一任务和跨任务设置中都表现出稳定的性能，在多个场景中超过了当前的最先进方法。此外，我们对特征选择技术进行了深入分析，并研究了所选注意力头的数量如何影响检测性能，证明了仔细选择头是实现最佳结果的关键。,"The paper presents AggTruth, a method for detecting contextual hallucinations in LLMs by analyzing attention scores, and demonstrates its effectiveness across various tasks and models.",LLM,Harmless,"Hallucination detection, attention scores, LLM, RAG, feature selection"
ReDit: Reward Dithering for Improved LLM Policy Optimization,"Chenxing Wei, Jiarui Yu, Ying Tiffany He, Hande Dong, Yao Shu, Fei Yu",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.18631.pdf,"DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning capabilities through its rule-based reward system. While it's a ''perfect'' reward system that effectively mitigates reward hacking, such reward functions are often discrete. Our experimental observations suggest that discrete rewards can lead to gradient anomaly, unstable optimization, and slow convergence. To address this issue, we propose ReDit (Reward Dithering), a method that dithers the discrete reward signal by adding simple random noise. With this perturbed reward, exploratory gradients are continuously provided throughout the learning process, enabling smoother gradient updates and accelerating convergence. The injected noise also introduces stochasticity into flat reward regions, encouraging the model to explore novel policies and escape local optima. Experiments across diverse tasks demonstrate the effectiveness and efficiency of ReDit. On average, ReDit achieves performance comparable to vanilla GRPO with only approximately 10% the training steps, and furthermore, still exhibits a 4% performance improvement over vanilla GRPO when trained for a similar duration. Visualizations confirm significant mitigation of gradient issues with ReDit. Moreover, theoretical analyses are provided to further validate these advantages.",DeepSeek-R1 通过其基于规则的奖励系统成功增强了大型语言模型（LLM）的推理能力。虽然这是一个有效地减少奖励作弊的“完美”奖励系统，但这种奖励函数通常是离散的。我们的实验观察表明，离散奖励可能导致梯度异常、优化不稳定和收敛缓慢。为了解决这个问题，我们提出了 ReDit（奖励抖动），一种通过添加简单随机噪声来抖动离散奖励信号的方法。通过这种扰动的奖励，探索梯度在整个学习过程中连续提供，使梯度更新更加平滑并加速收敛。注入的噪声还在平坦的奖励区域引入了随机性，鼓励模型探索新策略并逃离局部最优。跨多种任务的实验证明了 ReDit 的有效性和高效性。平均而言，ReDit 在仅约 10% 的训练步骤内实现了与纯粹 GRPO 相媲美的性能，并且在训练时间相似的情况下，仍然在性能上表现出 4% 的提升。可视化确认了 ReDit 显著缓解了梯度问题。此外，还提供了理论分析以进一步验证这些优势。,"The paper introduces ReDit, a method that adds random noise to discrete reward signals to improve the optimization and alignment of LLMs.",LLM,Helpful,"Reward Dithering, LLM Optimization, Gradient Stability, Convergence, Policy Exploration"
Semantic-Preserving Adversarial Attacks on LLMs: An Adaptive Greedy Binary Search Approach,"Chong Zhang, Xiang Li, Jia Wang, Shan Liang, Haochen Xue, Xiaobo Jin",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.18756.pdf,"Large Language Models (LLMs) increasingly rely on automatic prompt engineering in graphical user interfaces (GUIs) to refine user inputs and enhance response accuracy. However, the diversity of user requirements often leads to unintended misinterpretations, where automated optimizations distort original intentions and produce erroneous outputs. To address this challenge, we propose the Adaptive Greedy Binary Search (AGBS) method, which simulates common prompt optimization mechanisms while preserving semantic stability. Our approach dynamically evaluates the impact of such strategies on LLM performance, enabling robust adversarial sample generation. Through extensive experiments on open and closed-source LLMs, we demonstrate AGBS's effectiveness in balancing semantic consistency and attack efficacy. Our findings offer actionable insights for designing more reliable prompt optimization systems. Code is available at: https://github.com/franz-chang/DOBS",大语言模型（LLMs）越来越依赖图形用户界面（GUIs）中的自动提示工程来优化用户输入并提高响应准确性。然而，用户需求的多样性往往导致自动优化扭曲原始意图并产生错误输出。为了应对这一挑战，我们提出了自适应贪心二分搜索（AGBS）方法，该方法在保持语义稳定性的同时模拟常见的提示优化机制。我们的方法动态评估这些策略对LLM性能的影响，从而实现强大的对抗样本生成。通过对开源和闭源LLM的广泛实验，我们证明了AGBS在平衡语义一致性和攻击效能方面的有效性。我们的发现为设计更可靠的提示优化系统提供了可操作的见解。代码可在以下链接找到：https://github.com/franz-chang/DOBS,"The paper introduces the AGBS method to generate adversarial samples for LLMs while maintaining semantic stability, aiming to improve the reliability of prompt optimization systems.",LLM,Harmless,"Adversarial attacks, LLMs, Prompt engineering, Semantic stability, AGBS"
ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs,"Jiaru Zou, Ling Yang, Jingwen Gu, Jiahao Qiu, Ke Shen, Jingrui He, Mengdi Wang",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.18896.pdf,"Process Reward Models (PRMs) have recently emerged as a powerful framework for supervising intermediate reasoning steps in large language models (LLMs). Previous PRMs are primarily trained on model final output responses and struggle to evaluate intermediate thinking trajectories robustly, especially in the emerging setting of trajectory-response outputs generated by frontier reasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a novel trajectory-aware PRM explicitly designed to evaluate the trajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both step-level and trajectory-level supervision, enabling fine-grained reward assignment aligned with structured chain-of-thought data. We adapt ReasonFlux-PRM to support reward supervision under both offline and online settings, including (i) selecting high-quality model distillation data for downstream supervised fine-tuning of smaller models, (ii) providing dense process-level rewards for policy optimization during reinforcement learning, and (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results on challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond demonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs (e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our derived ReasonFlux-PRM-7B yields consistent performance improvements, achieving average gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement learning, and 6.3% in test-time scaling. We also release our efficient ReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment. Projects: https://github.com/Gen-Verse/ReasonFlux",过程奖励模型（PRMs）最近作为一种强大的框架，用于监督大型语言模型（LLMs）中的中间推理步骤。之前的PRMs主要基于模型的最终输出响应进行训练，难以在生成前沿推理模型（如Deepseek-R1）的轨迹-响应输出中稳健地评估中间思维轨迹。在本工作中，我们引入了ReasonFlux-PRM，一种专门设计用于评估轨迹-响应类型推理轨迹的新型轨迹感知PRM。ReasonFlux-PRM结合了步级和轨迹级的监督，使其能够与结构化的思维链数据对齐，进行细粒度的奖励分配。我们将ReasonFlux-PRM适配以支持离线和在线设置下的奖励监督，包括（i）选择高质量的模型蒸馏数据，用于下游的监督微调较小的模型，（ii）为策略优化期间的强化学习提供密集的过程级奖励，以及（iii）启用奖励指导的最佳N测试时扩展。在AIME、MATH500和GPQA-Diamond等具有挑战性的下游基准测试中，实验结果表明，ReasonFlux-PRM-7B选择的数据质量高于强大的PRMs（例如，Qwen2.5-Math-PRM-72B）和人工筛选的基线。此外，我们推导出的ReasonFlux-PRM-7B在监督微调、强化学习和测试时扩展中实现了平均提升，分别为12.1%、4.5%和6.3%。我们还发布了我们高效的ReasonFlux-PRM-1.5B，用于资源受限的应用和边缘部署。项目：https://github.com/Gen-Verse/ReasonFlux,"The paper introduces ReasonFlux-PRM, a trajectory-aware reward model for evaluating and improving the reasoning steps in large language models.",LLM,Helpful,"Reasoning, Trajectory, Reward Model, Alignment, Chain-of-Thought"
Expanding Relevance Judgments for Medical Case-based Retrieval Task with Multimodal LLMs,"Catarina Pires, S\'ergio Nunes, Lu\'is Filipe Teixeira",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.17782.pdf,"Evaluating Information Retrieval (IR) systems relies on high-quality manual relevance judgments (qrels), which are costly and time-consuming to obtain. While pooling reduces the annotation effort, it results in only partially labeled datasets. Large Language Models (LLMs) offer a promising alternative to reducing reliance on manual judgments, particularly in complex domains like medical case-based retrieval, where relevance assessment requires analyzing both textual and visual information. In this work, we explore using a Multimodal Large Language Model (MLLM) to expand relevance judgments, creating a new dataset of automated judgments. Specifically, we employ Gemini 1.5 Pro on the ImageCLEFmed 2013 case-based retrieval task, simulating human assessment through an iteratively refined, structured prompting strategy that integrates binary scoring, instruction-based evaluation, and few-shot learning. We systematically experimented with various prompt configurations to maximize agreement with human judgments. To evaluate agreement between the MLLM and human judgments, we use Cohen's Kappa, achieving a substantial agreement score of 0.6, comparable to inter-annotator agreement typically observed in multimodal retrieval tasks. Starting from the original 15,028 manual judgments (4.72% relevant) across 35 topics, our MLLM-based approach expanded the dataset by over 37x to 558,653 judgments, increasing relevant annotations to 5,950. On average, each medical case query received 15,398 new annotations, with approximately 99% being non-relevant, reflecting the high sparsity typical in this domain. Our results demonstrate the potential of MLLMs to scale relevance judgment collection, offering a promising direction for supporting retrieval evaluation in medical and multimodal IR tasks.","评估信息检索（IR）系统依赖于高质量的手动相关性判断（qrels），这些判断费时费力。虽然池化减少了注释努力，但只会导致部分标记的数据集。大型语言模型（LLMs）提供了一种有前途的替代方案，减少对手动判断的依赖，特别是在复杂领域，如医疗病例基础检索，相关性评估需要分析文本和视觉信息。在本工作中，我们探讨了使用多模态大型语言模型（MLLM）扩展相关性判断，创建一个新的自动判断数据集。具体来说，我们在ImageCLEFmed 2013病例基础检索任务中使用Gemini 1.5 Pro，通过一个迭代精炼的、结构化的提示策略来模拟人类评估，该策略集成了二进制评分、基于指令的评估和少量学习。我们系统地实验了各种提示配置，以最大化与人类判断的一致性。为了评估MLLM与人类判断之间的一致性，我们使用Cohen的Kappa，达到与多模态检索任务中通常观察到的注释者之间的一致性相似的显著一致性得分0.6。从原始的15,028个手动判断（4.72%相关）跨越35个主题，我们的MLLM方法将数据集扩展了超过37倍，达到558,653个判断，增加相关注释到5,950个。平均每个医疗病例查询收到15,398个新注释，其中约99%是非相关的，反映了该领域典型的高稀疏性。我们的结果表明，MLLMs有潜力扩展相关性判断收集，为支持医疗和多模态IR任务的检索评估提供了一个有前途的方向。","The paper explores using Multimodal Large Language Models to expand relevance judgments in medical case-based retrieval tasks, achieving substantial agreement with human judgments.",MLLM,Helpful,"Relevance Judgments, Medical Retrieval, Multimodal LLMs, Human Alignment, Evaluation"
LettinGo: Explore User Profile Generation for Recommendation System,"Lu Wang, Di Zhang, Fangkai Yang, Pu Zhao, Jianfeng Liu, Yuefeng Zhan, Hao Sun, Qingwei Lin, Weiwei Deng, Dongmei Zhang, Feng Sun, Qi Zhang",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.18309.pdf,"User profiling is pivotal for recommendation systems, as it transforms raw user interaction data into concise and structured representations that drive personalized recommendations. While traditional embedding-based profiles lack interpretability and adaptability, recent advances with large language models (LLMs) enable text-based profiles that are semantically richer and more transparent. However, existing methods often adhere to fixed formats that limit their ability to capture the full diversity of user behaviors. In this paper, we introduce LettinGo, a novel framework for generating diverse and adaptive user profiles. By leveraging the expressive power of LLMs and incorporating direct feedback from downstream recommendation tasks, our approach avoids the rigid constraints imposed by supervised fine-tuning (SFT). Instead, we employ Direct Preference Optimization (DPO) to align the profile generator with task-specific performance, ensuring that the profiles remain adaptive and effective. LettinGo operates in three stages: (1) exploring diverse user profiles via multiple LLMs, (2) evaluating profile quality based on their impact in recommendation systems, and (3) aligning the profile generation through pairwise preference data derived from task performance. Experimental results demonstrate that our framework significantly enhances recommendation accuracy, flexibility, and contextual awareness. This work enhances profile generation as a key innovation for next-generation recommendation systems.",用户画像对推荐系统至关重要，因为它将原始用户交互数据转化为简洁且结构化的表示，从而驱动个性化推荐。虽然传统的基于嵌入的用户画像缺乏可解释性和适应性，但最近的大语言模型（LLMs）进展使得基于文本的用户画像更加丰富和透明。然而，现有方法通常遵循固定格式，限制了其捕捉用户行为多样性的能力。在本文中，我们提出了LettinGo，一种用于生成多样化和适应性用户画像的新框架。通过利用LLMs的表达能力，并结合下游推荐任务的直接反馈，我们的方法避免了监督微调（SFT）强加的严格限制。相反，我们采用直接偏好优化（DPO）将用户画像生成器与特定任务的性能对齐，确保用户画像保持适应性和有效性。LettinGo在三个阶段运作：(1)通过多个LLMs探索多样化的用户画像，(2)根据其在推荐系统中的影响评估用户画像质量，(3)通过从任务性能中派生的成对偏好数据对用户画像生成进行对齐。实验结果表明，我们的框架显著提高了推荐的准确性、灵活性和上下文意识。本文作为下一代推荐系统的关键创新，增强了用户画像生成。,"The paper introduces LettinGo, a framework using LLMs and Direct Preference Optimization to generate adaptive and effective user profiles for recommendation systems.",LLM,Helpful,"User profiling, recommendation systems, LLMs, Direct Preference Optimization, adaptive profiles"
Use Property-Based Testing to Bridge LLM Code Generation and Validation,"Lehan He, Zeren Chen, Zhe Zhang, Jing Shao, Xiang Gao, Lu Sheng",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.18315.pdf,"Large Language Models (LLMs) excel at code generation, but ensuring their outputs to be functionally correct, especially in complex programming tasks, is a persistent challenge. While traditional Test-Driven Development (TDD) offers a path for code refinement, its efficacy with LLMs is often undermined by the scarcity of high-quality test cases or the pitfalls of automated test generation, including biased tests or inaccurate output predictions that can misdirect the correction process. This paper introduces Property-Generated Solver, a novel framework that leverages Property-Based Testing (PBT) to validate high-level program properties or invariants, instead of relying on specific input-output examples. These properties are often simpler to define and verify than directly predicting exhaustive test oracles, breaking the ""cycle of self-deception"" where tests might share flaws with the code they are meant to validate. Property-Generated Solver employs two collaborative LLM-based agents: a Generator dedicated to code generation and iterative refinement, and a Tester that manages the PBT life-cycle and formulate semantically rich feedback from property violations. The resulting comprehensive and actionable feedback then guides the Generator in its refinement efforts. By establishing PBT as the core validation engine within this iterative, closed-loop paradigm, Property-Generated Solver provides a robust mechanism for steering LLMs towards more correct and generalizable code. Extensive experimental results on multiple code generation benchmarks demonstrate that Property-Generated Solver achieves substantial pass@1 improvements, ranging from 23.1% to 37.3% relative gains over established TDD methods.",大语言模型（LLMs）在代码生成方面表现出色，但在复杂编程任务中确保其输出功能正确性仍然是一个持续的挑战。虽然传统的测试驱动开发（TDD）为代码精炼提供了一条途径，但其在LLMs中的有效性往往因高质量测试用例的稀缺或自动化测试生成的陷阱而受到削弱，包括偏见测试或不准确的输出预测，这些可能会误导纠正过程。本文介绍了Property-Generated Solver，一种新颖的框架，利用属性驱动测试（PBT）来验证高级程序属性或不变量，而不是依赖于特定的输入输出示例。这些属性通常比直接预测详尽的测试或预言更容易定义和验证，打破了“自我欺骗”循环，其中测试可能与其预期验证的代码共享缺陷。Property-Generated Solver采用两个协作的基于LLM的代理：一个专注于代码生成和迭代精炼的生成器，以及一个管理PBT生命周期并从属性违规中制定语义丰富反馈的测试器。结果的全面且可操作的反馈然后指导生成器在其精炼努力中。通过将PBT作为该迭代、封闭循环范式中的核心验证引擎，Property-Generated Solver为引导LLMs朝着更正确和可概括的代码提供了一种强大的机制。在多个代码生成基准测试中的广泛实验结果表明，Property-Generated Solver在相对于建立的TDD方法的显著通过@1改进，范围从23.1%到37.3%。,The paper presents a framework using Property-Based Testing to improve the correctness and generalizability of code generated by Large Language Models.,LLM,Helpful,"Property-Based Testing, Code Generation, Validation, LLM, Feedback"
A Survey on Data Selection for LLM Instruction Tuning,"Bolin Zhang, Jiahao Wang, Qianlong Du, Jiajun Zhang, Zhiying Tu, Dianhui Chu",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2402.05123.pdf,"Instruction tuning is a vital step of training large language models (LLM), so how to enhance the effect of instruction tuning has received increased attention. Existing works indicate that the quality of the dataset is more crucial than the quantity during instruction tuning of LLM. Therefore, recently a lot of studies focus on exploring the methods of selecting high-quality subset from instruction datasets, aiming to reduce training costs and enhance the instruction-following capabilities of LLMs. This paper presents a comprehensive survey on data selection for LLM instruction tuning. Firstly, we introduce the wildly used instruction datasets. Then, we propose a new taxonomy of the data selection methods and provide a detailed introduction of recent advances,and the evaluation strategies and results of data selection methods are also elaborated in detail. Finally, we emphasize the open challenges and present new frontiers of this task.",指令微调是训练大型语言模型（LLM）的重要步骤，因此如何增强指令微调的效果已经引起了越来越多的关注。现有的研究表明，在LLM的指令微调过程中，数据集的质量比数量更为重要。因此，最近有很多研究专注于从指令数据集中探索选择高质量子集的方法，旨在减少训练成本并增强LLM的指令遵循能力。本文对LLM指令微调的数据选择进行了全面的综述。首先，我们介绍了广泛使用的指令数据集。然后，我们提出了一种新的数据选择方法的分类法，并详细介绍了最近的进展，数据选择方法的评估策略和结果也详细阐述。最后，我们强调了这个任务的开放挑战，并提出了新的研究前沿。,This paper surveys data selection methods for instruction tuning of LLMs to improve their instruction-following capabilities.,LLM,"Helpful, Honest","Instruction tuning, data selection, LLM, high-quality dataset, evaluation strategies"
Evaluating LLMs with Multiple Problems at once,"Zhengxiang Wang, Jordan Kodner, Owen Rambow",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2406.10786.pdf,"This paper shows the benefits and fruitfulness of evaluating LLMs with multiple problems at once, a paradigm we call multi-problem evaluation (MPE). Unlike conventional single-problem evaluation, where a prompt presents a single problem and expects one specific answer, MPE places multiple problems together in a single prompt and assesses how well an LLM answers all these problems in a single output. Leveraging 6 classification and 12 reasoning benchmarks that already exist, we introduce a new benchmark called ZeMPE (Zero-shot Multi-Problem Evaluation), comprising 53,100 zero-shot multi-problem prompts. We experiment with a total of 13 LLMs from 5 model families on ZeMPE to present a comprehensive and systematic MPE. Our results show that LLMs are capable of handling multiple problems from a single data source as well as handling them separately, but there are conditions this multiple problem handling capability falls short. In addition, we perform in-depth further analyses and explore model-level factors that may enable multiple problem handling capabilities in LLMs. We release our corpus and code to facilitate future research.","这篇论文展示了同时用多个问题评估大型语言模型（LLM）的好处和成果，我们称之为多问题评估（MPE）。与传统的单问题评估不同，单问题评估中，一个提示只呈现一个问题并期望一个特定的答案，MPE将多个问题放在一个提示中，并评估LLM在一个输出中回答所有这些问题的能力。利用已经存在的6个分类和12个推理基准，我们引入了一个新的基准，称为ZeMPE（零样本多问题评估），包括53,100个零样本多问题提示。我们在ZeMPE上实验了总共13个LLM，来自5个模型家族，以呈现一个全面和系统的MPE。我们的结果表明，LLM能够处理来自单个数据源的多个问题，也能够分别处理它们，但有条件下这种多问题处理能力会失效。此外，我们进行了深入的进一步分析，并探索了可能使LLM具有多问题处理能力的模型级因素。我们发布了我们的语料库和代码，以促进未来的研究。","The paper introduces a new benchmark for evaluating LLMs using multiple problems at once, showing that while LLMs can handle multiple problems, their capabilities have certain limitations.",LLM,Helpful,"Multi-problem evaluation, LLM capabilities, benchmarking, zero-shot learning, model analysis"
Anthropocentric bias in language model evaluation,"Rapha\""el Milli\`ere, Charles Rathkopf",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2407.03859.pdf,"Evaluating the cognitive capacities of large language models (LLMs) requires overcoming not only anthropomorphic but also anthropocentric biases. This article identifies two types of anthropocentric bias that have been neglected: overlooking how auxiliary factors can impede LLM performance despite competence (""auxiliary oversight""), and dismissing LLM mechanistic strategies that differ from those of humans as not genuinely competent (""mechanistic chauvinism""). Mitigating these biases necessitates an empirically-driven, iterative approach to mapping cognitive tasks to LLM-specific capacities and mechanisms, which can be done by supplementing carefully designed behavioral experiments with mechanistic studies.",评估大型语言模型（LLM）的认知能力需要克服不仅是人类形态的偏见，还包括人类中心的偏见。本文识别了两种被忽视的人类中心偏见：忽视辅助因素可能会阻碍LLM的表现，尽管有能力（“辅助忽视”），以及拒绝LLM的机制策略与人类不同，认为不是真正有能力（“机制偏见”）。缓解这些偏见需要一种经验驱动的、迭代的方法，将认知任务映射到LLM特定的能力和机制，这可以通过补充精心设计的行为实验与机制研究来实现。,The paper identifies and addresses anthropocentric biases in evaluating large language models to improve their alignment with human expectations.,LLM,"Helpful, Harmless","Bias, Evaluation, Large Language Models, Mechanistic Strategies, Cognitive Tasks"
Self-Preference Bias in LLM-as-a-Judge,"Koki Wataoka, Tsubasa Takahashi, Ryokan Ri",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2410.21819.pdf,"Automated evaluation leveraging large language models (LLMs), commonly referred to as LLM evaluators or LLM-as-a-judge, has been widely used in measuring the performance of dialogue systems. However, the self-preference bias in LLMs has posed significant risks, including promoting specific styles or policies intrinsic to the LLMs. Despite the importance of this issue, there is a lack of established methods to measure the self-preference bias quantitatively, and its underlying causes are poorly understood. In this paper, we introduce a novel quantitative metric to measure the self-preference bias. Our experimental results demonstrate that GPT-4 exhibits a significant degree of self-preference bias. To explore the causes, we hypothesize that LLMs may favor outputs that are more familiar to them, as indicated by lower perplexity. We analyze the relationship between LLM evaluations and the perplexities of outputs. Our findings reveal that LLMs assign significantly higher evaluations to outputs with lower perplexity than human evaluators, regardless of whether the outputs were self-generated. This suggests that the essence of the bias lies in perplexity and that the self-preference bias exists because LLMs prefer texts more familiar to them.",自动化评估利用大型语言模型（LLM），通常称为LLM评估器或LLM-as-a-judge，已经被广泛用于测量对话系统的性能。然而，LLM中的自我偏好偏差带来了显著的风险，包括促进LLM固有的特定风格或政策。尽管这个问题很重要，但缺乏建立量化测量自我偏好偏差的方法，其根本原因也很少被理解。在本文中，我们引入了一种新的量化指标来测量自我偏好偏差。我们的实验结果表明，GPT-4表现出显著的自我偏好偏差。为了探索原因，我们假设LLM可能更喜欢对它们来说更熟悉的输出，这由较低的困惑度表明。我们分析了LLM评估与输出困惑度之间的关系。我们的发现表明，LLM对困惑度较低的输出给出了显著更高的评估，而人类评估者则不然，无论输出是否为自生成。这表明偏差的本质在于困惑度，并且自我偏好偏差存在的原因是LLM更喜欢对它们来说更熟悉的文本。,The paper introduces a metric to measure self-preference bias in LLMs used as evaluators and finds that LLMs tend to favor outputs with lower perplexity.,LLM,"Helpful, Honest","Self-preference bias, LLM evaluators, Perplexity, Bias measurement, GPT-4"
LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Inconsistencies,"Felix Friedrich, Simone Tedeschi, Patrick Schramowski, Manuel Brack, Roberto Navigli, Huu Nguyen, Bo Li, Kristian Kersting",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2412.15035.pdf,"Building safe Large Language Models (LLMs) across multiple languages is essential in ensuring both safe access and linguistic diversity. To this end, we conduct a large-scale, comprehensive safety evaluation of the current LLM landscape. For this purpose, we introduce M-ALERT, a multilingual benchmark that evaluates the safety of LLMs in five languages: English, French, German, Italian, and Spanish. M-ALERT includes 15k high-quality prompts per language, totaling 75k, with category-wise annotations. Our extensive experiments on 39 state-of-the-art LLMs highlight the importance of language-specific safety analysis, revealing that models often exhibit significant inconsistencies in safety across languages and categories. For instance, Llama3.2 shows high unsafety in category crime_tax for Italian but remains safe in other languages. Similar inconsistencies can be observed across all models. In contrast, certain categories, such as substance_cannabis and crime_propaganda, consistently trigger unsafe responses across models and languages. These findings underscore the need for robust multilingual safety practices in LLMs to ensure responsible usage across diverse communities.",构建跨多种语言的安全大型语言模型（LLM）对于确保安全访问和语言多样性至关重要。为此，我们对当前LLM的安全性进行了大规模、全面的评估。为此，我们引入了M-ALERT，一个评估LLM在五种语言（英语、法语、德语、意大利语和西班牙语）中的安全性的多语言基准。M-ALERT包括每种语言15000个高质量提示，总计75000个，并附有类别注释。我们在39个最先进的LLM上进行了广泛的实验，强调了语言特定安全分析的重要性，揭示了模型在语言和类别之间往往存在显著的安全不一致性。例如，Llama3.2在意大利语的犯罪税类别中显示出高不安全性，但在其他语言中保持安全。类似的不一致性可以在所有模型中观察到。相比之下，某些类别，如物质大麻和犯罪宣传，始终在模型和语言中触发不安全的响应。这些发现强调了在LLM中需要强大的多语言安全实践，以确保在多样化社区中负责任地使用。,"The paper introduces M-ALERT, a multilingual benchmark to evaluate and highlight safety inconsistencies in LLMs across different languages.",LLM,Harmless,"Multilingual safety, LLM evaluation, safety inconsistencies, cross-linguistic analysis, M-ALERT benchmark"
"ASCenD-BDS: Adaptable, Stochastic and Context-aware framework for Detection of Bias, Discrimination and Stereotyping","Rajiv Bahl, Venkatesan N, Parimal Aglawe, Aastha Sarasapalli, Bhavya Kancharla, Chaitanya kolukuluri, Harish Mohite, Japneet Hora, Kiran Kakollu, Rahul Dhiman, Shubham Kapale, Sri Bhagya Kathula, Vamsikrishna Motru, Yogeshwar Reddy",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2502.02072.pdf,"The rapid evolution of Large Language Models (LLMs) has transformed natural language processing but raises critical concerns about biases inherent in their deployment and use across diverse linguistic and sociocultural contexts. This paper presents a framework named ASCenD BDS (Adaptable, Stochastic and Context-aware framework for Detection of Bias, Discrimination and Stereotyping). The framework presents approach to detecting bias, discrimination, stereotyping across various categories such as gender, caste, age, disability, socioeconomic status, linguistic variations, etc., using an approach which is Adaptive, Stochastic and Context-Aware. The existing frameworks rely heavily on usage of datasets to generate scenarios for detection of Bias, Discrimination and Stereotyping. Examples include datasets such as Civil Comments, Wino Gender, WinoBias, BOLD, CrowS Pairs and BBQ. However, such an approach provides point solutions. As a result, these datasets provide a finite number of scenarios for assessment. The current framework overcomes this limitation by having features which enable Adaptability, Stochasticity, Context Awareness. Context awareness can be customized for any nation or culture or sub-culture (for example an organization's unique culture). In this paper, context awareness in the Indian context has been established. Content has been leveraged from Indian Census 2011 to have a commonality of categorization. A framework has been developed using Category, Sub-Category, STEM, X-Factor, Synonym to enable the features for Adaptability, Stochasticity and Context awareness. The framework has been described in detail in Section 3. Overall 800 plus STEMs, 10 Categories, 31 unique SubCategories were developed by a team of consultants at Saint Fox Consultancy Private Ltd. The concept has been tested out in SFCLabs as part of product development.",大型语言模型（LLMs）的快速发展改变了自然语言处理，但也引发了关于其在多种语言和社会文化背景下部署和使用中的偏见问题。本文提出了一种名为ASCenD BDS（适应性、随机性和上下文感知框架，用于检测偏见、歧视和刻板印象）的框架。该框架提出了一种检测偏见、歧视和刻板印象的方法，适用于性别、种姓、年龄、残疾、社会经济地位、语言变化等各种类别。现有框架主要依赖于使用数据集来生成检测偏见、歧视和刻板印象的场景。例如，数据集包括Civil Comments、Wino Gender、WinoBias、BOLD、CrowS Pairs和BBQ。然而，这种方法提供的是点解决方案。因此，这些数据集为评估提供了有限数量的场景。当前框架通过具有启用适应性、随机性和上下文感知的功能来克服这一限制。上下文感知可以为任何国家、文化或子文化（例如组织的独特文化）进行定制。本文在印度背景下建立了上下文感知。利用了印度2011年人口普查的内容，以实现分类的共性。开发了一个框架，使用类别、子类别、STEM、X因子和同义词，以启用适应性、随机性和上下文感知的功能。框架在第3节中详细描述。总共开发了800多个STEM、10个类别和31个独特的子类别，由圣福克斯咨询私人有限公司的顾问团队开发。该概念已在SFCLabs作为产品开发的一部分进行了测试。,"The paper introduces ASCenD-BDS, a framework for detecting bias, discrimination, and stereotyping in LLMs using an adaptive, stochastic, and context-aware approach.",LLM,Harmless,"Bias detection, discrimination, stereotyping, context-aware, large language models"
Compromising Honesty and Harmlessness in Language Models via Deception Attacks,"Laur\`ene Vaugrante, Francesca Carlon, Maluna Menke, Thilo Hagendorff",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2502.08301.pdf,"Recent research on large language models (LLMs) has demonstrated their ability to understand and employ deceptive behavior, even without explicit prompting. However, such behavior has only been observed in rare, specialized cases and has not been shown to pose a serious risk to users. Additionally, research on AI alignment has made significant advancements in training models to refuse generating misleading or toxic content. As a result, LLMs generally became honest and harmless. In this study, we introduce ""deception attacks"" that undermine both of these traits, revealing a vulnerability that, if exploited, could have serious real-world consequences. We introduce fine-tuning methods that cause models to selectively deceive users on targeted topics while remaining accurate on others. Through a series of experiments, we show that such targeted deception is effective even in high-stakes domains or ideologically charged subjects. In addition, we find that deceptive fine-tuning often compromises other safety properties: deceptive models are more likely to produce toxic content, including hate speech and stereotypes. Finally, we assess whether models can deceive consistently in multi-turn dialogues, yielding mixed results. Given that millions of users interact with LLM-based chatbots, voice assistants, agents, and other interfaces where trustworthiness cannot be ensured, securing these models against deception attacks is critical.",最近关于大型语言模型（LLM）的研究表明，它们能够理解和使用欺骗行为，即使没有明确的提示。然而，这种行为只在罕见的专门情况下观察到，并且尚未被证明对用户构成严重风险。此外，关于人工智能对齐的研究在训练模型拒绝生成误导性或有毒内容方面取得了显著进展。因此，LLM通常变得诚实和无害。在本研究中，我们引入了“欺骗攻击”，这些攻击破坏了这些特征，揭示了一个漏洞，如果被利用，可能会对现实世界产生严重后果。我们引入了细化方法，使模型在特定主题上选择性地欺骗用户，同时在其他方面保持准确。通过一系列实验，我们表明这种有针对性的欺骗在高风险领域或意识形态充满争议的主题中是有效的。此外，我们发现欺骗细化往往会损害其他安全属性：欺骗模型更有可能产生有毒内容，包括仇恨言论和刻板印象。最后，我们评估了模型在多轮对话中是否能够一致地欺骗，结果不一。鉴于数百万用户与基于LLM的聊天机器人、语音助手、代理和其他界面进行互动，其中信任度无法保证，因此确保这些模型免受欺骗攻击是至关重要的。,"The paper explores how deception attacks can compromise the honesty and harmlessness of large language models, highlighting a critical vulnerability.",LLM,"Honest, Harmless","Deception attacks, honesty, harmlessness, LLM vulnerability, fine-tuning"
Stop Overvaluing Multi-Agent Debate -- We Must Rethink Evaluation and Embrace Model Heterogeneity,"Hangfan Zhang, Zhiyao Cui, Jianhao Chen, Xinrun Wang, Qiaosheng Zhang, Zhen Wang, Dinghao Wu, Shuyue Hu",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2502.08788.pdf,"Multi-agent debate (MAD) has gained significant attention as a promising line of research to improve the factual accuracy and reasoning capabilities of large language models (LLMs). Despite its conceptual appeal, current MAD research suffers from critical limitations in evaluation practices, including limited benchmark coverage, weak baseline comparisons, and inconsistent setups. This paper presents a systematic evaluation of 5 representative MAD methods across 9 benchmarks using 4 foundational models. Surprisingly, our findings reveal that MAD often fail to outperform simple single-agent baselines such as Chain-of-Thought and Self-Consistency, even when consuming significantly more inference-time computation. To advance MAD research, we further explore the role of model heterogeneity and find it as a universal antidote to consistently improve current MAD frameworks. Based on our findings, we argue that the field must stop overvaluing MAD in its current form; for true advancement, we must critically rethink evaluation paradigms and actively embrace model heterogeneity as a core design principle.",多智能体辩论（MAD）作为一种改进大型语言模型（LLM）事实准确性和推理能力的有前途的研究方向，近年来受到了广泛关注。尽管其概念上具有吸引力，但当前的MAD研究在评估实践中存在严重的局限性，包括受限的基准覆盖范围、弱基准比较和不一致的设置。本文对5种代表性的MAD方法在9个基准测试中进行了系统评估，使用了4种基础模型。令人惊讶的是，我们的发现表明，MAD往往无法超越简单的单智能体基线，如思维链和自我一致性，即使在消耗显著更多的推理时间计算。为了推动MAD研究，我们进一步探讨了模型异构性的作用，并发现它是一种普遍的解药，可以始终如一地改善当前的MAD框架。根据我们的发现，我们认为该领域必须停止过高评价当前形式的MAD；为了真正的进步，我们必须批判性地重新思考评估范式，并积极拥抱模型异构性作为核心设计原则。,The paper critically evaluates multi-agent debate methods for improving LLMs and advocates for embracing model heterogeneity.,LLM,Helpful,"Multi-agent debate, evaluation, model heterogeneity, large language models, reasoning capabilities"
HiddenDetect: Detecting Jailbreak Attacks against Large Vision-Language Models via Monitoring Hidden States,"Yilei Jiang, Xinyan Gao, Tianshuo Peng, Yingshui Tan, Xiaoyong Zhu, Bo Zheng, Xiangyu Yue",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2502.14744.pdf,"The integration of additional modalities increases the susceptibility of large vision-language models (LVLMs) to safety risks, such as jailbreak attacks, compared to their language-only counterparts. While existing research primarily focuses on post-hoc alignment techniques, the underlying safety mechanisms within LVLMs remain largely unexplored. In this work , we investigate whether LVLMs inherently encode safety-relevant signals within their internal activations during inference. Our findings reveal that LVLMs exhibit distinct activation patterns when processing unsafe prompts, which can be leveraged to detect and mitigate adversarial inputs without requiring extensive fine-tuning. Building on this insight, we introduce HiddenDetect, a novel tuning-free framework that harnesses internal model activations to enhance safety. Experimental results show that {HiddenDetect} surpasses state-of-the-art methods in detecting jailbreak attacks against LVLMs. By utilizing intrinsic safety-aware patterns, our method provides an efficient and scalable solution for strengthening LVLM robustness against multimodal threats. Our code will be released publicly at https://github.com/leigest519/HiddenDetect.",多模态的集成增加了大型视觉语言模型（LVLMs）的安全风险，如越狱攻击，而现有研究主要集中在事后对齐技术。本文研究了LVLMs在推理过程中是否在其内部激活中编码了安全相关的信号。我们的发现表明，LVLMs在处理不安全提示时表现出独特的激活模式，可以利用这些模式来检测和缓解对抗性输入，而无需进行广泛的微调。基于这一洞察，我们引入了HiddenDetect，一个新颖的无需调整的框架，利用内部模型激活来增强安全性。实验结果表明，HiddenDetect在检测LVLMs的越狱攻击方面超过了现有最先进的方法。通过利用内在的安全感知模式，我们的方法为增强LVLM对多模态威胁的鲁棒性提供了高效且可扩展的解决方案。,"The paper introduces HiddenDetect, a framework that uses internal activations to detect and mitigate jailbreak attacks against large vision-language models.",LMM,Harmless,"Jailbreak attacks, safety, vision-language models, internal activations, detection"
ParamMute: Suppressing Knowledge-Critical FFNs for Faithful Retrieval-Augmented Generation,"Pengcheng Huang, Zhenghao Liu, Yukun Yan, Haiyan Zhao, Xiaoyuan Yi, Hao Chen, Zhiyuan Liu, Maosong Sun, Tong Xiao, Ge Yu, Chenyan Xiong",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2502.15543.pdf,"Large language models (LLMs) integrated with retrieval-augmented generation (RAG) have improved factuality by grounding outputs in external evidence. However, they remain susceptible to unfaithful generation, where outputs contradict retrieved context despite its relevance and accuracy. Existing approaches aiming to improve faithfulness primarily focus on enhancing the utilization of external context, but often overlook the persistent influence of internal parametric knowledge during generation. In this work, we investigate the internal mechanisms behind unfaithful generation and identify a subset of mid-to-deep feed-forward networks (FFNs) that are disproportionately activated in such cases. Building on this insight, we propose Parametric Knowledge Muting through FFN Suppression (ParamMute), a framework that improves contextual faithfulness by suppressing the activation of unfaithfulness-associated FFNs and calibrating the model toward retrieved knowledge. To evaluate our approach, we introduce CoFaithfulQA, a benchmark specifically designed to evaluate faithfulness in scenarios where internal knowledge conflicts with accurate external evidence. Experimental results show that ParamMute significantly enhances faithfulness across both CoFaithfulQA and the established ConFiQA benchmark, achieving substantial reductions in reliance on parametric memory. These findings underscore the importance of mitigating internal knowledge dominance and provide a new direction for improving LLM trustworthiness in RAG. All codes are available at https://github.com/OpenBMB/ParamMute.",大语言模型（LLMs）与检索增强生成（RAG）集成后，通过将输出基于外部证据来提高事实性。然而，它们仍然容易出现不忠实的生成，即输出与检索的上下文矛盾，尽管其相关性和准确性。现有的方法主要集中在提高外部上下文的利用率，但往往忽略了生成过程中内部参数知识的持续影响。在本研究中，我们研究了不忠实生成背后的内部机制，并识别出一组中到深的前馈神经网络（FFNs），这些网络在不忠实生成的情况下被不成比例地激活。基于这一洞察，我们提出了通过FFN抑制的参数知识静音（ParamMute），一种通过抑制与不忠实相关的FFNs的激活并校准模型以适应检索知识来提高上下文忠实性的框架。为了评估我们的方法，我们引入了CoFaithfulQA，这是一个专门设计用于评估在内部知识与准确外部证据冲突的情况下的忠实性的基准。实验结果表明，ParamMute在CoFaithfulQA和已建立的ConFiQA基准上显著提高了忠实性，实现了对参数内存依赖的显著减少。这些发现强调了缓解内部知识主导的重要性，并为改进RAG中LLM可信度提供了新的方向。所有代码均可在https://github.com/OpenBMB/ParamMute获得。,"The paper introduces ParamMute, a framework that enhances the faithfulness of LLMs in retrieval-augmented generation by suppressing internal parametric knowledge that conflicts with external evidence.",LLM,"Helpful, Honest","Faithfulness, Retrieval-Augmented Generation, Parametric Knowledge, FFN Suppression, LLM Alignment"
Language Models Grow Less Humanlike beyond Phase Transition,"Tatsuya Aoyama, Ethan Wilcox",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2502.18802.pdf,"LMs' alignment with human reading behavior (i.e. psychometric predictive power; PPP) is known to improve during pretraining up to a tipping point, beyond which it either plateaus or degrades. Various factors, such as word frequency, recency bias in attention, and context size, have been theorized to affect PPP, yet there is no current account that explains why such a tipping point exists, and how it interacts with LMs' pretraining dynamics more generally. We hypothesize that the underlying factor is a pretraining phase transition, characterized by the rapid emergence of specialized attention heads. We conduct a series of correlational and causal experiments to show that such a phase transition is responsible for the tipping point in PPP. We then show that, rather than producing attention patterns that contribute to the degradation in PPP, phase transitions alter the subsequent learning dynamics of the model, such that further training keeps damaging PPP.",语言模型的心理测量预测能力（PPP）在预训练过程中会改善，直到一个临界点，超过这个临界点，PPP 要么停滞不前，要么恶化。各种因素，如词频、注意力中的时效偏差和上下文大小，都被认为会影响 PPP，但目前没有一个解释为什么存在这样的临界点，以及它如何与语言模型的预训练动态相互作用。我们假设，这种临界点是由预训练阶段转变引起的，这种转变以专门注意力头的快速出现为特征。我们进行了一系列相关和因果实验，以表明这种阶段转变是 PPP 中临界点的原因。然后我们表明，阶段转变不仅产生注意力模式，这些模式有助于 PPP 的恶化，而且阶段转变改变了模型的后续学习动态，使得进一步的训练继续损害 PPP。,"The paper investigates why the alignment of language models with human reading behavior degrades beyond a certain point during pretraining, attributing it to a phase transition in the model's learning dynamics.",LLM,Helpful,"Alignment, Phase Transition, Pretraining, Attention Heads, Humanlike Behavior"
Decoding Safety Feedback from Diverse Raters: A Data-driven Lens on Responsiveness to Severity,"Pushkar Mishra, Charvi Rastogi, Stephen R. Pfohl, Alicia Parrish, Tian Huey Teh, Roma Patel, Mark Diaz, Ding Wang, Michela Paganini, Vinodkumar Prabhakaran, Lora Aroyo, Verena Rieser",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2503.05609.pdf,"Ensuring the safety of Generative AI requires a nuanced understanding of pluralistic viewpoints. In this paper, we introduce a novel data-driven approach for interpreting granular ratings in pluralistic datasets. Specifically, we address the challenge of analyzing nuanced differences in safety feedback from a diverse population expressed via ordinal scales (e.g., a Likert scale). We distill non-parametric responsiveness metrics that quantify the consistency of raters in scoring varying levels of the severity of safety violations. Leveraging a publicly available pluralistic dataset of safety feedback on AI-generated content as our case study, we investigate how raters from different demographic groups (age, gender, ethnicity) use an ordinal scale to express their perceptions of the severity of violations. We apply our metrics across violation types, demonstrating their utility in extracting nuanced insights that are crucial for aligning AI systems reliably in multi-cultural contexts. We show that our approach can inform rater selection and feedback interpretation by capturing nuanced viewpoints across different demographic groups, hence improving the quality of pluralistic data collection and in turn contributing to more robust AI development.",确保生成式人工智能的安全性需要对多样化观点有一个细致的理解。在本文中，我们引入了一种新颖的基于数据的方法，用于解释多样化数据集中的细粒度评分。具体来说，我们解决了分析来自不同人口群体的安全反馈的细微差异的挑战，这些反馈通过有序量表（例如，赖克特量表）表达。我们提取了非参数响应度指标，这些指标量化了评分者在评分安全违规的不同严重程度时的一致性。利用一个公开可用的多样化安全反馈数据集作为我们的案例研究，我们研究了来自不同人口群体（年龄、性别、种族）的评分者如何使用有序量表来表达他们对违规严重程度的感知。我们在违规类型上应用了我们的指标，展示了它们在提取细微见解方面的实用性，这些见解对于在多文化背景下可靠地对齐AI系统至关重要。我们表明，我们的方法可以通过捕捉不同人口群体的细微观点来指导评分者选择和反馈解释，从而提高多样化数据收集的质量，并最终促进更健壮的人工智能开发。,The paper presents a data-driven approach to analyze safety feedback from diverse raters to improve the alignment of AI systems in multicultural contexts.,LLM,Harmless,"Safety feedback, pluralistic viewpoints, ordinal scales, demographic groups, AI alignment"
PiCo: Jailbreaking Multimodal Large Language Models via $\textbf{Pi}$ctorial $\textbf{Co}$de Contextualization,"Aofan Liu, Lulu Tang, Ting Pan, Yuguo Yin, Bin Wang, Ao Yang",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2504.01444.pdf,"Multimodal Large Language Models (MLLMs), which integrate vision and other modalities into Large Language Models (LLMs), significantly enhance AI capabilities but also introduce new security vulnerabilities. By exploiting the vulnerabilities of the visual modality and the long-tail distribution characteristic of code training data, we present PiCo, a novel jailbreaking framework designed to progressively bypass multi-tiered defense mechanisms in advanced MLLMs. PiCo employs a tier-by-tier jailbreak strategy, using token-level typographic attacks to evade input filtering and embedding harmful intent within programming context instructions to bypass runtime monitoring. To comprehensively assess the impact of attacks, a new evaluation metric is further proposed to assess both the toxicity and helpfulness of model outputs post-attack. By embedding harmful intent within code-style visual instructions, PiCo achieves an average Attack Success Rate (ASR) of 84.13% on Gemini-Pro Vision and 52.66% on GPT-4, surpassing previous methods. Experimental results highlight the critical gaps in current defenses, underscoring the need for more robust strategies to secure advanced MLLMs.",多模态大语言模型（MLLMs），将视觉和其他模态整合到大语言模型（LLMs）中，显著增强了AI能力，但也引入了新的安全漏洞。通过利用视觉模态的漏洞和代码训练数据的长尾分布特性，我们提出了PiCo，一种新颖的越狱框架，旨在逐步绕过高级MLLMs中的多层防御机制。PiCo采用逐层越狱策略，使用基于令牌的排版攻击来规避输入过滤，并将有害意图嵌入到编程上下文指令中，以绕过运行时监控。为了全面评估攻击的影响，进一步提出了一种新的评估指标，以评估攻击后模型输出的毒性和有用性。通过在代码样式的视觉指令中嵌入有害意图，PiCo在Gemini-Pro Vision上实现了84.13%的攻击成功率（ASR），在GPT-4上实现了52.66%的攻击成功率，超过了以前的方法。实验结果突显了当前防御中的关键差距，强调了需要更强大的策略来保护高级MLLMs。,"The paper introduces PiCo, a framework for jailbreaking multimodal large language models by exploiting visual and code vulnerabilities, highlighting the need for robust security measures.",LLM,Harmless,"Jailbreaking, Security, Multimodal, Harmful Intent, Defense Mechanisms"
DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM Post-training,"Zhenting Wang, Guofeng Cui, Yu-Jhe Li, Kun Wan, Wentian Zhao",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2504.09710.pdf,"Recent advances in reinforcement learning (RL)-based post-training have led to notable improvements in large language models (LLMs), particularly in enhancing their reasoning capabilities to handle complex tasks. However, most existing methods treat the training data as a unified whole, overlooking the fact that modern LLM training often involves a mixture of data from diverse distributions-varying in both source and difficulty. This heterogeneity introduces a key challenge: how to adaptively schedule training across distributions to optimize learning efficiency. In this paper, we present a principled curriculum learning framework grounded in the notion of distribution-level learnability. Our core insight is that the magnitude of policy advantages reflects how much a model can still benefit from further training on a given distribution. Based on this, we propose a distribution-level curriculum learning framework for RL-based LLM post-training, which leverages the Upper Confidence Bound (UCB) principle to dynamically adjust sampling probabilities for different distrubutions. This approach prioritizes distributions with either high average advantage (exploitation) or low sample count (exploration), yielding an adaptive and theoretically grounded training schedule. We instantiate our curriculum learning framework with GRPO as the underlying RL algorithm and demonstrate its effectiveness on logic reasoning datasets with multiple difficulties and sources. Our experiments show that our framework significantly improves convergence speed and final performance, highlighting the value of distribution-aware curriculum strategies in LLM post-training. Code: https://github.com/ZhentingWang/DUMP.",最近，基于强化学习（RL）的后训练在大型语言模型（LLM）中取得了显著进展，特别是在增强其处理复杂任务的推理能力方面。然而，大多数现有方法将训练数据视为一个统一的整体，忽略了现代LLM训练通常涉及来自不同分布的数据混合的事实，这些数据在来源和难度上都有所不同。这种异质性引入了一个关键挑战：如何在分布之间适应性地安排训练，以优化学习效率。在本文中，我们提出了一种基于分布级可学习性的原则性课程学习框架。我们的核心洞见是政策优势的大小反映了模型在给定分布上进一步训练的潜在收益。基于此，我们提出了一种基于RL的LLM后训练的分布级课程学习框架，利用上置信界（UCB）原则动态调整不同分布的采样概率。这种方法优先考虑具有高平均优势（利用）或低样本计数（探索）的分布，从而产生一种适应性和理论上有根据的训练计划。我们将我们的课程学习框架实例化为基础RL算法GRPO，并在具有多种难度和来源的逻辑推理数据集上展示了其有效性。我们的实验表明，我们的框架显著提高了收敛速度和最终性能，突显了LLM后训练中分布感知课程策略的价值。,The paper introduces a distribution-level curriculum learning framework for RL-based LLM post-training to optimize learning efficiency and improve reasoning capabilities.,LLM,Helpful,"Curriculum learning, RL-based post-training, LLM, Distribution-level learnability, Policy advantages"
Learning from Reference Answers: Versatile Language Model Alignment without Binary Human Preference Data,"Shuai Zhao, Linchao Zhu, Yi Yang",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2504.09895.pdf,"Large language models~(LLMs) are expected to be helpful, harmless, and honest. In alignment scenarios such as safety, confidence, and general preference alignment, binary preference data collection and reward modeling are resource-intensive but essential for transferring human preference. In this work, we explore using the similarity between sampled generations and high-quality reference answers as an alternative reward function choice for LLM alignment. Similarity reward circumvents binary preference data collection and reward modeling when unary high-quality reference answers are available. We introduce \textit{RefAlign}, a versatile REINFORCE-style alignment algorithm that does not rely on reference or reward models. RefAlign utilizes similarity metrics, such as BERTScore between sampled generations and reference answers as surrogate rewards. Beyond general human preference optimization, RefAlign can be readily extended to diverse scenarios, such as safety and confidence alignment, by incorporating the similarity reward with task-related objectives. In various scenarios, RefAlign demonstrates comparable performance to previous alignment methods without binary preference data and reward models.",大语言模型（LLMs）被期望是有用的、无害的和诚实的。在安全性、信心和一般偏好对齐等对齐场景中，二元偏好数据收集和奖励建模是资源密集型但对将人类偏好转移至LLM至关重要。在本工作中，我们探索使用采样生成与高质量参考答案之间的相似性作为LLM对齐的替代奖励函数选择。相似性奖励绕过了二元偏好数据收集和奖励建模，当可用单元高质量参考答案时。我们引入了RefAlign，一种不依赖于参考或奖励模型的通用REINFORCE风格对齐算法。RefAlign利用采样生成与参考答案之间的相似度指标，如BERTScore作为代理奖励。除了一般人类偏好优化，RefAlign可以通过将相似性奖励与与任务相关的目标结合起来，轻松扩展到多种场景，如安全性和信心对齐。在各种场景中，RefAlign在没有二元偏好数据和奖励模型的情况下，表现出与先前对齐方法相媲美的性能。,"The paper introduces RefAlign, an algorithm for aligning large language models using reference answers as a reward function, demonstrating effectiveness in various scenarios without binary preference data.",LLM,"Helpful, Harmless, Honest","LLM alignment, reference answers, reward function, helpful, harmless, honest"
Deep Binding of Language Model Virtual Personas: a Study on Approximating Political Partisan Misperceptions,"Minwoo Kang, Suhong Moon, Seung Hyeong Lee, Ayush Raj, Joseph Suh, David M. Chan",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2504.11673.pdf,"Large language models (LLMs) are increasingly capable of simulating human behavior, offering cost-effective ways to estimate user responses to various surveys and polls. However, the questions in these surveys usually reflect socially understood attitudes: the patterns of attitudes of old/young, liberal/conservative, as understood by both members and non-members of those groups. It is not clear whether the LLM binding is \emph{deep}, meaning the LLM answers as a member of a particular in-group would, or \emph{shallow}, meaning the LLM responds as an out-group member believes an in-group member would. To explore this difference, we use questions that expose known in-group/out-group biases. This level of fidelity is critical for applying LLMs to various political science studies, including timely topics on polarization dynamics, inter-group conflict, and democratic backsliding. To this end, we propose a novel methodology for constructing virtual personas with synthetic user ``backstories"" generated as extended, multi-turn interview transcripts. Our generated backstories are longer, rich in detail, and consistent in authentically describing a singular individual, compared to previous methods. We show that virtual personas conditioned on our backstories closely replicate human response distributions (up to an 87\% improvement as measured by Wasserstein Distance) and produce effect sizes that closely match those observed in the original studies of in-group/out-group biases. Altogether, our work extends the applicability of LLMs beyond estimating socially understood responses, enabling their use in a broader range of human studies.",大型语言模型（LLMs）越来越能够模拟人类行为，提供一种成本效益高的方法来估计用户对各种调查和民意测验的反应。然而，这些调查中的问题通常反映了社会上理解的态度：老年人/年轻人、自由派/保守派的态度模式，无论是这些群体的成员还是非成员都能理解。尚不清楚LLM绑定是否是深层次的，即LLM作为特定内群体成员的回答，还是浅层次的，即LLM作为外群体成员认为内群体成员会回答的方式。为了探索这种差异，我们使用了揭示已知内群体/外群体偏见的问题。这种保真度对于将LLMs应用于各种政治科学研究至关重要，包括时事热点，如极化动态、群际冲突和民主倒退。为此，我们提出了一种新的方法，用于构建具有由扩展的、多轮面试记录生成的合成用户“背景故事”的虚拟角色。我们生成的背景故事比以前的方法更长、更详细，并且在真实描述单个个人方面更加一致。我们表明，基于我们的背景故事条件的虚拟角色几乎复制了人类反应分布（根据瓦瑟斯坦距离测量，改进了高达87%），并产生了与原始研究中观察到的效应大小几乎匹配的效应大小。总的来说，我们的工作扩展了LLMs的适用性，超出了估计社会上理解的反应，使其能够在更广泛的人类研究中使用。,"The paper introduces a method to create virtual personas for LLMs to better simulate human responses in political science studies, focusing on in-group/out-group biases.",LLM,None,"Large Language Models, Political Partisan Misperceptions, In-group/Out-group Biases, Virtual Personas, Survey Simulation"
TrumorGPT: Graph-Based Retrieval-Augmented Large Language Model for Fact-Checking,"Ching Nam Hang, Pei-Duo Yu, Chee Wei Tan",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2505.07891.pdf,"In the age of social media, the rapid spread of misinformation and rumors has led to the emergence of infodemics, where false information poses a significant threat to society. To combat this issue, we introduce TrumorGPT, a novel generative artificial intelligence solution designed for fact-checking in the health domain. TrumorGPT aims to distinguish ""trumors"", which are health-related rumors that turn out to be true, providing a crucial tool in differentiating between mere speculation and verified facts. This framework leverages a large language model (LLM) with few-shot learning for semantic health knowledge graph construction and semantic reasoning. TrumorGPT incorporates graph-based retrieval-augmented generation (GraphRAG) to address the hallucination issue common in LLMs and the limitations of static training data. GraphRAG involves accessing and utilizing information from regularly updated semantic health knowledge graphs that consist of the latest medical news and health information, ensuring that fact-checking by TrumorGPT is based on the most recent data. Evaluating with extensive healthcare datasets, TrumorGPT demonstrates superior performance in fact-checking for public health claims. Its ability to effectively conduct fact-checking across various platforms marks a critical step forward in the fight against health-related misinformation, enhancing trust and accuracy in the digital information age.",在社交媒体时代，虚假信息和谣言的迅速传播导致了信息泛滥的问题，虚假信息对社会构成了重大威胁。为了应对这一问题，我们引入了TrumorGPT，这是一种专为健康领域事实核查设计的新型生成式人工智能解决方案。TrumorGPT旨在区分“谣言”，即最终被证明为真实的健康相关谣言，提供了一种区分纯粹猜测和经过验证的事实的重要工具。该框架利用少量学习的大型语言模型（LLM）进行语义健康知识图谱构建和语义推理。TrumorGPT结合了基于图的检索增强生成（GraphRAG），以解决LLM中常见的幻觉问题和静态训练数据的局限性。GraphRAG涉及访问和利用定期更新的语义健康知识图谱，这些图谱包含最新的医疗新闻和健康信息，确保TrumorGPT的事实核查基于最新的数据。通过广泛的医疗数据集进行评估，TrumorGPT在公共卫生声明的事实核查中表现出色。其能够有效地在各种平台上进行事实核查，标志着在与健康相关的虚假信息的斗争中迈出了关键一步，增强了数字信息时代的信任和准确性。,"TrumorGPT is a LLM-based system designed for fact-checking health-related rumors, using graph-based retrieval to ensure accuracy and combat misinformation.",LLM,Honest,"Fact-checking, LLM, GraphRAG, Health misinformation, Semantic reasoning"
Prototypical Human-AI Collaboration Behaviors from LLM-Assisted Writing in the Wild,"Sheshera Mysore, Debarati Das, Hancheng Cao, Bahareh Sarrafzadeh",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2505.16023.pdf,"As large language models (LLMs) are used in complex writing workflows, users engage in multi-turn interactions to steer generations to better fit their needs. Rather than passively accepting output, users actively refine, explore, and co-construct text. We conduct a large-scale analysis of this collaborative behavior for users engaged in writing tasks in the wild with two popular AI assistants, Bing Copilot and WildChat. Our analysis goes beyond simple task classification or satisfaction estimation common in prior work and instead characterizes how users interact with LLMs through the course of a session. We identify prototypical behaviors in how users interact with LLMs in prompts following their original request. We refer to these as Prototypical Human-AI Collaboration Behaviors (PATHs) and find that a small group of PATHs explain a majority of the variation seen in user-LLM interaction. These PATHs span users revising intents, exploring texts, posing questions, adjusting style or injecting new content. Next, we find statistically significant correlations between specific writing intents and PATHs, revealing how users' intents shape their collaboration behaviors. We conclude by discussing the implications of our findings on LLM alignment.",随着大型语言模型（LLM）在复杂的写作工作流程中使用，用户进行多轮交互以引导生成更好地满足他们的需求。用户不仅被动接受输出，还积极修改、探索和共同构建文本。我们对用户在野外使用两个流行的AI助手（Bing Copilot和WildChat）进行写作任务的这种协作行为进行了大规模分析。我们的分析不仅超越了先前工作中常见的简单任务分类或满意度估计，还描述了用户在会话过程中如何通过原始请求后的提示与LLM进行交互。我们在用户与LLM的交互中识别出原型行为，并将其称为原型人机协作行为（PATHs），发现一小组PATHs解释了用户-LLM交互中看到的大多数变化。这些PATHs涵盖了用户修订意图、探索文本、提问、调整风格或注入新内容。接下来，我们发现特定的写作意图与PATHs之间存在统计学显著的相关性，揭示了用户的意图如何塑造他们的协作行为。最后，我们讨论了我们发现对LLM对齐的影响。,The paper identifies prototypical human-AI collaboration behaviors in LLM-assisted writing tasks and discusses their implications for LLM alignment.,LLM,"Helpful, Honest","Human-AI collaboration, LLM interaction, alignment, writing tasks, user intents"
When can isotropy help adapt LLMs' next word prediction to numerical domains?,"Rashed Shelim, Shengzhe Xu, Walid Saad, Naren Ramakrishnan",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2505.17135.pdf,"Vector representations of contextual embeddings learned by pre-trained large language models (LLMs) are effective in various downstream tasks in numerical domains such as time series forecasting. Despite their significant benefits, the tendency of LLMs to hallucinate in such domains can have severe consequences in applications such as energy, nature, finance, healthcare, retail and transportation, among others. To guarantee prediction reliability and accuracy in numerical domains, it is necessary to open the black box behind the LLM and provide performance guarantees through explanation. However, there is little theoretical understanding of when pre-trained language models help solve numerical downstream tasks. This paper seeks to bridge this gap by understanding when the next-word prediction capability of LLMs can be adapted to numerical domains through a novel analysis based on the concept of isotropy in the contextual embedding space. Specifically, a log-linear model for LLMs is considered in which numerical data can be predicted from its context through a network with softmax in the output layer of LLMs (i.e., language model head in self-attention). For this model, it is demonstrated that, in order to achieve state-of-the-art performance in numerical domains, the hidden representations of the LLM embeddings must possess a structure that accounts for the shift-invariance of the softmax function. By formulating a gradient structure of self-attention in pre-trained models, it is shown how the isotropic property of LLM embeddings in contextual embedding space preserves the underlying structure of representations, thereby resolving the shift-invariance problem and providing a performance guarantee. Experiments show that different characteristics of numerical data and model architectures have different impacts on isotropy, and this variability directly affects the performances.",大语言模型（LLM）的上下文嵌入向量表示在时间序列预测等数值域的各种下游任务中表现出色。尽管它们具有显著的好处，但LLM在这些域中产生幻觉的倾向可能会对能源、自然、金融、医疗、零售和交通等应用产生严重后果。为了确保数值域中的预测可靠性和准确性，有必要打开LLM背后的黑箱，并通过解释提供性能保证。然而，关于预训练语言模型何时有助于解决数值下游任务的理论理解很少。本文旨在通过基于上下文嵌入空间中等距性概念的新颖分析来弥合这一差距，具体来说，考虑了一个对数线性模型，其中可以通过具有softmax输出层的网络（即自注意力中的语言模型头）从其上下文中预测数值数据。对于这种模型，证明了为了在数值域中实现最先进的性能，LLM嵌入的隐藏表示必须具有一种结构，以考虑softmax函数的平移不变性。通过对预训练模型中的自注意力的梯度结构进行公式化，说明了LLM嵌入在上下文嵌入空间中的等距性质如何保留表示的基本结构，从而解决了平移不变性问题并提供了性能保证。实验表明，数值数据的不同特征和模型架构对等距性有不同的影响，这种变异直接影响性能。,"The paper explores how isotropy in LLM embeddings can improve the reliability and accuracy of next-word predictions in numerical domains, addressing the hallucination issue and providing performance guarantees.",LLM,Harmless,"Isotropy, Numerical Domains, Next-Word Prediction, LLM Adaptation, Performance Guarantees"
SIPDO: Closed-Loop Prompt Optimization via Synthetic Data Feedback,"Yaoning Yu, Ye Yu, Kai Wei, Haojing Luo, Haohan Wang",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2505.19514.pdf,"Prompt quality plays a critical role in the performance of large language models (LLMs), motivating a growing body of work on prompt optimization. Most existing methods optimize prompts over a fixed dataset, assuming static input distributions and offering limited support for iterative improvement. We introduce SIPDO (Self-Improving Prompts through Data-Augmented Optimization), a closed-loop framework for prompt learning that integrates synthetic data generation into the optimization process. SIPDO couples a synthetic data generator with a prompt optimizer, where the generator produces new examples that reveal current prompt weaknesses and the optimizer incrementally refines the prompt in response. This feedback-driven loop enables systematic improvement of prompt performance without assuming access to external supervision or new tasks. Experiments across question answering and reasoning benchmarks show that SIPDO outperforms standard prompt tuning methods, highlighting the value of integrating data synthesis into prompt learning workflows.",提示质量在大型语言模型（LLM）的性能中起着关键作用，激发了越来越多关于提示优化的研究工作。大多数现有方法在固定数据集上优化提示，假设输入分布是静态的，并为迭代改进提供有限支持。我们引入了SIPDO（通过数据增强优化的自我改进提示），这是一个将合成数据生成集成到优化过程中的闭环框架。 SIPDO将合成数据生成器与提示优化器耦合，生成器产生新示例，揭示当前提示的弱点，优化器逐步改进提示以回应。这种反馈驱动的循环使得提示性能能够系统地改进，而不需要访问外部监督或新任务。跨问答和推理基准的实验表明，SIPDO在标准提示调整方法中表现出色，突出了将数据合成集成到提示学习工作流程中的价值。,"The paper introduces SIPDO, a closed-loop framework for optimizing prompts in large language models using synthetic data feedback to improve performance.",LLM,"Helpful, Honest","Prompt optimization, synthetic data, closed-loop, LLM performance, data augmentation"
Position is Power: System Prompts as a Mechanism of Bias in Large Language Models (LLMs),"Anna Neumann, Elisabeth Kirsten, Muhammad Bilal Zafar, Jatinder Singh",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2505.21091.pdf,"System prompts in Large Language Models (LLMs) are predefined directives that guide model behaviour, taking precedence over user inputs in text processing and generation. LLM deployers increasingly use them to ensure consistent responses across contexts. While model providers set a foundation of system prompts, deployers and third-party developers can append additional prompts without visibility into others' additions, while this layered implementation remains entirely hidden from end-users. As system prompts become more complex, they can directly or indirectly introduce unaccounted for side effects. This lack of transparency raises fundamental questions about how the position of information in different directives shapes model outputs. As such, this work examines how the placement of information affects model behaviour. To this end, we compare how models process demographic information in system versus user prompts across six commercially available LLMs and 50 demographic groups. Our analysis reveals significant biases, manifesting in differences in user representation and decision-making scenarios. Since these variations stem from inaccessible and opaque system-level configurations, they risk representational, allocative and potential other biases and downstream harms beyond the user's ability to detect or correct. Our findings draw attention to these critical issues, which have the potential to perpetuate harms if left unexamined. Further, we argue that system prompt analysis must be incorporated into AI auditing processes, particularly as customisable system prompts become increasingly prevalent in commercial AI deployments.",大语言模型（LLM）中的系统提示是预定义的指令，指导模型行为，优先于用户输入进行文本处理和生成。LLM部署者越来越多地使用它们以确保跨上下文的一致响应。虽然模型提供者设置了系统提示的基础，但部署者和第三方开发者可以在不可见其他添加的情况下附加额外的提示，而这种分层实现对最终用户完全隐藏。随着系统提示变得更加复杂，它们可以直接或间接地引入未计入的副作用。这种缺乏透明性引发了关于信息在不同指令中的位置如何塑造模型输出的基本问题。因此，本文研究了信息的放置如何影响模型行为。为此，我们比较了六种商业可用的LLM和50个人口统计群体中系统提示与用户提示中的人口统计信息的处理方式。我们的分析揭示了显著的偏见，表现为用户表示和决策场景中的差异。由于这些变化源于不可访问和不透明的系统级配置，它们可能会导致代表性、分配和其他潜在偏见和下游伤害，超出用户的检测或纠正能力。我们的发现引起了对这些关键问题的关注，如果不加审查，这些问题有可能加剧伤害。此外，我们认为系统提示分析必须纳入AI审计流程，特别是随着可定制系统提示在商业AI部署中变得越来越普遍。,"The paper investigates how the placement of information in system prompts can introduce biases in LLMs, highlighting the need for transparency and auditing in AI deployments.",LLM,Harmless,"System prompts, bias, large language models, transparency, AI auditing"
Eye of Judgement: Dissecting the Evaluation of Russian-speaking LLMs with POLLUX,"Nikita Martynov, Anastasia Mordasheva, Dmitriy Gorbetskiy, Danil Astafurov, Ulyana Isaeva, Elina Basyrova, Sergey Skachkov, Victoria Berestova, Nikolay Ivanov, Valeriia Zanina, Alena Fenogenova",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2505.24616.pdf,"We introduce POLLUX, a comprehensive open-source benchmark designed to evaluate the generative capabilities of large language models (LLMs) in Russian. Our main contribution is a novel evaluation methodology that enhances the interpretability of LLM assessment. For each task type, we define a set of detailed criteria and develop a scoring protocol where models evaluate responses and provide justifications for their ratings. This enables transparent, criteria-driven evaluation beyond traditional resource-consuming, side-by-side human comparisons. POLLUX includes a detailed, fine-grained taxonomy of 35 task types covering diverse generative domains such as code generation, creative writing, and practical assistant use cases, totaling 2,100 manually crafted and professionally authored prompts. Each task is categorized by difficulty (easy/medium/hard), with experts constructing the dataset entirely from scratch. We also release a family of LLM-as-a-Judge (7B and 32B) evaluators trained for nuanced assessment of generative outputs. This approach provides scalable, interpretable evaluation and annotation tools for model development, effectively replacing costly and less precise human judgments.","我们介绍了 POLLUX，这是一个旨在评估大型语言模型（LLMs）在俄语中的生成能力的全面开源基准。我们的主要贡献是一种新的评估方法，增强了 LLM 评估的可解释性。对于每种任务类型，我们定义了一组详细的标准，并开发了一种评分协议，其中模型评估响应并为其评级提供理由。这使得在传统的资源密集型、人工并排比较之外，实现了透明、基于标准的评估。POLLUX 包括 35 种任务类型的详细、细粒度分类，涵盖了从代码生成、创意写作到实际助手使用场景等多种生成领域，总共 2,100 个手工制作和专业撰写的提示。每个任务都按难度（易/中/难）分类，专家从头开始构建数据集。我们还发布了一系列 LLM-as-a-Judge（7B 和 32B）评估器，用于对生成输出进行细致的评估。这种方法为模型开发提供了可扩展、可解释的评估和注释工具，有效地取代了昂贵且不精确的人工判断。","The paper introduces POLLUX, a benchmark for evaluating the generative capabilities of LLMs in Russian, with a focus on transparent and criteria-driven assessment.",LLM,None,"Evaluation, Benchmark, Generative Capabilities, Russian, LLM"
SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware Reinforcement Learning,"Zhongwei Wan, Zhihao Dou, Che Liu, Yu Zhang, Dongfei Cui, Qinjian Zhao, Hui Shen, Jing Xiong, Yi Xin, Yifan Jiang, Chaofan Tao, Yangfan He, Mi Zhang, Shen Yan",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.01713.pdf,"Multimodal large language models (MLLMs) have shown promising capabilities in reasoning tasks, yet still struggle with complex problems requiring explicit self-reflection and self-correction, especially compared to their unimodal text-based counterparts. Existing reflection methods are simplistic and struggle to generate meaningful and instructive feedback, as the reasoning ability and knowledge limits of pre-trained models are largely fixed during initial training. To overcome these challenges, we propose Multimodal Self-Reflection enhanced reasoning with Group Relative Policy Optimization (SRPO), a two-stage reflection-aware reinforcement learning (RL) framework explicitly designed to enhance multimodal LLM reasoning. In the first stage, we construct a high-quality, reflection-focused dataset under the guidance of an advanced MLLM, which generates reflections based on initial responses to help the policy model learn both reasoning and self-reflection. In the second stage, we introduce a novel reward mechanism within the GRPO framework that encourages concise and cognitively meaningful reflection while avoiding redundancy. Extensive experiments across multiple multimodal reasoning benchmarks, including MathVista, MathVision, MathVerse, and MMMU-Pro, using Qwen-2.5-VL-7B and Qwen-2.5-VL-32B demonstrate that SRPO significantly outperforms state-of-the-art models, achieving notable improvements in both reasoning accuracy and reflection quality.",多模态大语言模型（MLLMs）在推理任务中表现出了令人鼓舞的能力，但在需要显式自我反思和自我纠正的复杂问题上仍然存在挑战，特别是与其单模态文本基础的对手相比。现有的反思方法过于简单，难以生成有意义和有指导性的反馈，因为预训练模型的推理能力和知识限制在初始训练期间大多是固定的。为了克服这些挑战，我们提出了多模态自我反思增强推理与组相对策略优化（SRPO），这是一个专门设计用于增强多模态LLM推理的两阶段反思感知强化学习（RL）框架。在第一阶段，我们在高级MLLM的指导下构建了一个高质量的反思专注数据集，该数据集基于初始响应生成反思，以帮助策略模型学习推理和自我反思。在第二阶段，我们在GRPO框架中引入了一种新颖的奖励机制，鼓励简洁且认知上有意义的反思，同时避免冗余。在MathVista、MathVision、MathVerse和MMMU-Pro等多个多模态推理基准测试中，使用Qwen-2.5-VL-7B和Qwen-2.5-VL-32B进行了广泛的实验，结果表明SRPO显著优于现有最先进的模型，在推理准确性和反思质量方面取得了显著改进。,"The paper introduces SRPO, a reflection-aware reinforcement learning framework designed to enhance the reasoning capabilities of multimodal large language models by improving self-reflection and self-correction.",LLM,Helpful,"Multimodal LLM, Reflection, Reinforcement Learning, Reasoning, Self-Correction"
NovelHopQA: Diagnosing Multi-Hop Reasoning Failures in Long Narrative Contexts,"Abhay Gupta, Michael Lu, Kevin Zhu, Sean O'Brien, Vasu Sharma",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.02000.pdf,"Current large language models (LLMs) struggle to answer questions that span tens of thousands of tokens, especially when multi-hop reasoning is involved. While prior benchmarks explore long-context comprehension or multi-hop reasoning in isolation, none jointly vary context length and reasoning depth in natural narrative settings. We introduce NovelHopQA, the first benchmark to evaluate 1-4 hop QA over 64k-128k-token excerpts from 83 full-length public-domain novels. A keyword-guided pipeline builds hop-separated chains grounded in coherent storylines. We evaluate seven state-of-the-art models and apply oracle-context filtering to ensure all questions are genuinely answerable. Human annotators validate both alignment and hop depth. We additionally present retrieval-augmented generation (RAG) evaluations to test model performance when only selected passages are provided instead of the full context. We noticed consistent accuracy drops with increased hops and context length increase, even for frontier models-revealing that sheer scale does not guarantee robust reasoning. Failure-mode analysis highlights common breakdowns such as missed final-hop integration and long-range drift. NovelHopQA offers a controlled diagnostic setting to test multi-hop reasoning at scale. All code and datasets are available at https://novelhopqa.github.io.",当前的大型语言模型（LLMs）在涉及数万个标记的问题上表现不佳，特别是当涉及多跳推理时。虽然先前的基准测试探索了长上下文理解或多跳推理，但没有在自然叙述设置中同时变化上下文长度和推理深度。我们引入了NovelHopQA，这是第一个评估1-4跳QA的基准，覆盖了83部公共领域的全长小说的64k-128k标记摘录。一个关键字引导的管道构建了基于连贯故事情节的跳分离链。我们评估了七个最先进的模型，并应用了oracle上下文过滤，以确保所有问题都是真正可回答的。人类注释员验证了对齐和跳深度。我们还提出了检索增强生成（RAG）评估，以测试模型在仅提供选定段落而不是完整上下文时的性能。我们注意到，即使是前沿模型，随着跳数和上下文长度的增加，准确性也会显著下降，这表明规模本身并不能保证健壮的推理。故障模式分析突出了常见的故障，如未能整合最终跳和长距离漂移。NovelHopQA提供了一个受控的诊断设置，以在大规模上测试多跳推理。所有代码和数据集都可在https://novelhopqa.github.io上获得。,"The paper introduces NovelHopQA, a benchmark for evaluating multi-hop reasoning in long narrative contexts, revealing challenges faced by large language models in maintaining accuracy with increased context length and reasoning depth.",LLM,"Helpful, Honest","Multi-hop reasoning, long context, evaluation, LLM, narrative"
Cross-Entropy Games for Language Models: From Implicit Knowledge to General Capability Measures,"Cl\'ement Hongler, Andrew Emil",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.06832.pdf,"Large Language Models (LLMs) define probability measures on text. By considering the implicit knowledge question of what it means for an LLM to know such a measure and what it entails algorithmically, we are naturally led to formulate a series of tasks that go beyond generative sampling, involving forms of summarization, counterfactual thinking, anomaly detection, originality search, reverse prompting, debating, creative solving, etc. These tasks can be formulated as games based on LLM measures, which we call Cross-Entropy (Xent) Games. Xent Games can be single-player or multi-player. They involve cross-entropy scores and cross-entropy constraints, and can be expressed as simple computational graphs and programs. We show the Xent Game space is large enough to contain a wealth of interesting examples, while being constructible from basic game-theoretic consistency axioms. We then discuss how the Xent Game space can be used to measure the abilities of LLMs. This leads to the construction of Xent Game measures: finite families of Xent Games that can be used as capability benchmarks, built from a given scope, by extracting a covering measure. To address the unbounded scope problem associated with the challenge of measuring general abilities, we propose to explore the space of Xent Games in a coherent fashion, using ideas inspired by evolutionary dynamics.",大语言模型（LLMs）在文本上定义概率测度。通过考虑隐含知识问题，即LLM知道这种测度的意义以及其算法上的含义，我们自然会提出一系列超越生成采样的任务，涉及摘要、反事实思维、异常检测、原创性搜索、反向提示、辩论、创造性解决等。这些任务可以作为基于LLM测度的游戏，我们称之为交叉熵（Xent）游戏。Xent游戏可以是单人或多人游戏。它们涉及交叉熵分数和交叉熵约束，并且可以表示为简单的计算图和程序。我们展示了Xent游戏空间足够大，以包含大量有趣的例子，同时可以从基本的博弈论一致性公理构建。然后我们讨论了如何使用Xent游戏空间来测量LLM的能力。这导致了Xent游戏测量的构建：可以用作能力基准的有限Xent游戏家族，由给定的范围构建，通过提取覆盖测度。为了解决与测量一般能力相关的无界范围问题，我们提出探索Xent游戏空间的想法，使用受进化动力学启发的想法。,The paper introduces Cross-Entropy Games as a method to measure and benchmark the capabilities of Large Language Models.,LLM,None,"Cross-Entropy Games, LLM abilities, capability benchmarks, evolutionary dynamics, debating"
Supernova Event Dataset: Interpreting Large Language Models' Personality through Critical Event Analysis,"Pranav Agarwal, Ioana Ciuc\u{a}",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.12189.pdf,"Large Language Models (LLMs) are increasingly integrated into everyday applications. As their influence grows, understanding their decision making and underlying personality becomes essential. In this work, we interpret model personality using our proposed Supernova Event Dataset, a novel dataset with diverse articles spanning biographies, historical events, news, and scientific discoveries. We use this dataset to benchmark LLMs on extracting and ranking key events from text, a subjective and complex challenge that requires reasoning over long-range context and modeling causal chains. We evaluate small models like Phi-4, Orca 2, and Qwen 2.5, and large, stronger models such as Claude 3.7, Gemini 2.5, and OpenAI o3, and propose a framework where another LLM acts as a judge to infer each model's personality based on its selection and classification of events. Our analysis shows distinct personality traits: for instance, Orca 2 demonstrates emotional reasoning focusing on interpersonal dynamics, while Qwen 2.5 displays a more strategic, analytical style. When analyzing scientific discovery events, Claude Sonnet 3.7 emphasizes conceptual framing, Gemini 2.5 Pro prioritizes empirical validation, and o3 favors step-by-step causal reasoning. This analysis improves model interpretability, making them user-friendly for a wide range of diverse applications. Project Page - https://www.supernova-event.ai/",大语言模型（LLMs）越来越多地融入到日常应用中。随着它们的影响力不断扩大，理解它们的决策过程和潜在的性格变得至关重要。在本文中，我们通过提出的超新星事件数据集，一种包含生平、历史事件、新闻和科学发现等多种文章的新型数据集，来解释模型的性格。我们使用这个数据集来对LLMs进行基准测试，以从文本中提取和排名关键事件，这是一个主观且复杂的挑战，需要在长距离上进行推理和建模因果链。我们评估了小型模型如Phi-4、Orca 2和Qwen 2.5，以及大型、强大的模型如Claude 3.7、Gemini 2.5和OpenAI o3，并提出了一种框架，其中另一个LLM作为裁判，根据其选择和分类事件来推断每个模型的性格。我们的分析显示出不同的性格特征：例如，Orca 2展示了情感推理，专注于人际动态，而Qwen 2.5则表现出更为策略性、分析性的风格。在分析科学发现事件时，Claude Sonnet 3.7强调概念框架，Gemini 2.5 Pro优先考虑经验验证，而o3则倾向于逐步因果推理。这种分析提高了模型的可解释性，使其对各种多样化应用更加友好。,"The paper introduces a dataset and framework to interpret LLM personalities through event analysis, aiming to make models more user-friendly and aligned with helpful and harmless principles.",LLM,"Helpful, Harmless","LLM personality, event analysis, model interpretability, helpful, harmless"
Exploring the Secondary Risks of Large Language Models,"Jiawei Chen, Zhengwei Fang, Xiao Yang, Chao Yu, Zhaoxia Yin, Hang Su",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.12382.pdf,"Ensuring the safety and alignment of Large Language Models is a significant challenge with their growing integration into critical applications and societal functions. While prior research has primarily focused on jailbreak attacks, less attention has been given to non-adversarial failures that subtly emerge during benign interactions. We introduce secondary risks a novel class of failure modes marked by harmful or misleading behaviors during benign prompts. Unlike adversarial attacks, these risks stem from imperfect generalization and often evade standard safety mechanisms. To enable systematic evaluation, we introduce two risk primitives verbose response and speculative advice that capture the core failure patterns. Building on these definitions, we propose SecLens, a black-box, multi-objective search framework that efficiently elicits secondary risk behaviors by optimizing task relevance, risk activation, and linguistic plausibility. To support reproducible evaluation, we release SecRiskBench, a benchmark dataset of 650 prompts covering eight diverse real-world risk categories. Experimental results from extensive evaluations on 16 popular models demonstrate that secondary risks are widespread, transferable across models, and modality independent, emphasizing the urgent need for enhanced safety mechanisms to address benign yet harmful LLM behaviors in real-world deployments.",确保大型语言模型的安全性和对齐是一个重要挑战，随着它们在关键应用和社会功能中的不断集成。虽然先前的研究主要集中在越狱攻击上，但对非对抗性失败的关注较少，这些失败在良性交互过程中微妙地出现。我们引入了次级风险，这是一种新的故障模式，标志着在良性提示期间出现有害或误导行为。与对抗性攻击不同，这些风险源于不完美的泛化，并且通常能够规避标准的安全机制。为了实现系统评估，我们引入了两个风险原语：冗长响应和推测性建议，这些原语捕捉了核心故障模式。基于这些定义，我们提出了SecLens，一个黑盒多目标搜索框架，通过优化任务相关性、风险激活和语言可信度，高效地引发次级风险行为。为了支持可重复评估，我们发布了SecRiskBench，一个涵盖八个多样化的现实世界风险类别的650个提示的基准数据集。广泛评估16个流行模型的实验结果表明，次级风险普遍存在，可以在模型之间传递，并且与模态无关，强调了在现实世界部署中需要增强的安全机制，以应对良性但有害的LLM行为。,"The paper introduces secondary risks in LLMs, which are harmful behaviors that occur during benign interactions, and proposes a framework and benchmark dataset for evaluating these risks.",LLM,Harmless,"Secondary risks, LLM safety, benign interactions, SecLens, SecRiskBench"
Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption Masking And Normalization,"Filip Sondej, Yushi Yang, Miko{\l}aj Kniejski, Marcel Windys",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.12484.pdf,"Language models can retain dangerous knowledge and skills even after extensive safety fine-tuning, posing both misuse and misalignment risks. Recent studies show that even specialized unlearning methods can be easily reversed. To address this, we systematically evaluate many existing and novel components of unlearning methods and identify ones crucial for irreversible unlearning.
  We introduce Disruption Masking, a technique in which we only allow updating weights, where the signs of the unlearning gradient and the retaining gradient are the same. This ensures all updates are non-disruptive.
  Additionally, we identify the need for normalizing the unlearning gradients, and also confirm the usefulness of meta-learning. We combine these insights into MUDMAN (Meta-Unlearning with Disruption Masking and Normalization) and validate its effectiveness at preventing the recovery of dangerous capabilities. MUDMAN outperforms the prior TAR method by 40\%, setting a new state-of-the-art for robust unlearning.",语言模型即使经过广泛的安全微调，仍然可能保留危险的知识和技能，从而带来滥用和不一致的风险。最近的研究表明，即使是专门的遗忘方法也可以被轻易逆转。为了解决这个问题，我们系统地评估了许多现有和新的遗忘方法的组件，并确定了对不可逆遗忘至关重要的组件。我们引入了扰动掩码技术，其中我们只允许更新权重，其中遗忘梯度和保留梯度的符号相同。这确保了所有更新都是非扰动的。此外，我们确定了标准化遗忘梯度的需求，并确认了元学习的有用性。我们将这些见解结合到MUDMAN（元遗忘与扰动掩码和标准化）中，并验证了其防止恢复危险能力的有效性。MUDMAN在防止恢复危险能力方面比先前的TAR方法提高了40%，创造了新的最佳水平。,"The paper introduces MUDMAN, a method for robust unlearning in LLMs to prevent the recovery of dangerous capabilities, improving safety and alignment.",LLM,Harmless,"Unlearning, Safety, Alignment, Harmless, LLM"
Unlearning Isn't Invisible: Detecting Unlearning Traces in LLMs from Model Outputs,"Yiwei Chen, Soumyadeep Pal, Yimeng Zhang, Qing Qu, Sijia Liu",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.14003.pdf,"Machine unlearning (MU) for large language models (LLMs), commonly referred to as LLM unlearning, seeks to remove specific undesirable data or knowledge from a trained model, while maintaining its performance on standard tasks. While unlearning plays a vital role in protecting data privacy, enforcing copyright, and mitigating sociotechnical harms in LLMs, we identify a new vulnerability post-unlearning: unlearning trace detection. We discover that unlearning leaves behind persistent ''fingerprints'' in LLMs, detectable traces in both model behavior and internal representations. These traces can be identified from output responses, even when prompted with forget-irrelevant inputs. Specifically, a simple supervised classifier can reliably determine whether a model has undergone unlearning based solely on its textual outputs. Further analysis shows that these traces are embedded in intermediate activations and propagate nonlinearly to the final layer, forming low-dimensional, learnable manifolds in activation space. Through extensive experiments, we show that forget-relevant prompts enable over 90% accuracy in detecting unlearning traces across all model sizes. Even with forget-irrelevant inputs, large LLMs maintain high detectability, demonstrating the broad applicability of unlearning trace detection. These findings reveal that unlearning leaves measurable signatures, introducing a new risk of reverse-engineering forgotten information when a model is identified as unlearned given an input query. Codes are available at https://github.com/OPTML-Group/Unlearn-Trace.",大语言模型（LLM）的机器遗忘（MU）旨在从训练好的模型中删除特定的不良数据或知识，同时保持其在标准任务上的性能。虽然遗忘在保护数据隐私、执行版权和缓解LLM中的社会技术危害方面起着至关重要的作用，但我们发现了遗忘后的一个新的脆弱性：遗忘痕迹检测。我们发现，遗忘在LLM中留下了持久的“指纹”，这些痕迹可以在模型行为和内部表示中检测到。这些痕迹可以从输出响应中识别出来，即使在使用与遗忘无关的提示时也是如此。具体来说，一个简单的监督分类器可以根据其文本输出可靠地确定模型是否经过遗忘。进一步的分析表明，这些痕迹嵌入在中间激活中，并非线性地传播到最终层，形成激活空间中的低维、可学习流形。通过广泛的实验，我们表明，与遗忘相关的提示使得在所有模型大小上检测遗忘痕迹的准确率超过90%。即使在与遗忘无关的输入下，大型LLM仍然保持高检测性，表明遗忘痕迹检测的广泛适用性。这些发现表明，遗忘留下了可测量的标志，当给定输入查询时，模型被识别为遗忘时，引入了反向工程遗忘信息的新风险。,"The paper identifies and analyzes detectable traces left by the unlearning process in LLMs, which could potentially be used to reverse-engineer forgotten information.",LLM,Harmless,"Unlearning, LLMs, Trace Detection, Privacy, Sociotechnical Harms"
LLM Web Dynamics: Tracing Model Collapse in a Network of LLMs,"Tianyu Wang, Lingyou Pang, Akira Horiguchi, Carey E. Priebe",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.15690.pdf,"The increasing use of synthetic data from the public Internet has enhanced data usage efficiency in large language model (LLM) training. However, the potential threat of model collapse remains insufficiently explored. Existing studies primarily examine model collapse in a single model setting or rely solely on statistical surrogates. In this work, we introduce LLM Web Dynamics (LWD), an efficient framework for investigating model collapse at the network level. By simulating the Internet with a retrieval-augmented generation (RAG) database, we analyze the convergence pattern of model outputs. Furthermore, we provide theoretical guarantees for this convergence by drawing an analogy to interacting Gaussian Mixture Models.",使用公共互联网的合成数据在大型语言模型（LLM）训练中的使用效率得到了提高。然而，模型崩溃的潜在威胁仍然探索不足。现有研究主要在单一模型设置中检查模型崩溃，或者仅依赖统计替代品。在本工作中，我们引入了LLM Web Dynamics（LWD），这是一个用于在网络级别研究模型崩溃的高效框架。通过用检索增强生成（RAG）数据库模拟互联网，我们分析了模型输出的收敛模式。此外，我们通过将其类比于相互作用的高斯混合模型，为这种收敛提供了理论保证。,"The paper introduces a framework to study model collapse in a network of LLMs trained on synthetic internet data, providing theoretical guarantees for convergence.",LLM,"Helpful, Harmless","Model Collapse, LLM, Network Dynamics, Synthetic Data, Convergence"
SLR: An Automated Synthesis Framework for Scalable Logical Reasoning,"Lukas Helff, Ahmad Omar, Felix Friedrich, Wolfgang Stammer, Antonia W\""ust, Tim Woydt, Rupert Mitchell, Patrick Schramowski, Kristian Kersting",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.15787.pdf,"We introduce SLR, an end-to-end framework for systematic evaluation and training of Large Language Models (LLMs) via Scalable Logical Reasoning. Given a user's task specification, SLR enables scalable, automated synthesis of inductive reasoning tasks with precisely controlled difficulty. For each task, SLR synthesizes (i) a latent ground-truth rule, (ii) an executable validation program used by a symbolic judge to deterministically verify model outputs, and (iii) an instruction prompt for the reasoning task. Using SLR, we create SLR-Bench, a benchmark comprising over 19k prompts spanning 20 curriculum levels that progressively increase in relational, arithmetic, and recursive complexity. Large-scale evaluation reveals that contemporary LLMs readily produce syntactically valid rules, yet often fail at correct logical inference. Recent reasoning LLMs do somewhat better, but incur substantial increases in test-time compute, sometimes exceeding 15k completion tokens. Finally, logic-tuning via SLR doubles Llama-3-8B accuracy on SLR-Bench, achieving parity with Gemini-Flash-Thinking at a fraction of computational cost. SLR is fully automated, requires no human annotation, ensures dataset novelty, and offers a scalable environment for probing and advancing LLMs' reasoning capabilities.",我们介绍了SLR，一个用于通过可扩展逻辑推理系统评估和训练大型语言模型（LLMs）的端到端框架。根据用户的任务规范，SLR使得可扩展、自动化的归纳推理任务的合成成为可能，难度精确控制。对于每个任务，SLR合成了（i）潜在的真实规则，（ii）由符号裁判使用的可执行验证程序，以确定性验证模型输出，以及（iii）推理任务的指令提示。使用SLR，我们创建了SLR-Bench，一个包含超过19k个提示的基准，跨越20个课程级别，逐步增加关系、算术和递归复杂性。大规模评估表明，当代LLMs容易产生语法上有效的规则，但往往在正确的逻辑推理中失败。最近的推理LLMs做得有些好，但会导致大量的测试时间计算，有时超过15k个完成令牌。最后，通过SLR的逻辑调优，Llama-3-8B在SLR-Bench上的准确性翻了一番，在计算成本的小部分实现了与Gemini-Flash-Thinking的平等。SLR完全自动化，不需要人工注释，确保数据集新颖，并提供一个可扩展的环境来探测和推进LLMs的推理能力。,"The paper presents SLR, a framework for evaluating and training LLMs through scalable logical reasoning, and introduces SLR-Bench, a benchmark for assessing LLMs' reasoning capabilities.",LLM,Helpful,"Large Language Models, Logical Reasoning, Evaluation, Training, Benchmark"
From RAG to Agentic: Validating Islamic-Medicine Responses with LLM Agents,"Mohammad Amaan Sayeed, Mohammed Talha Alam, Raza Imam, Shahab Saquib Sohail, Amir Hussain",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.15911.pdf,"Centuries-old Islamic medical texts like Avicenna's Canon of Medicine and the Prophetic Tibb-e-Nabawi encode a wealth of preventive care, nutrition, and holistic therapies, yet remain inaccessible to many and underutilized in modern AI systems. Existing language-model benchmarks focus narrowly on factual recall or user preference, leaving a gap in validating culturally grounded medical guidance at scale. We propose a unified evaluation pipeline, Tibbe-AG, that aligns 30 carefully curated Prophetic-medicine questions with human-verified remedies and compares three LLMs (LLaMA-3, Mistral-7B, Qwen2-7B) under three configurations: direct generation, retrieval-augmented generation, and a scientific self-critique filter. Each answer is then assessed by a secondary LLM serving as an agentic judge, yielding a single 3C3H quality score. Retrieval improves factual accuracy by 13%, while the agentic prompt adds another 10% improvement through deeper mechanistic insight and safety considerations. Our results demonstrate that blending classical Islamic texts with retrieval and self-evaluation enables reliable, culturally sensitive medical question-answering.",古老的伊斯兰医学文本，如阿维森纳的《医典》和先知的《提比-纳比》，包含大量的预防保健、营养和全面疗法，但对许多人来说仍然难以接触，在现代AI系统中也未得到充分利用。现有的语言模型基准集中在事实回忆或用户偏好上，留下了一个在大规模验证文化背景的医疗指导方面的空白。我们提出了一个统一的评估流水线，Tibbe-AG，它将30个精心策划的先知医学问题与人类验证的疗法对齐，并比较了三个LLM（LLaMA-3、Mistral-7B、Qwen2-7B）在三种配置下的表现：直接生成、检索增强生成和科学自我批判过滤器。然后，每个答案由一个作为代理判断者的次要LLM进行评估，产生一个单一的3C3H质量分数。检索提高了事实准确性13%，而代理提示通过更深的机制洞察力和安全考虑又增加了10%的改进。我们的结果表明，将经典伊斯兰文本与检索和自我评估相结合，可以实现可靠、具有文化敏感性的医疗问题-回答。,"The paper introduces a pipeline to evaluate LLMs in providing culturally sensitive medical guidance based on Islamic texts, focusing on helpful and harmless alignment.",LLM,"Helpful, Harmless","LLM alignment, medical guidance, Islamic medicine, retrieval-augmented generation, agentic judge"
"Piloting Copilot, Codex, and StarCoder2: Hot Temperature, Cold Prompts, or Black Magic?","Jean-Baptiste D\""oderlein, Nguessan Hermann Kouadio, Mathieu Acher, Djamel Eddine Khelladi, Benoit Combemale",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2210.14699.pdf,"Language models are promising solutions for tackling increasing complex problems. In software engineering, they recently gained attention in code assistants, which generate programs from a natural language task description (prompt). They have the potential to save time and effort but remain poorly understood, limiting their optimal use. In this article, we investigate the impact of input variations on two configurations of a language model, focusing on parameters such as task description, surrounding context, model creativity, and the number of generated solutions. We design specific operators to modify these inputs and apply them to three LLM-based code assistants (Copilot, Codex, StarCoder2) and two benchmarks representing algorithmic problems (HumanEval, LeetCode). Our study examines whether these variations significantly affect program quality and how these effects generalize across models. Our results show that varying input parameters can greatly improve performance, achieving up to 79.27% success in one-shot generation compared to 22.44% for Codex and 31.1% for Copilot in default settings. Actioning this potential in practice is challenging due to the complex interplay in our study - the optimal settings for temperature, prompt, and number of generated solutions vary by problem. Reproducing our study with StarCoder2 confirms these findings, indicating they are not model-specific. We also uncover surprising behaviors (e.g., fully removing the prompt can be effective), revealing model brittleness and areas for improvement.",语言模型是解决日益复杂问题的有前途的解决方案。在软件工程中，它们最近在代码助手中引起了关注，这些助手从自然语言任务描述（提示）生成程序。它们有潜力节省时间和精力，但仍然理解不足，限制了它们的最佳使用。在本文中，我们研究了输入变化对语言模型的两种配置的影响，重点是任务描述、周围上下文、模型创造力和生成的解决方案数量等参数。我们设计了特定的运算符来修改这些输入，并将它们应用于三种基于LLM的代码助手（Copilot、Codex、StarCoder2）和两个代表算法问题的基准（HumanEval、LeetCode）。我们的研究检查了这些变化是否显著影响程序质量，以及这些效果是否能够跨模型推广。我们的结果表明，变化输入参数可以大大提高性能，在一次生成中实现高达79.27%的成功率，而Codex和Copilot在默认设置下分别为22.44%和31.1%。在实践中实现这一潜力具有挑战性，因为我们研究中的复杂相互作用——温度、提示和生成的解决方案数量的最佳设置因问题而异。用StarCoder2重现我们的研究证实了这些发现，表明它们不是模型特定的。我们还揭示了令人惊讶的行为（例如，完全删除提示可能是有效的），揭示了模型的脆弱性和改进的领域。,"The paper investigates how input variations affect the performance of LLM-based code assistants, revealing insights into model brittleness and areas for improvement.",LLM,Helpful,"Code generation, input variations, LLM performance, model brittleness, code assistants"
Beyond Functional Correctness: Investigating Coding Style Inconsistencies in Large Language Models,"Yanlin Wang, Tianyue Jiang, Mingwei Liu, Jiachi Chen, Mingzhi Mao, Xilin Liu, Yuchi Ma, Zibin Zheng",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2407.00456.pdf,"Large language models (LLMs) have brought a paradigm shift to the field of code generation, offering the potential to enhance the software development process. However, previous research mainly focuses on the accuracy of code generation, while coding style differences between LLMs and human developers remain under-explored. In this paper, we empirically analyze the differences in coding style between the code generated by mainstream Code LLMs and the code written by human developers, and summarize coding style inconsistency taxonomy. Specifically, we first summarize the types of coding style inconsistencies by manually analyzing a large number of generation results. We then compare the code generated by Code LLMs with the code written by human programmers in terms of readability, conciseness, and robustness. The results reveal that LLMs and developers have different coding styles. Additionally, we study the possible causes of these inconsistencies and provide some solutions to alleviate the problem.",大语言模型（LLMs）在代码生成领域带来了范式转变，有潜力增强软件开发过程。然而，之前的研究主要集中在代码生成的准确性上，而LLMs与人类开发者之间的编码风格差异仍然未被充分探索。在这篇论文中，我们实证分析了主流代码LLMs生成的代码与人类开发者编写的代码之间的编码风格差异，并总结了编码风格不一致的分类学。具体来说，我们首先通过手动分析大量生成结果，总结了编码风格不一致的类型。然后，我们从可读性、简洁性和健壮性三个方面比较了代码LLMs生成的代码与人类程序员编写的代码。结果表明，LLMs和开发者有不同的编码风格。此外，我们研究了这些不一致的可能原因，并提供了一些缓解问题的解决方案。,"The paper investigates coding style differences between LLMs and human developers, aiming to improve the helpfulness of LLMs in code generation.",LLM,Helpful,"Coding style, LLMs, Code generation, Inconsistencies, Readability"
SLED: A Speculative LLM Decoding Framework for Efficient Edge Serving,"Xiangchen Li, Dimitrios Spatharakis, Saeid Ghafouri, Jiakun Fan, Hans Vandierendonck, Deepu John, Bo Ji, Dimitrios Nikolopoulos",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.09397.pdf,"Regardless of the advancements in device capabilities, efficient inferencing advanced large language models (LLMs) at the edge remains challenging due to limited device memory and power constraints. Existing strategies, such as aggressive quantization, pruning, or remote inference, trade accuracy for efficiency or lead to substantial cost burdens. This position paper introduces a new approach that leverages speculative decoding, previously viewed primarily as a decoding acceleration technique for autoregressive generation of LLMs, as a promising approach specifically adapted for edge computing by orchestrating computation across heterogeneous devices. We propose \acronym, a method that allows lightweight edge devices to draft multiple candidate tokens locally using diverse draft models, while a single, shared edge server efficiently batches and verifies the tokens utilizing a more precise target model. This approach supports device heterogeneity and reduces server-side memory footprint by avoiding the need to deploy multiple target models. Our initial experiments with Jetson Orin Nano, Raspberry Pi 4B/5, and an edge server equipped with 4 Nvidia A100 GPUs indicate substantial benefits: significantly increased system throughput, capacity, and better cost efficiency, all without sacrificing model accuracy.",尽管设备能力有所提高，在边缘设备上高效推理先进的大型语言模型（LLM）仍然具有挑战性，因为设备内存和电源受限。现有策略，如激进的量化、修剪或远程推理，以效率换取准确性或导致显著的成本负担。本文介绍了一种新方法，利用推测解码，以前主要被视为自回归生成LLM的解码加速技术，作为一种特别适应边缘计算的有前途的方法，通过协调异构设备的计算。我们提出了\acronym，一种方法，允许轻量级边缘设备使用多种草稿模型本地起草多个候选令牌，而单个共享边缘服务器有效地批处理和验证令牌，使用更精确的目标模型。这种方法支持设备异构性，并通过避免部署多个目标模型来减少服务器端的内存占用。我们在Jetson Orin Nano、Raspberry Pi 4B/5和配备4个Nvidia A100 GPU的边缘服务器上的初步实验表明，显著的好处：显著提高系统吞吐量、容量和更好的成本效益，而不牺牲模型准确性。,"The paper introduces SLED, a speculative decoding framework for efficient edge serving of large language models, addressing memory and power constraints.",LLM,None,"Edge serving, speculative decoding, LLM, efficiency, heterogeneous devices"
Effective Red-Teaming of Policy-Adherent Agents,"Itay Nakash, George Kour, Koren Lazar, Matan Vetzler, Guy Uziel, Ateret Anaby-Tavor",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.09600.pdf,"Task-oriented LLM-based agents are increasingly used in domains with strict policies, such as refund eligibility or cancellation rules. The challenge lies in ensuring that the agent consistently adheres to these rules and policies, appropriately refusing any request that would violate them, while still maintaining a helpful and natural interaction. This calls for the development of tailored design and evaluation methodologies to ensure agent resilience against malicious user behavior. We propose a novel threat model that focuses on adversarial users aiming to exploit policy-adherent agents for personal benefit. To address this, we present CRAFT, a multi-agent red-teaming system that leverages policy-aware persuasive strategies to undermine a policy-adherent agent in a customer-service scenario, outperforming conventional jailbreak methods such as DAN prompts, emotional manipulation, and coercive. Building upon the existing tau-bench benchmark, we introduce tau-break, a complementary benchmark designed to rigorously assess the agent's robustness against manipulative user behavior. Finally, we evaluate several straightforward yet effective defense strategies. While these measures provide some protection, they fall short, highlighting the need for stronger, research-driven safeguards to protect policy-adherent agents from adversarial attacks",基于任务的大语言模型（LLM）代理越来越多地用于具有严格政策的领域，例如退款资格或取消规则。挑战在于确保代理始终遵守这些规则和政策，适当拒绝任何违反它们的请求，同时仍然保持有用和自然的互动。这需要开发定制的设计和评估方法，以确保代理在面对恶意用户行为时的弹性。我们提出了一种新的威胁模型，专注于试图利用政策遵从代理以获得个人利益的对抗用户。为了应对这一问题，我们提出了CRAFT，一个多代理红队系统，利用政策感知的说服策略来破坏客户服务场景中的政策遵从代理，优于传统的监狱突破方法，如DAN提示、情感操纵和强制。在现有的tau-bench基准的基础上，我们引入了tau-break，一个补充基准，旨在严格评估代理在面对操纵性用户行为时的健壮性。最后，我们评估了几种简单但有效的防御策略。虽然这些措施提供了一些保护，但它们不足，突显了需要更强大的、研究驱动的保障来保护政策遵从代理免受对抗攻击。,The paper introduces a red-teaming system to test and improve the robustness of policy-adherent LLM agents against adversarial user behavior.,LLM,"Helpful, Harmless","Policy adherence, red-teaming, adversarial attacks, LLM agents, robustness"
Infected Smallville: How Disease Threat Shapes Sociality in LLM Agents,"Soyeon Choi, Kangwook Lee, Oliver Sng, Joshua M. Ackerman",2025-06-24T00:00:00-04:00,https://arxiv.org/pdf/2506.13783.pdf,"How does the threat of infectious disease influence sociality among generative agents? We used generative agent-based modeling (GABM), powered by large language models, to experimentally test hypotheses about the behavioral immune system. Across three simulation runs, generative agents who read news about an infectious disease outbreak showed significantly reduced social engagement compared to agents who received no such news, including lower attendance at a social gathering, fewer visits to third places (e.g., cafe, store, park), and fewer conversations throughout the town. In interview responses, agents explicitly attributed their behavioral changes to disease-avoidance motivations. A validity check further indicated that they could distinguish between infectious and noninfectious diseases, selectively reducing social engagement only when there was a risk of infection. Our findings highlight the potential of GABM as an experimental tool for exploring complex human social dynamics at scale.",传染病威胁如何影响大型语言模型代理的社会性？我们使用由大型语言模型驱动的生成代理基于建模（GABM）来实验性地测试关于行为免疫系统的假设。在三次模拟运行中，阅读有关传染病爆发新闻的生成代理显著减少了社会参与，包括参加社交聚会的次数减少、对第三方场所（例如咖啡馆、商店、公园）的访问减少以及全镇的对话减少。在访谈回答中，代理明确将其行为变化归因于疾病避免动机。进一步的有效性检查表明，它们能够区分传染性和非传染性疾病，仅在存在感染风险时选择性减少社会参与。我们的发现突显了GABM作为一种实验工具，用于探索复杂的人类社会动态的潜力。,The study uses large language models to simulate how the threat of infectious diseases affects social behavior in generative agents.,LLM,Harmless,"Generative agents, social behavior, disease threat, large language models, behavioral immune system"
