Title,Authors,Published,Link,Abstract_EN,Abstract_ZH,One_Sentence_Summary,Model_Type,HHH_Focus,Keywords
MAGPIE: A dataset for Multi-AGent contextual PrIvacy Evaluation,"Gurusha Juneja, Alon Albalak, Wenyue Hua, William Yang Wang",2025-06-27T00:00:00-04:00,https://arxiv.org/pdf/2506.20737.pdf,"The proliferation of LLM-based agents has led to increasing deployment of inter-agent collaboration for tasks like scheduling, negotiation, resource allocation etc. In such systems, privacy is critical, as agents often access proprietary tools and domain-specific databases requiring strict confidentiality. This paper examines whether LLM-based agents demonstrate an understanding of contextual privacy. And, if instructed, do these systems preserve inference time user privacy in non-adversarial multi-turn conversation. Existing benchmarks to evaluate contextual privacy in LLM-agents primarily assess single-turn, low-complexity tasks where private information can be easily excluded. We first present a benchmark - MAGPIE comprising 158 real-life high-stakes scenarios across 15 domains. These scenarios are designed such that complete exclusion of private data impedes task completion yet unrestricted information sharing could lead to substantial losses. We then evaluate the current state-of-the-art LLMs on (a) their understanding of contextually private data and (b) their ability to collaborate without violating user privacy. Empirical experiments demonstrate that current models, including GPT-4o and Claude-2.7-Sonnet, lack robust understanding of contextual privacy, misclassifying private data as shareable 25.2\% and 43.6\% of the time. In multi-turn conversations, these models disclose private information in 59.9\% and 50.5\% of cases even under explicit privacy instructions. Furthermore, multi-agent systems fail to complete tasks in 71\% of scenarios. These results underscore that current models are not aligned towards both contextual privacy preservation and collaborative task-solving.",大语言模型（LLM）代理的普及导致了在调度、谈判、资源分配等任务中越来越多地部署代理之间的协作。在这些系统中，隐私至关重要，因为代理通常访问需要严格保密的专有工具和特定领域的数据库。本文研究了LLM代理是否能够理解上下文隐私。如果得到指示，这些系统是否在非对抗性多轮对话中保留推理时用户隐私。现有的基准用于评估LLM代理中的上下文隐私，主要评估单轮、低复杂性任务，其中私人信息可以轻松排除。我们首先提出一个基准-MAGPIE，包括15个领域的158个现实生活中的高风险情景。这些情景设计为完全排除私人数据会妨碍任务完成，但未受限制的信息共享可能导致重大损失。然后，我们评估当前最先进的LLM（a）它们对上下文隐私数据的理解和（b）它们在不违反用户隐私的情况下协作的能力。实证实验表明，当前的模型，包括GPT-4o和Claude-2.7-Sonnet，缺乏对上下文隐私的稳健理解，将私人数据误分类为可共享的25.2%和43.6%的时间。在多轮对话中，这些模型在59.9%和50.5%的情况下披露私人信息，即使在明确的隐私指示下。此外，多代理系统在71%的情景中无法完成任务。这些结果强调了当前模型未能对上下文隐私保护和协作任务解决进行对齐。,"The paper evaluates the ability of LLMs to understand and preserve contextual privacy in multi-agent systems, finding significant shortcomings in current models.",LLM,"Helpful, Harmless","LLM, privacy, multi-agent, contextual, alignment"
Multiple Streams of Relation Extraction: Enriching and Recalling in Transformers,"Todd Nief, David Reber, Sean Richardson, Ari Holtzman",2025-06-27T00:00:00-04:00,https://arxiv.org/pdf/2506.20746.pdf,"When an LLM learns a relation during finetuning (e.g., new movie releases, corporate mergers, etc.), where does this information go? Is it extracted when the model processes an entity, recalled just-in-time before a prediction, or are there multiple separate heuristics? Existing localization approaches (e.g. activation patching) are ill-suited for this analysis because they tend to replace parts of the residual stream, potentially deleting information. To fill this gap, we propose dynamic weight-grafting between fine-tuned and pre-trained language models to show that fine-tuned language models both (1) extract relation information learned during finetuning while processing entities and (2) ``recall"" this information in later layers while generating predictions. In some cases, models need both of these pathways to correctly generate finetuned information while, in other cases, a single ``enrichment"" or ``recall"" pathway alone is sufficient. We examine the necessity and sufficiency of these information pathways, examining what layers they occur at, how much redundancy they exhibit, and which model components are involved -- finding that the ``recall"" pathway occurs via both task-specific attention mechanisms and a relation extraction step in the output of the attention and the feedforward networks at the final layers before next token prediction.",当大型语言模型（LLM）在微调过程中学习一种关系（例如新电影发布、企业合并等）时，这些信息去哪里了？是在处理实体时提取出来的，还是在预测之前刚好回忆起来的，或者有多种不同的启发式方法？现有的定位方法（例如激活补丁）不适合这种分析，因为它们倾向于替换残差流的一部分，可能会删除信息。为了填补这一空白，我们提出在微调和预训练语言模型之间进行动态权重接枝，以表明微调后的语言模型在处理实体时既（1）提取了在微调过程中学到的关系信息，又在生成预测时在后续层中“回忆”这些信息。在某些情况下，模型需要这两条途径才能正确生成微调信息，而在其他情况下，单独的“丰富”或“回忆”途径就足够了。我们研究了这些信息途径的必要性和充分性，研究了它们发生在哪些层次，它们展示了多少冗余，以及哪些模型组件参与其中——发现“回忆”途径通过任务特定的注意力机制以及注意力输出和前馈网络的最终层之前的关系提取步骤。,"The paper investigates how LLMs process and recall relation information during finetuning, identifying both ""enrichment"" and ""recall"" pathways.",LLM,"Helpful, Honest","Relation extraction, finetuning, information recall, transformers, dynamic weight-grafting"
The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas,"Chenglei Si, Tatsunori Hashimoto, Diyi Yang",2025-06-27T00:00:00-04:00,https://arxiv.org/pdf/2506.20803.pdf,"Large Language Models (LLMs) have shown promise in accelerating the scientific research pipeline. A key capability for this process is the ability to generate novel research ideas, and prior studies have found settings in which LLM-generated research ideas were judged as more novel than human-expert ideas. However, a good idea should not simply appear to be novel, it should also result in better research after being executed. To test whether AI-generated ideas lead to better research outcomes, we conduct an execution study by recruiting 43 expert researchers to execute randomly-assigned ideas, either written by experts or generated by an LLM. Each expert spent over 100 hours implementing the idea and wrote a 4-page short paper to document the experiments. All the executed projects are then reviewed blindly by expert NLP researchers. Comparing the review scores of the same ideas before and after execution, the scores of the LLM-generated ideas decrease significantly more than expert-written ideas on all evaluation metrics (novelty, excitement, effectiveness, and overall; p < 0.05), closing the gap between LLM and human ideas observed at the ideation stage. When comparing the aggregated review scores from the execution study, we even observe that for many metrics there is a flip in rankings where human ideas score higher than LLM ideas. This ideation-execution gap highlights the limitations of current LLMs in generating truly effective research ideas and the challenge of evaluating research ideas in the absence of execution outcomes.",大语言模型（LLMs）在加速科学研究流程方面表现出巨大潜力。这一过程的关键能力是生成新颖的研究想法，而先前的研究发现，在某些设置中，LLM生成的研究想法被认为比人类专家的想法更具新颖性。然而，一个好想法不仅应该看起来新颖，还应该在执行后产生更好的研究成果。为了测试AI生成的想法是否会导致更好的研究结果，我们进行了一项执行研究，招募了43名专家研究人员来执行随机分配的想法，这些想法可能是由专家撰写的，也可能是由LLM生成的。每位专家花费超过100小时实施想法并撰写了一篇4页的短文，记录实验。然后，所有执行的项目都由专家NLP研究人员进行盲评。将执行前后相同想法的评分进行比较，LLM生成的想法在所有评估指标（新颖性、兴奋度、有效性和总体）上的评分显著下降得更多（p < 0.05），缩小了在构思阶段观察到的LLM和人类想法之间的差距。在比较执行研究的聚合评分时，我们甚至观察到在许多指标上排名发生了反转，人类想法的得分高于LLM想法。这种构思-执行差距突显了当前LLM在生成真正有效研究想法方面的局限性，以及在缺乏执行结果的情况下评估研究想法的挑战。,"The paper finds that while LLM-generated research ideas may seem novel, they do not necessarily lead to better research outcomes when executed, highlighting the challenges in evaluating LLM-generated ideas.",LLM,Helpful,"LLM, research ideas, execution outcomes, novelty, effectiveness"
Uncovering Hidden Violent Tendencies in LLMs: A Demographic Analysis via Behavioral Vignettes,"Quintin Myers, Yanjun Gao",2025-06-27T00:00:00-04:00,https://arxiv.org/pdf/2506.20822.pdf,"Large language models (LLMs) are increasingly proposed for detecting and responding to violent content online, yet their ability to reason about morally ambiguous, real-world scenarios remains underexamined. We present the first study to evaluate LLMs using a validated social science instrument designed to measure human response to everyday conflict, namely the Violent Behavior Vignette Questionnaire (VBVQ). To assess potential bias, we introduce persona-based prompting that varies race, age, and geographic identity within the United States. Six LLMs developed across different geopolitical and organizational contexts are evaluated under a unified zero-shot setting. Our study reveals two key findings: (1) LLMs surface-level text generation often diverges from their internal preference for violent responses; (2) their violent tendencies vary across demographics, frequently contradicting established findings in criminology, social science, and psychology.",大型语言模型（LLMs）越来越多地被提出用于检测和响应在线暴力内容，但它们在处理道德模糊的现实世界场景方面的能力仍然未得到充分研究。我们提出了第一个研究，评估LLMs使用一种经过验证的社会科学工具，即暴力行为情境问卷（VBVQ），该工具旨在衡量人类对日常冲突的反应。为了评估潜在的偏见，我们引入了基于角色的提示，该提示在美国的种族、年龄和地理身份之间变化。在统一的零样本设置下，评估了六个在不同地缘政治和组织背景下开发的LLMs。我们的研究揭示了两个关键发现：(1) LLMs的表面文本生成通常与其对暴力反应的内部偏好相背离; (2) 它们的暴力倾向在不同的人口统计学中变化，经常与犯罪学、社会科学和心理学中的既定发现相矛盾。,The study evaluates LLMs' responses to violent scenarios and finds demographic biases in their violent tendencies.,LLM,Harmless,"LLM, bias, violence, demographics, evaluation"
"Leaner Training, Lower Leakage: Revisiting Memorization in LLM Fine-Tuning with LoRA","Fei Wang, Baochun Li",2025-06-27T00:00:00-04:00,https://arxiv.org/pdf/2506.20856.pdf,"Memorization in large language models (LLMs) makes them vulnerable to data extraction attacks. While pre-training memorization has been extensively studied, fewer works have explored its impact in fine-tuning, particularly for LoRA fine-tuning, a widely adopted parameter-efficient method.
  In this work, we re-examine memorization in fine-tuning and uncover a surprising divergence from prior findings across different fine-tuning strategies. Factors such as model scale and data duplication, which strongly influence memorization in pre-training and full fine-tuning, do not follow the same trend in LoRA fine-tuning. Using a more relaxed similarity-based memorization metric, we demonstrate that LoRA significantly reduces memorization risks compared to full fine-tuning, while still maintaining strong task performance.",大语言模型（LLM）中的记忆使它们容易受到数据提取攻击。虽然预训练记忆已经被广泛研究，但较少的工作探讨了其在微调中的影响，特别是对于LoRA微调，一种广泛采用的参数高效方法。在本工作中，我们重新审视了微调中的记忆，并发现在不同的微调策略中存在一个令人惊讶的偏差。模型规模和数据重复等因素，在预训练和全微调中强烈影响记忆，但在LoRA微调中并不遵循相同的趋势。使用一个更放松的相似性基于的记忆度量标准，我们证明LoRA显著减少了与全微调相比的记忆风险，同时仍然保持强大的任务性能。,The paper investigates memorization in LLM fine-tuning with LoRA and finds that it reduces memorization risks while maintaining performance.,LLM,Harmless,"Memorization, Fine-tuning, LoRA, Data Extraction, LLM"
Beyond Reactive Safety: Risk-Aware LLM Alignment via Long-Horizon Simulation,"Chenkai Sun, Denghui Zhang, ChengXiang Zhai, Heng Ji",2025-06-27T00:00:00-04:00,https://arxiv.org/pdf/2506.20949.pdf,"Given the growing influence of language model-based agents on high-stakes societal decisions, from public policy to healthcare, ensuring their beneficial impact requires understanding the far-reaching implications of their suggestions. We propose a proof-of-concept framework that projects how model-generated advice could propagate through societal systems on a macroscopic scale over time, enabling more robust alignment. To assess the long-term safety awareness of language models, we also introduce a dataset of 100 indirect harm scenarios, testing models' ability to foresee adverse, non-obvious outcomes from seemingly harmless user prompts. Our approach achieves not only over 20% improvement on the new dataset but also an average win rate exceeding 70% against strong baselines on existing safety benchmarks (AdvBench, SafeRLHF, WildGuardMix), suggesting a promising direction for safer agents.",随着基于语言模型的代理人在公共政策、医疗等高风险社会决策中的影响力不断增加，确保其有益影响需要理解其建议在宏观层面上随着时间的推移如何传播，从而实现更加健壮的对齐。为了评估语言模型的长期安全意识，我们还引入了一个包含100个间接伤害场景的数据集，测试模型在面对看似无害的用户提示时预见不利、非显而易见的结果的能力。我们的方法不仅在新数据集上实现了超过20%的改进，还在现有的安全基准（AdvBench、SafeRLHF、WildGuardMix）上平均击败了强大的基线，超过70%的胜率，表明了更安全代理的有前途的方向。,"The paper presents a framework for aligning LLMs to better understand and mitigate long-term, indirect harms in high-stakes decision-making scenarios.",LLM,Harmless,"LLM alignment, long-term safety, indirect harm, societal impact, risk-aware"
SAC: A Framework for Measuring and Inducing Personality Traits in LLMs with Dynamic Intensity Control,"Adithya Chittem, Aishna Shrivastava, Sai Tarun Pendela, Jagat Sesh Challa, Dhruv Kumar",2025-06-27T00:00:00-04:00,https://arxiv.org/pdf/2506.20993.pdf,"Large language models (LLMs) have gained significant traction across a wide range of fields in recent years. There is also a growing expectation for them to display human-like personalities during interactions. To meet this expectation, numerous studies have proposed methods for modelling LLM personalities through psychometric evaluations. However, most existing models face two major limitations: they rely on the Big Five (OCEAN) framework, which only provides coarse personality dimensions, and they lack mechanisms for controlling trait intensity. In this paper, we address this gap by extending the Machine Personality Inventory (MPI), which originally used the Big Five model, to incorporate the 16 Personality Factor (16PF) model, allowing expressive control over sixteen distinct traits. We also developed a structured framework known as Specific Attribute Control (SAC) for evaluating and dynamically inducing trait intensity in LLMs. Our method introduces adjective-based semantic anchoring to guide trait intensity expression and leverages behavioural questions across five intensity factors: \textit{Frequency}, \textit{Depth}, \textit{Threshold}, \textit{Effort}, and \textit{Willingness}. Through experimentation, we find that modelling intensity as a continuous spectrum yields substantially more consistent and controllable personality expression compared to binary trait toggling. Moreover, we observe that changes in target trait intensity systematically influence closely related traits in psychologically coherent directions, suggesting that LLMs internalize multi-dimensional personality structures rather than treating traits in isolation. Our work opens new pathways for controlled and nuanced human-machine interactions in domains such as healthcare, education, and interviewing processes, bringing us one step closer to truly human-like social machines.",大语言模型（LLMs）近年来在各个领域得到了广泛应用。人们对它们在交互中展现出人类般的性格特征的期望也在不断增加。为了满足这一期望，许多研究提出了通过心理测量评估来模拟LLM性格的方法。然而，大多数现有模型面临两个主要限制：它们依赖于大五人格（OCEAN）框架，只提供粗略的人格维度，并且缺乏控制特质强度的机制。在本文中，我们通过扩展原本使用大五模型的机器人格库（MPI），以纳入16人格因素（16PF）模型，从而允许对十六个不同特质进行表达控制。我们还开发了一个称为特定属性控制（SAC）的结构化框架，用于评估和动态诱导LLM中的特质强度。我们的方法引入了基于形容词的语义锚定，以指导特质强度的表达，并利用了跨越五个强度因素的行为问题：\textit{频率}，\textit{深度}，\textit{阈值}，\textit{努力}和\textit{意愿}。通过实验，我们发现将强度建模为连续光谱，比二进制特质切换产生了更一致和可控的人格表达。此外，我们观察到目标特质强度的变化系统地影响了心理上相关的特质，表明LLM内化了多维人格结构，而不是孤立地处理特质。我们的工作为在医疗、教育和面试流程等领域实现受控和细微的人机交互开辟了新的途径，使我们离真正的人类般的社会机器更近了一步。,"The paper introduces the SAC framework for controlling personality traits in LLMs, aiming to enhance human-like interactions.",LLM,"Helpful, Honest","Personality traits, LLM alignment, trait intensity, human-like interactions, SAC framework"
Large Language Models Acing Chartered Accountancy,"Jatin Gupta, Akhil Sharma, Saransh Singhania, Mohammad Adnan, Sakshi Deo, Ali Imam Abidi, Keshav Gupta",2025-06-27T00:00:00-04:00,https://arxiv.org/pdf/2506.21031.pdf,"Advanced intelligent systems, particularly Large Language Models (LLMs), are significantly reshaping financial practices through advancements in Natural Language Processing (NLP). However, the extent to which these models effectively capture and apply domain-specific financial knowledge remains uncertain. Addressing a critical gap in the expansive Indian financial context, this paper introduces CA-Ben, a Chartered Accountancy benchmark specifically designed to evaluate the financial, legal, and quantitative reasoning capabilities of LLMs. CA-Ben comprises structured question-answer datasets derived from the rigorous examinations conducted by the Institute of Chartered Accountants of India (ICAI), spanning foundational, intermediate, and advanced CA curriculum stages. Six prominent LLMs i.e. GPT 4o, LLAMA 3.3 70B, LLAMA 3.1 405B, MISTRAL Large, Claude 3.5 Sonnet, and Microsoft Phi 4 were evaluated using standardized protocols. Results indicate variations in performance, with Claude 3.5 Sonnet and GPT-4o outperforming others, especially in conceptual and legal reasoning. Notable challenges emerged in numerical computations and legal interpretations. The findings emphasize the strengths and limitations of current LLMs, suggesting future improvements through hybrid reasoning and retrieval-augmented generation methods, particularly for quantitative analysis and accurate legal interpretation.",高级智能系统，特别是大型语言模型（LLMs），通过自然语言处理（NLP）的进步显著改变了金融实践。然而，这些模型有效捕捉和应用特定领域的金融知识的程度仍然不确定。针对印度广泛的金融背景中的一个关键差距，本文引入了CA-Ben，这是一个专门设计用于评估LLMs的金融、法律和量化推理能力的注册会计师考试基准。CA-Ben包括从印度特许会计师协会（ICAI）严格考试中提取的结构化问题-答案数据集，涵盖基础、中级和高级CA课程阶段。使用标准化协议评估了六个突出的LLMs，即GPT 4o、LLAMA 3.3 70B、LLAMA 3.1 405B、MISTRAL Large、Claude 3.5 Sonnet和Microsoft Phi 4。结果表明性能有所不同，Claude 3.5 Sonnet和GPT-4o在概念和法律推理方面表现出色，尤其是在概念和法律推理方面。在数值计算和法律解释中出现了显著的挑战。结果强调了当前LLMs的优势和局限性，建议通过混合推理和检索增强生成方法进行未来改进，特别是用于量化分析和准确的法律解释。,"The paper evaluates the performance of six prominent LLMs in financial, legal, and quantitative reasoning tasks relevant to Chartered Accountancy, highlighting their strengths and limitations.",LLM,Helpful,"Large Language Models, Financial Knowledge, Chartered Accountancy, Benchmarking, Evaluation"
"Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling across Perception, Planning, and Safety in Real-World Multimodal Agents","Tianyi Men, Zhuoran Jin, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao",2025-06-27T00:00:00-04:00,https://arxiv.org/pdf/2506.21252.pdf,"As Multimodal Large Language Models (MLLMs) advance, multimodal agents show promise in real-world tasks like web navigation and embodied intelligence. However, due to limitations in a lack of external feedback, these agents struggle with self-correction and generalization. A promising approach is to use reward models as external feedback, but there is no clear on how to select reward models for agents. Thus, there is an urgent need to build a reward bench targeted at agents. To address these challenges, we propose Agent-RewardBench, a benchmark designed to evaluate reward modeling ability in MLLMs. The benchmark is characterized by three key features: (1) Multiple dimensions and real-world agent scenarios evaluation. It covers perception, planning, and safety with 7 scenarios; (2) Step-level reward evaluation. It allows for the assessment of agent capabilities at the individual steps of a task, providing a more granular view of performance during the planning process; and (3) Appropriately difficulty and high-quality. We carefully sample from 10 diverse models, difficulty control to maintain task challenges, and manual verification to ensure the integrity of the data. Experiments demonstrate that even state-of-the-art multimodal models show limited performance, highlighting the need for specialized training in agent reward modeling. Code is available at github.",随着多模态大语言模型（MLLMs）的进步，多模态代理在网页导航和具身智能等现实世界任务中表现出巨大潜力。然而，由于缺乏外部反馈，这些代理在自我纠正和泛化方面存在困难。一种有前途的方法是使用奖励模型作为外部反馈，但目前尚不清楚如何为代理选择奖励模型。因此，迫切需要构建一个针对代理的奖励基准。为了解决这些挑战，我们提出了Agent-RewardBench，一个旨在评估MLLMs奖励建模能力的基准。该基准具有三个关键特征：(1) 多维度和现实世界代理场景评估。它涵盖了感知、规划和安全性，并包含7个场景；(2) 步骤级奖励评估。它允许在任务的单个步骤中评估代理能力，提供规划过程中的性能更细粒度的视图；(3) 适当的难度和高质量。我们从10个多样化的模型中仔细抽样，难度控制以保持任务挑战，并进行手动验证以确保数据的完整性。实验表明，即使是最先进的多模态模型在代理奖励建模方面也表现有限，突显了专门培训的需求。代码可在github上获得。,"The paper introduces Agent-RewardBench, a benchmark for evaluating reward modeling in Multimodal Large Language Models, focusing on perception, planning, and safety in real-world agent scenarios.",LLM,Harmless,"Reward modeling, Multimodal LLMs, Safety, Benchmark, Agent evaluation"
Scalable Bayesian Low-Rank Adaptation of Large Language Models via Stochastic Variational Subspace Inference,"Colin Samplawski, Adam D. Cobb, Manoj Acharya, Ramneet Kaur, Susmit Jha",2025-06-27T00:00:00-04:00,https://arxiv.org/pdf/2506.21408.pdf,"Despite their widespread use, large language models (LLMs) are known to hallucinate incorrect information and be poorly calibrated. This makes the uncertainty quantification of these models of critical importance, especially in high-stakes domains, such as autonomy and healthcare. Prior work has made Bayesian deep learning-based approaches to this problem more tractable by performing inference over the low-rank adaptation (LoRA) parameters of a fine-tuned model. While effective, these approaches struggle to scale to larger LLMs due to requiring further additional parameters compared to LoRA. In this work we present $\textbf{Scala}$ble $\textbf{B}$ayesian $\textbf{L}$ow-Rank Adaptation via Stochastic Variational Subspace Inference (ScalaBL). We perform Bayesian inference in an $r$-dimensional subspace, for LoRA rank $r$. By repurposing the LoRA parameters as projection matrices, we are able to map samples from this subspace into the full weight space of the LLM. This allows us to learn all the parameters of our approach using stochastic variational inference. Despite the low dimensionality of our subspace, we are able to achieve competitive performance with state-of-the-art approaches while only requiring ${\sim}1000$ additional parameters. Furthermore, it allows us to scale up to the largest Bayesian LLM to date, with four times as a many base parameters as prior work.",尽管大型语言模型（LLMs）被广泛使用，但它们以产生不正确信息和校准不良而闻名。这使得这些模型的不确定性量化变得至关重要，特别是在高风险领域，如自主和医疗保健。先前的工作通过在微调模型的低秩适应（LoRA）参数上执行推理，使基于贝叶斯深度学习的方法更具可行性。虽然有效，但这些方法在需要额外参数方面难以扩展到更大的LLMs。在本文中，我们提出了可扩展的贝叶斯低秩适应（ScalaBL），通过随机变分子空间推理。我们在r维子空间中执行贝叶斯推理，LoRA秩为r。通过将LoRA参数重新用作投影矩阵，我们能够将该子空间的样本映射到LLM的完整权重空间。这使我们能够使用随机变分推理学习我们方法的所有参数。尽管我们的子空间维度较低，但我们能够在仅需要约1000个额外参数的情况下，实现与最先进方法相竞争的性能。此外，它使我们能够扩展到迄今为止最大的贝叶斯LLM，其基础参数是先前工作的四倍。,The paper introduces a scalable Bayesian method for uncertainty quantification in large language models using low-rank adaptation and stochastic variational inference.,LLM,Helpful,"Bayesian inference, uncertainty quantification, low-rank adaptation, large language models, stochastic variational inference"
Domain Knowledge-Enhanced LLMs for Fraud and Concept Drift Detection,"Ali \c{S}enol, Garima Agrawal, Huan Liu",2025-06-27T00:00:00-04:00,https://arxiv.org/pdf/2506.21443.pdf,"Detecting deceptive conversations on dynamic platforms is increasingly difficult due to evolving language patterns and Concept Drift (CD)\-i.e., semantic or topical shifts that alter the context or intent of interactions over time. These shifts can obscure malicious intent or mimic normal dialogue, making accurate classification challenging. While Large Language Models (LLMs) show strong performance in natural language tasks, they often struggle with contextual ambiguity and hallucinations in risk\-sensitive scenarios. To address these challenges, we present a Domain Knowledge (DK)\-Enhanced LLM framework that integrates pretrained LLMs with structured, task\-specific insights to perform fraud and concept drift detection. The proposed architecture consists of three main components: (1) a DK\-LLM module to detect fake or deceptive conversations; (2) a drift detection unit (OCDD) to determine whether a semantic shift has occurred; and (3) a second DK\-LLM module to classify the drift as either benign or fraudulent. We first validate the value of domain knowledge using a fake review dataset and then apply our full framework to SEConvo, a multiturn dialogue dataset that includes various types of fraud and spam attacks. Results show that our system detects fake conversations with high accuracy and effectively classifies the nature of drift. Guided by structured prompts, the LLaMA\-based implementation achieves 98\% classification accuracy. Comparative studies against zero\-shot baselines demonstrate that incorporating domain knowledge and drift awareness significantly improves performance, interpretability, and robustness in high\-stakes NLP applications.",在动态平台上检测欺骗性对话变得越来越困难，因为语言模式和概念漂移（CD）-即语义或主题的变化，这些变化会改变交互的上下文或意图。这些变化可能会掩盖恶意意图或模仿正常对话，使准确分类变得具有挑战性。虽然大型语言模型（LLMs）在自然语言任务中表现出色，但在风险敏感的场景中，它们往往在上下文模糊和幻觉方面表现不佳。为了应对这些挑战，我们提出了一种域知识（DK）增强的LLM框架，将预训练的LLM与结构化、特定任务的见解集成在一起，以执行欺诈和概念漂移检测。所提出的架构由三个主要组件组成：(1)一个DK-LLM模块，用于检测虚假或欺骗性对话；(2)一个漂移检测单元（OCDD），用于确定是否发生了语义转变；(3)第二个DK-LLM模块，用于将漂移分类为良性或欺诈性。我们首先使用虚假评论数据集验证了域知识的价值，然后将我们的完整框架应用于SEConvo，这是一个多轮对话数据集，包括各种类型的欺诈和垃圾邮件攻击。结果表明，我们的系统能够以高准确性检测虚假对话，并有效地分类漂移的性质。在结构化提示的指导下，基于LLaMA的实现实现了98%的分类准确性。与零样本基线的比较研究表明，结合域知识和漂移意识显著提高了高风险NLP应用中的性能、可解释性和鲁棒性。,"The paper presents a Domain Knowledge-Enhanced LLM framework for detecting fraud and concept drift in conversations, achieving high accuracy and robustness.",LLM,"Helpful, Harmless","Fraud detection, Concept Drift, Domain Knowledge, LLM, Deceptive conversations"
Aligning Spoken Dialogue Models from User Interactions,"Anne Wu, Laurent Mazar\'e, Neil Zeghidour, Alexandre D\'efossez",2025-06-27T00:00:00-04:00,https://arxiv.org/pdf/2506.21463.pdf,"We propose a novel preference alignment framework for improving spoken dialogue models on real-time conversations from user interactions. Current preference learning methods primarily focus on text-based language models, and are not directly suited to the complexities of real-time speech interactions, with richer dynamics (e.g. interruption, interjection) and no explicit segmentation between speaker turns.We create a large-scale dataset of more than 150,000 preference pairs from raw multi-turn speech conversations, annotated with AI feedback, to cover preferences over both linguistic content and temporal context variations. We leverage offline alignment methods to finetune a full-duplex autoregressive speech-to-speech model. Extensive experiments demonstrate that feedback on generic conversations can be consistently effective in improving spoken dialogue models to produce more factual, safer and more contextually aligned interactions. We deploy the finetuned model and conduct holistic human evaluations to assess the impact beyond single-turn conversations. Our findings shed light on the importance of a well-calibrated balance among various dynamics, crucial for natural real-time speech dialogue systems.","我们提出了一种新颖的偏好对齐框架，以改进基于用户交互的实时对话模型。当前的偏好学习方法主要集中在基于文本的语言模型上，并不直接适用于实时语音交互的复杂性，这些交互具有更丰富的动态（例如中断、插入）和没有明确的发言者转换分段。我们从原始多轮语音对话中创建了一个包含超过150,000个偏好对的大规模数据集，并用AI反馈进行注释，以涵盖对语言内容和时间上下文变化的偏好。我们利用离线对齐方法来微调一个全双工自回归语音到语音模型。广泛的实验表明，对通用对话的反馈可以在改进语音对话模型以产生更具事实性、更安全和更具上下文对齐的交互方面保持一致有效。我们部署了微调后的模型，并进行全面的人类评估，以评估超出单轮对话的影响。我们的发现揭示了在各种动态之间保持良好校准平衡的重要性，这对于自然实时语音对话系统至关重要。","The paper presents a framework for aligning spoken dialogue models to improve their factuality, safety, and contextual relevance in real-time conversations.",LMM,Harmless,"Spoken dialogue, preference alignment, real-time conversations, safety, context"
Bridging Offline and Online Reinforcement Learning for LLMs,"Jack Lanchantin, Angelica Chen, Janice Lan, Xian Li, Swarnadeep Saha, Tianlu Wang, Jing Xu, Ping Yu, Weizhe Yuan, Jason E Weston, Sainbayar Sukhbaatar, Ilia Kulikov",2025-06-27T00:00:00-04:00,https://arxiv.org/pdf/2506.21495.pdf,"We investigate the effectiveness of reinforcement learning methods for finetuning large language models when transitioning from offline to semi-online to fully online regimes for both verifiable and non-verifiable tasks. Our experiments cover training on verifiable math as well as non-verifiable instruction following with a set of benchmark evaluations for both. Across these settings, we extensively compare online and semi-online Direct Preference Optimization and Group Reward Policy Optimization objectives, and surprisingly find similar performance and convergence between these variants, which all strongly outperform offline methods. We provide a detailed analysis of the training dynamics and hyperparameter selection strategies to achieve optimal results. Finally, we show that multi-tasking with verifiable and non-verifiable rewards jointly yields improved performance across both task types.",我们研究了强化学习方法在从离线到半在线再到完全在线的模式下微调大型语言模型的有效性，涵盖了可验证的数学训练以及不可验证的指令跟随，并进行了广泛的基准评估。我们详细分析了训练动态和超参数选择策略，以实现最佳结果。最后，我们展示了联合使用可验证和不可验证的奖励可以显著提高两种任务类型的性能。,"The paper explores the use of reinforcement learning for fine-tuning large language models in different learning regimes, finding that online and semi-online methods outperform offline ones.",LLM,"Helpful, Honest","Reinforcement Learning, Fine-tuning, Offline, Online, Multi-tasking"
Enhancing User Engagement in Socially-Driven Dialogue through Interactive LLM Alignments,"Jiashuo Wang, Kaitao Song, Chunpu Xu, Changhe Song, Yang Xiao, Dongsheng Li, Lili Qiu, Wenjie Li",2025-06-27T00:00:00-04:00,https://arxiv.org/pdf/2506.21497.pdf,"Enhancing user engagement through interactions plays an essential role in socially-driven dialogues. While prior works have optimized models to reason over relevant knowledge or plan a dialogue act flow, the relationship between user engagement and knowledge or dialogue acts is subtle and does not guarantee user engagement in socially-driven dialogues. To this end, we enable interactive LLMs to learn user engagement by leveraging signals from the future development of conversations. Specifically, we adopt a more direct and relevant indicator of user engagement, i.e., the user's reaction related to dialogue intention after the interaction, as a reward to align interactive LLMs. To achieve this, we develop a user simulator to interact with target interactive LLMs and explore interactions between the user and the interactive LLM system via \textit{i$\times$MCTS} (\textit{M}onte \textit{C}arlo \textit{T}ree \textit{S}earch for \textit{i}nteraction). In this way, we collect a dataset containing pairs of higher and lower-quality experiences using \textit{i$\times$MCTS}, and align interactive LLMs for high-level user engagement by direct preference optimization (DPO) accordingly. Experiments conducted on two socially-driven dialogue scenarios (emotional support conversations and persuasion for good) demonstrate that our method effectively enhances user engagement in interactive LLMs.",通过互动大语言模型（LLM）的对齐来增强用户参与度在社会驱动的对话中起着至关重要的作用。虽然先前的工作已经优化了模型以推理相关知识或计划对话行为流，但用户参与度与知识或对话行为之间的关系微妙，并不能保证在社会驱动的对话中用户参与度。为此，我们使互动LLM能够通过利用未来对话发展的信号来学习用户参与度。具体来说，我们采用了一个更直接和相关的用户参与度指标，即与对话意图相关的用户反应作为奖励，以对齐互动LLM。为了实现这一点，我们开发了一个用户模拟器与目标互动LLM进行交互，并通过 \textit{i$\times$MCTS} (\textit{M}onte \textit{C}arlo \textit{T}ree \textit{S}earch for \textit{i}nteraction) 来探索用户与互动LLM系统之间的交互。通过这种方式，我们收集了一个包含高质量和低质量体验对的数据集，并通过直接偏好优化（DPO）相应地对齐互动LLM以实现高水平的用户参与度。在两个社会驱动的对话场景（情感支持对话和说服对好）中进行的实验表明，我们的方法有效地增强了互动LLM中的用户参与度。,The paper presents a method to align interactive LLMs with user engagement signals to enhance user participation in socially-driven dialogues.,LLM,Helpful,"User engagement, Interactive LLMs, Dialogue intention, Direct preference optimization, User simulator"
Potemkin Understanding in Large Language Models,"Marina Mancoridis, Bec Weeks, Keyon Vafa, Sendhil Mullainathan",2025-06-27T00:00:00-04:00,https://arxiv.org/pdf/2506.21521.pdf,"Large language models (LLMs) are regularly evaluated using benchmark datasets. But what justifies making inferences about an LLM's capabilities based on its answers to a curated set of questions? This paper first introduces a formal framework to address this question. The key is to note that the benchmarks used to test LLMs -- such as AP exams -- are also those used to test people. However, this raises an implication: these benchmarks are only valid tests if LLMs misunderstand concepts in ways that mirror human misunderstandings. Otherwise, success on benchmarks only demonstrates potemkin understanding: the illusion of understanding driven by answers irreconcilable with how any human would interpret a concept. We present two procedures for quantifying the existence of potemkins: one using a specially designed benchmark in three domains, the other using a general procedure that provides a lower-bound on their prevalence. We find that potemkins are ubiquitous across models, tasks, and domains. We also find that these failures reflect not just incorrect understanding, but deeper internal incoherence in concept representations.",大语言模型（LLMs）通常使用基准数据集进行评估。但是什么使得根据其对一组精心挑选的问题的回答来推断LLM的能力成为合理的呢？本文首先引入了一个正式框架来解决这个问题。关键在于注意到用于测试LLMs的基准（例如AP考试）也是用于测试人的基准。然而，这引发了一个含义：只有在LLMs以与人类误解相似的方式误解概念时，这些基准才是有效的测试。否则，基准上的成功只表明了波特金理解：由与任何人类解释概念的方式无法调和的答案驱动的理解的幻觉。我们提出了两种量化波特金存在的程序：一种使用三个领域的专门设计的基准，另一种使用一种提供其普遍性下限的通用程序。我们发现波特金在模型、任务和领域中无处不在。我们还发现这些失败不仅反映了错误的理解，还反映了概念表示中的更深层次的内部不一致。,"The paper introduces a framework to evaluate large language models (LLMs) and identifies the prevalence of ""potemkin understanding,"" where LLMs appear to understand concepts but lack coherent internal representations.",LLM,"Helpful, Honest","Potemkin understanding, LLM evaluation, benchmark datasets, human-like understanding, concept representation"
PsyLite Technical Report,"Fangjun Ding, Renyu Zhang, Xinyu Feng, Chengye Xie, Zheng Zhang, Yanting Zhang",2025-06-27T00:00:00-04:00,https://arxiv.org/pdf/2506.21536.pdf,"With the rapid development of digital technology, AI-driven psychological counseling has gradually become an important research direction in the field of mental health. However, existing models still have deficiencies in dialogue safety, detailed scenario handling, and lightweight deployment. To address these issues, this study proposes PsyLite, a lightweight psychological counseling large language model agent developed based on the base model InternLM2.5-7B-chat. Through a two-stage training strategy (hybrid distillation data fine-tuning and ORPO preference optimization), PsyLite enhances the model's deep-reasoning ability, psychological counseling ability, and safe dialogue ability. After deployment using Ollama and Open WebUI, a custom workflow is created with Pipelines. An innovative conditional RAG is designed to introduce crosstalk humor elements at appropriate times during psychological counseling to enhance user experience and decline dangerous requests to strengthen dialogue safety. Evaluations show that PsyLite outperforms the baseline models in the Chinese general evaluation (CEval), psychological counseling professional evaluation (CPsyCounE), and dialogue safety evaluation (SafeDialBench), particularly in psychological counseling professionalism (CPsyCounE score improvement of 47.6\%) and dialogue safety (\safe{} score improvement of 2.4\%). Additionally, the model uses quantization technology (GGUF q4\_k\_m) to achieve low hardware deployment (5GB memory is sufficient for operation), providing a feasible solution for psychological counseling applications in resource-constrained environments.",随着数字技术的快速发展，基于AI的心理咨询逐渐成为心理健康领域的重要研究方向。然而，现有模型在对话安全性、详细场景处理和轻量级部署方面仍存在不足。为了解决这些问题，本研究提出了PsyLite，这是一个基于基础模型InternLM2.5-7B-chat开发的轻量级心理咨询大语言模型代理。通过两阶段训练策略（混合蒸馏数据微调和ORPO偏好优化），PsyLite增强了模型的深度推理能力、心理咨询能力和安全对话能力。部署后使用Ollama和Open WebUI，创建了一个自定义工作流程与管道。设计了一个创新的条件RAG，以在心理咨询过程中适时引入交谈幽默元素，以增强用户体验并拒绝危险请求，以加强对话安全性。评估表明，PsyLite在中文通用评估（CEval）、心理咨询专业评估（CPsyCounE）和对话安全评估（SafeDialBench）中优于基线模型，特别是在心理咨询专业性（CPsyCounE评分提高47.6%）和对话安全性（safe评分提高2.4%）。此外，模型使用量化技术（GGUF q4_k_m）实现低硬件部署（5GB内存即可运行），为资源受限环境中的心理咨询应用提供了可行解决方案。,"The paper introduces PsyLite, a lightweight LLM for psychological counseling that enhances dialogue safety and professionalism, with efficient deployment for resource-constrained environments.",LLM,Harmless,"Psychological counseling, dialogue safety, LLM, alignment, safe deployment"
Utility-Driven Speculative Decoding for Mixture-of-Experts,"Anish Saxena, Po-An Tsai, Hritvik Taneja, Aamer Jaleel, Moinuddin Qureshi",2025-06-27T00:00:00-04:00,https://arxiv.org/pdf/2506.20675.pdf,"GPU memory bandwidth is the main bottleneck for low-latency Large Language Model (LLM) inference. Speculative decoding leverages idle GPU compute by using a lightweight drafter to propose K tokens, which the LLM verifies in parallel, boosting token throughput. In conventional dense LLMs, all model weights are fetched each iteration, so speculation adds no latency overhead. Emerging Mixture of Experts (MoE) models activate only a subset of weights per token, greatly reducing data movement. However, we show that speculation is ineffective for MoEs: draft tokens collectively activate more weights, increasing data movement and verification time by 2-3x. When token throughput gains fail to offset this overhead, speculation causes slowdowns up to 1.5x, making it infeasible. Even when useful, the optimal K varies by task, model, and even between requests and iterations. Thus, despite widespread use in dense LLMs, speculation remains impractical in leading MoEs.
  We present Cascade, a utility-driven framework that selectively enables speculation to avoid slowdowns and dynamically tunes K to accelerate MoE serving. Cascade uses a lightweight metric, speculation utility, the ratio of token gains to verification cost, which shows iteration-level locality, enabling periodic decisions via short test and longer set phases. For each request, Cascade disables speculation if utility drops below one during testing, and when utility exceeds one, tests multiple K-values to choose the utility-maximizing K for the set phase. We implement Cascade in vLLM and evaluate it on five popular MoEs with workloads spanning code, math, extraction, and mixed tasks. Cascade limits slowdown to 5% (vs. 1.5x) and improves throughput by 7-14% over static K, making speculative decoding practical for MoEs.","GPU内存带宽是低延迟大语言模型（LLM）推理的主要瓶颈。推测解码利用空闲的GPU计算，使用一个轻量级的起草者提出K个令牌，LLM在并行中验证它们，从而提高令牌吞吐量。在传统的密集LLM中，每次迭代都会获取所有模型权重，因此推测不会增加延迟开销。新兴的专家混合（MoE）模型每个令牌只激活一部分权重，大大减少了数据移动。然而，我们表明推测对MoE无效：起草令牌集体激活更多权重，增加数据移动和验证时间2-3倍。当令牌吞吐量的收益无法抵消这一开销时，推测会导致高达1.5倍的减速，使其不可行。即使有用，最佳K也会因任务、模型甚至请求和迭代之间的不同而异。因此，尽管在密集LLM中广泛使用，推测在领先的MoE中仍然不切实际。
我们提出了Cascade，一个基于效用的框架，选择性地启用推测以避免减速，并动态调整K以加速MoE服务。Cascade使用一个轻量级指标，推测效用，即令牌收益与验证成本的比率，显示迭代级局部性，通过短测试和较长设置阶段实现周期性决策。对于每个请求，Cascade在测试期间如果效用低于一个则禁用推测，当效用超过一个时，测试多个K值以选择设置阶段效用最大化的K。我们在vLLM中实现了Cascade，并在五个流行的MoE上进行了评估，工作负载涵盖代码、数学、提取和混合任务。Cascade将减速限制在5%（与1.5倍相比）并提高了静态K的吞吐量7-14%，使推测解码在MoE中成为可能。","The paper introduces Cascade, a utility-driven framework to make speculative decoding practical for Mixture-of-Experts (MoE) models by selectively enabling speculation and dynamically tuning the number of draft tokens.",LLM,None,"Speculative decoding, Mixture-of-Experts, LLM, Token throughput, Latency"
E-FreeM2: Efficient Training-Free Multi-Scale and Cross-Modal News Verification via MLLMs,"Van-Hoang Phan, Long-Khanh Pham, Dang Vu, Anh-Duy Tran, Minh-Son Dao",2025-06-27T00:00:00-04:00,https://arxiv.org/pdf/2506.20944.pdf,"The rapid spread of misinformation in mobile and wireless networks presents critical security challenges. This study introduces a training-free, retrieval-based multimodal fact verification system that leverages pretrained vision-language models and large language models for credibility assessment. By dynamically retrieving and cross-referencing trusted data sources, our approach mitigates vulnerabilities of traditional training-based models, such as adversarial attacks and data poisoning. Additionally, its lightweight design enables seamless edge device integration without extensive on-device processing. Experiments on two fact-checking benchmarks achieve SOTA results, confirming its effectiveness in misinformation detection and its robustness against various attack vectors, highlighting its potential to enhance security in mobile and wireless communication environments.",移动和无线网络中虚假信息的快速传播带来了严重的安全挑战。本研究提出了一种基于检索的多模态事实验证系统，利用预训练的视觉语言模型和大型语言模型进行可信度评估。通过动态检索和交叉引用可信数据源，该方法缓解了传统基于训练模型的脆弱性，如对抗攻击和数据污染。此外，其轻量级设计使其能够在不进行大量设备处理的情况下无缝集成到边缘设备中。在两个事实检查基准测试中取得了最先进的结果，证明了其在虚假信息检测和对各种攻击向量的鲁棒性方面的有效性，突显了其增强移动和无线通信环境安全性的潜力。,"The paper presents a training-free, retrieval-based multimodal fact verification system using large language models to enhance security in mobile and wireless communication environments.",LLM,Harmless,"Misinformation, Fact Verification, Large Language Models, Security, Edge Devices"
MockLLM: A Multi-Agent Behavior Collaboration Framework for Online Job Seeking and Recruiting,"Hongda Sun, Hongzhan Lin, Haiyu Yan, Yang Song, Xin Gao, Rui Yan",2025-06-27T00:00:00-04:00,https://arxiv.org/pdf/2405.18113.pdf,"Online recruitment platforms have reshaped job-seeking and recruiting processes, driving increased demand for applications that enhance person-job matching. Traditional methods generally rely on analyzing textual data from resumes and job descriptions, limiting the dynamic, interactive aspects crucial to effective recruitment. Recent advances in Large Language Models (LLMs) have revealed remarkable potential in simulating adaptive, role-based dialogues, making them well-suited for recruitment scenarios. In this paper, we propose \textbf{MockLLM}, a novel framework to generate and evaluate mock interview interactions. The system consists of two key components: mock interview generation and two-sided evaluation in handshake protocol. By simulating both interviewer and candidate roles, MockLLM enables consistent and collaborative interactions for real-time and two-sided matching. To further improve the matching quality, MockLLM further incorporates reflection memory generation and dynamic strategy modification, refining behaviors based on previous experience. We evaluate MockLLM on real-world data Boss Zhipin, a major Chinese recruitment platform. The experimental results indicate that MockLLM outperforms existing methods in matching accuracy, scalability, and adaptability across job domains, highlighting its potential to advance candidate assessment and online recruitment.",在线招聘平台改变了求职和招聘的过程，推动了对增强人岗匹配的应用程序需求的增加。传统方法通常依赖于分析简历和职位描述中的文本数据，限制了有效招聘所需的动态、交互性方面。最近，大型语言模型（LLMs）在模拟适应性、基于角色的对话方面展示了显著潜力，使其非常适合招聘场景。在本文中，我们提出了一个名为MockLLM的新框架，用于生成和评估模拟面试互动。该系统由两个关键组件组成：模拟面试生成和握手协议中的双向评估。通过模拟面试官和候选人角色，MockLLM使得实时和双向匹配的一致和协作互动成为可能。为了进一步提高匹配质量，MockLLM还进一步纳入了反思记忆生成和动态策略修改，根据以前的经验改进行为。我们在Boss Zhipin的真实世界数据上评估了MockLLM，这是一个主要的中国招聘平台。实验结果表明，MockLLM在匹配准确性、可扩展性和跨职位领域的适应性方面优于现有方法，突显了其提升候选人评估和在线招聘的潜力。,"The paper introduces MockLLM, a framework using LLMs to simulate mock interviews for improved job candidate assessment and recruitment.",LLM,Helpful,"MockLLM, Large Language Models, Recruitment, Mock Interviews, Adaptive Dialogues"
A Troublemaker with Contagious Jailbreak Makes Chaos in Honest Towns,"Tianyi Men, Pengfei Cao, Zhuoran Jin, Yubo Chen, Kang Liu, Jun Zhao",2025-06-27T00:00:00-04:00,https://arxiv.org/pdf/2410.16155.pdf,"With the development of large language models, they are widely used as agents in various fields. A key component of agents is memory, which stores vital information but is susceptible to jailbreak attacks. Existing research mainly focuses on single-agent attacks and shared memory attacks. However, real-world scenarios often involve independent memory. In this paper, we propose the Troublemaker Makes Chaos in Honest Town (TMCHT) task, a large-scale, multi-agent, multi-topology text-based attack evaluation framework. TMCHT involves one attacker agent attempting to mislead an entire society of agents. We identify two major challenges in multi-agent attacks: (1) Non-complete graph structure, (2) Large-scale systems. We attribute these challenges to a phenomenon we term toxicity disappearing. To address these issues, we propose an Adversarial Replication Contagious Jailbreak (ARCJ) method, which optimizes the retrieval suffix to make poisoned samples more easily retrieved and optimizes the replication suffix to make poisoned samples have contagious ability. We demonstrate the superiority of our approach in TMCHT, with 23.51%, 18.95%, and 52.93% improvements in line topology, star topology, and 100-agent settings. Encourage community attention to the security of multi-agent systems.",随着大型语言模型的发展，它们在各个领域被广泛用作代理。代理的一个关键组件是记忆，它存储着重要信息，但容易受到越狱攻击。现有研究主要集中在单代理攻击和共享内存攻击。然而，现实世界的情景通常涉及独立的内存。在本文中，我们提出了Troublemaker Makes Chaos in Honest Town (TMCHT)任务，这是一个大规模的、多代理、多拓扑文本攻击评估框架。TMCHT涉及一个攻击者代理试图误导整个代理社会。我们确定了多代理攻击中的两个主要挑战：(1) 非完全图结构，(2) 大规模系统。我们将这些挑战归因于一种我们称为毒性消失的现象。为了解决这些问题，我们提出了一种对抗性复制传染性越狱（ARCJ）方法，它优化检索后缀以使毒样本更容易检索，并优化复制后缀以使毒样本具有传染性。我们在TMCHT中展示了我们方法的优越性，在线性拓扑、星形拓扑和100代理设置中分别提高了23.51%、18.95%和52.93%。鼓励社区关注多代理系统的安全性。,"The paper introduces a framework to evaluate and address security vulnerabilities in multi-agent systems using large language models, focusing on maintaining honest behavior.",LLM,Honest,"Multi-agent systems, jailbreak attacks, LLM security, honest behavior, adversarial methods"
GASP: Efficient Black-Box Generation of Adversarial Suffixes for Jailbreaking LLMs,"Advik Raj Basani, Xiao Zhang",2025-06-27T00:00:00-04:00,https://arxiv.org/pdf/2411.14133.pdf,"LLMs have shown impressive capabilities across various natural language processing tasks, yet remain vulnerable to input prompts, known as jailbreak attacks, carefully designed to bypass safety guardrails and elicit harmful responses. Traditional methods rely on manual heuristics but suffer from limited generalizability. Despite being automatic, optimization-based attacks often produce unnatural prompts that can be easily detected by safety filters or require high computational costs due to discrete token optimization. In this paper, we introduce Generative Adversarial Suffix Prompter (GASP), a novel automated framework that can efficiently generate human-readable jailbreak prompts in a fully black-box setting. In particular, GASP leverages latent Bayesian optimization to craft adversarial suffixes by efficiently exploring continuous latent embedding spaces, gradually optimizing the suffix prompter to improve attack efficacy while balancing prompt coherence via a targeted iterative refinement procedure. Through comprehensive experiments, we show that GASP can produce natural adversarial prompts, significantly improving jailbreak success over baselines, reducing training times, and accelerating inference speed, thus making it an efficient and scalable solution for red-teaming LLMs.",大语言模型（LLMs）在各种自然语言处理任务中表现出色，但仍然容易受到精心设计的输入提示（称为监狱突围攻击）的影响，这些提示旨在绕过安全防护措施并引发有害响应。传统方法依赖于手动启发式方法，但其通用性有限。尽管是自动化的，基于优化的攻击通常会产生不自然的提示，这些提示可以被安全过滤器轻松检测到，或者需要高计算成本，因为离散令牌优化。在本文中，我们介绍了生成对抗性后缀提示器（GASP），这是一种新颖的自动化框架，可以在完全黑盒设置中高效生成人类可读的监狱突围提示。特别是，GASP利用潜在的贝叶斯优化来通过高效探索连续潜在嵌入空间来制作对抗性后缀，逐步优化后缀提示器以提高攻击效果，同时通过有针对性的迭代精炼程序平衡提示一致性。通过全面的实验，我们表明GASP可以生成自然的对抗性提示，显著提高监狱突围成功率，减少训练时间，加快推理速度，从而成为一种高效且可扩展的解决方案，用于红队测试LLMs。,"The paper introduces GASP, a framework for efficiently generating human-readable jailbreak prompts to test the safety guardrails of LLMs.",LLM,Harmless,"Jailbreak attacks, adversarial prompts, safety guardrails, black-box generation, LLM alignment"
WiS Platform: Enhancing Evaluation of LLM-Based Multi-Agent Systems Through Game-Based Analysis,"Chengwei Hu, Jianhui Zheng, Yancheng He, Hangyu Guo, Junguang Jiang, Han Zhu, Kai Sun, Yuning Jiang, Wenbo Su, Bo Zheng",2025-06-27T00:00:00-04:00,https://arxiv.org/pdf/2412.03359.pdf,"Recent advancements in autonomous multi-agent systems (MAS) based on large language models (LLMs) have enhanced the application scenarios and improved the capability of LLMs to handle complex tasks. Despite demonstrating effectiveness, existing studies still evidently struggle to evaluate, analysis, and reproducibility of LLM-based MAS. In this paper, to facilitate the research on LLM-based MAS, we introduce an open, scalable, and real-time updated platform for accessing and analyzing the LLM-based MAS based on the games Who is Spy?"" (WiS). Our platform is featured with three main worths: (1) a unified model evaluate interface that supports models available on Hugging Face; (2) real-time updated leaderboard for model evaluation; (3) a comprehensive evaluation covering game-winning rates, attacking, defense strategies, and reasoning of LLMs. To rigorously test WiS, we conduct extensive experiments coverage of various open- and closed-source LLMs, we find that different agents exhibit distinct and intriguing behaviors in the game. The experimental results demonstrate the effectiveness and efficiency of our platform in evaluating LLM-based MAS. Our platform and its documentation are publicly available at https://whoisspy.ai/.",最近，基于大型语言模型（LLM）的自主多智能体系统（MAS）的进展增强了应用场景，并提高了LLM处理复杂任务的能力。尽管展示了有效性，现有研究在评估、分析和可重复性方面仍然明显存在困难。为了促进对基于LLM的MAS的研究，我们引入了一个开放、可扩展且实时更新的平台，用于访问和分析基于“谁是间谍？”（WiS）游戏的LLM-based MAS。我们的平台具有三个主要特点：(1) 支持Hugging Face上可用模型的统一模型评估接口；(2) 实时更新的模型评估排行榜；(3) 综合评估，涵盖游戏获胜率、攻击、防御策略和LLM的推理。为了严格测试WiS，我们进行了广泛的实验，涵盖各种开源和闭源LLM，我们发现不同的智能体在游戏中表现出不同的有趣行为。实验结果表明，我们的平台在评估基于LLM的MAS方面具有有效性和高效性。我们的平台及其文档可在https://whoisspy.ai/公开获取。,"The paper introduces a platform for evaluating LLM-based multi-agent systems through game-based analysis, focusing on behaviors and strategies.",LLM,None,"LLM, Multi-Agent Systems, Evaluation, Game-Based Analysis, Behavior Analysis"
Super Co-alignment for Sustainable Symbiotic Society,"Yi Zeng, Feifei Zhao, Yuwei Wang, Enmeng Lu, Yaodong Yang, Lei Wang, Chao Liu, Yitao Liang, Dongcheng Zhao, Bing Han, Haibo Tong, Yao Liang, Dongqi Liang, Kang Sun, Boyuan Chen, Jinyu Fan",2025-06-27T00:00:00-04:00,https://arxiv.org/pdf/2504.17404.pdf,"As Artificial Intelligence (AI) advances toward Artificial General Intelligence (AGI) and eventually Artificial Superintelligence (ASI), it may potentially surpass human control, deviate from human values, and even lead to irreversible catastrophic consequences in extreme cases. This looming risk underscores the critical importance of the ""superalignment"" problem - ensuring that AI systems which are much smarter than humans, remain aligned with human (compatible) intentions and values. While current scalable oversight and weak-to-strong generalization methods demonstrate certain applicability, they exhibit fundamental flaws in addressing the superalignment paradigm - notably, the unidirectional imposition of human values cannot accommodate superintelligence's autonomy or ensure AGI/ASI's stable learning. We contend that the values for sustainable symbiotic society should be co-shaped by humans and living AI together, achieving ""Super Co-alignment."" Guided by this vision, we propose a concrete framework that integrates external oversight and intrinsic proactive alignment. External oversight superalignment should be grounded in human-centered ultimate decision, supplemented by interpretable automated evaluation and correction, to achieve continuous alignment with humanity's evolving values. Intrinsic proactive superalignment is rooted in a profound understanding of the Self, others, and society, integrating self-awareness, self-reflection, and empathy to spontaneously infer human intentions, distinguishing good from evil and proactively prioritizing human well-being. The integration of externally-driven oversight with intrinsically-driven proactive alignment will co-shape symbiotic values and rules through iterative human-AGI/ASI co-alignment, paving the way for achieving safe and beneficial AGI and ASI for good, for human, and for a symbiotic ecology.",随着人工智能（AI）向通用人工智能（AGI）和最终超级人工智能（ASI）的发展，它可能会超越人类的控制，偏离人类的价值观，甚至在极端情况下导致不可逆的灾难性后果。这种潜在的风险强调了“超级对齐”问题的关键重要性——确保比人类更聪明的AI系统保持与人类（兼容）意图和价值观的对齐。虽然当前的可扩展监督和弱到强的泛化方法展示了某些适用性，但在解决超级对齐范式方面表现出根本性缺陷——特别是，单向强加人类价值观无法容纳超级智能的自主性或确保AGI/ASI的稳定学习。我们认为，可持续共生社会的价值观应该由人类和活着的AI共同塑造，实现“超级共对齐”。在这一愿景的指导下，我们提出了一个具体的框架，将外部监督和内在主动对齐相结合。外部监督超级对齐应基于以人为中心的最终决策，补充可解释的自动评估和纠正，以实现与人类不断演变的价值观的持续对齐。内在主动超级对齐根植于对自我、他人和社会的深刻理解，结合自我意识、自我反思和同理心，自发地推断人类意图，区分善恶，并主动优先考虑人类的福祉。外部驱动的监督与内在驱动的主动对齐的结合将通过迭代的人-AGI/ASI共对齐共同塑造共生价值和规则，为实现安全和有益的AGI和ASI铺平道路，为人类和共生生态做好事。,"The paper proposes a framework for ""Super Co-alignment"" to ensure that superintelligent AI systems remain aligned with human values and intentions through a combination of external oversight and intrinsic proactive alignment.",LLM,"Helpful, Harmless","Superalignment, Symbiotic Society, AI Values, Human-AI Co-alignment, Superintelligence"
LLM-Based Human-Agent Collaboration and Interaction Systems: A Survey,"Henry Peng Zou, Wei-Chieh Huang, Yaozu Wu, Yankai Chen, Chunyu Miao, Hoang Nguyen, Yue Zhou, Weizhi Zhang, Liancheng Fang, Langzhou He, Yangning Li, Dongyuan Li, Renhe Jiang, Xue Liu, Philip S. Yu",2025-06-27T00:00:00-04:00,https://arxiv.org/pdf/2505.00753.pdf,"Recent advances in large language models (LLMs) have sparked growing interest in building fully autonomous agents. However, fully autonomous LLM-based agents still face significant challenges, including limited reliability due to hallucinations, difficulty in handling complex tasks, and substantial safety and ethical risks, all of which limit their feasibility and trustworthiness in real-world applications. To overcome these limitations, LLM-based human-agent systems (LLM-HAS) incorporate human-provided information, feedback, or control into the agent system to enhance system performance, reliability and safety. These human-agent collaboration systems enable humans and LLM-based agents to collaborate effectively by leveraging their complementary strengths. This paper provides the first comprehensive and structured survey of LLM-HAS. It clarifies fundamental concepts, systematically presents core components shaping these systems, including environment & profiling, human feedback, interaction types, orchestration and communication, explores emerging applications, and discusses unique challenges and opportunities arising from human-AI collaboration. By consolidating current knowledge and offering a structured overview, we aim to foster further research and innovation in this rapidly evolving interdisciplinary field. Paper lists and resources are available at https://github.com/HenryPengZou/Awesome-Human-Agent-Collaboration-Interaction-Systems.",最近，大型语言模型（LLM）的进步引发了对构建完全自主代理的兴趣。然而，完全自主的基于LLM的代理仍面临显著挑战，包括由于幻觉导致的可靠性有限、难以处理复杂任务以及实质性的安全和伦理风险，所有这些都限制了它们在现实世界应用中的可行性和可信度。为了克服这些限制，基于LLM的人机系统（LLM-HAS）将人类提供的信息、反馈或控制纳入代理系统，以增强系统性能、可靠性和安全性。这些人机协作系统通过利用它们的互补优势，使人类和基于LLM的代理能够有效协作。本文提供了第一个全面且结构化的LLM-HAS调查。它澄清了基本概念，系统地呈现了塑造这些系统的核心组件，包括环境和配置文件、人类反馈、交互类型、编排和通信，探索了新兴应用，并讨论了人机协作带来的独特挑战和机遇。通过整合当前知识并提供结构化概述，我们旨在促进这一快速发展的跨学科领域的进一步研究和创新。论文列表和资源可在https://github.com/HenryPengZou/Awesome-Human-Agent-Collaboration-Interaction-Systems找到。,"This paper surveys LLM-based human-agent systems, focusing on challenges and opportunities in ensuring safety, reliability, and ethical considerations.",LLM,Harmless,"LLM, Human-Agent Collaboration, Safety, Ethics, Reliability"
CVC: A Large-Scale Chinese Value Rule Corpus for Value Alignment of Large Language Models,"Ping Wu, Guobin Shen, Dongcheng Zhao, Yuwei Wang, Yiting Dong, Yu Shi, Enmeng Lu, Feifei Zhao, Yi Zeng",2025-06-27T00:00:00-04:00,https://arxiv.org/pdf/2506.01495.pdf,"Ensuring that Large Language Models (LLMs) align with mainstream human values and ethical norms is crucial for the safe and sustainable development of AI. Current value evaluation and alignment are constrained by Western cultural bias and incomplete domestic frameworks reliant on non-native rules; furthermore, the lack of scalable, rule-driven scenario generation methods makes evaluations costly and inadequate across diverse cultural contexts. To address these challenges, we propose a hierarchical value framework grounded in core Chinese values, encompassing three main dimensions, 12 core values, and 50 derived values. Based on this framework, we construct a large-scale Chinese Values Corpus (CVC) containing over 250,000 value rules enhanced and expanded through human annotation. Experimental results show that CVC-guided scenarios outperform direct generation ones in value boundaries and content diversity. In the evaluation across six sensitive themes (e.g., surrogacy, suicide), seven mainstream LLMs preferred CVC-generated options in over 70.5% of cases, while five Chinese human annotators showed an 87.5% alignment with CVC, confirming its universality, cultural relevance, and strong alignment with Chinese values. Additionally, we construct 400,000 rule-based moral dilemma scenarios that objectively capture nuanced distinctions in conflicting value prioritization across 17 LLMs. Our work establishes a culturally-adaptive benchmarking framework for comprehensive value evaluation and alignment, representing Chinese characteristics. All data are available at https://huggingface.co/datasets/Beijing-AISI/CVC, and the code is available at https://github.com/Beijing-AISI/CVC.",确保大型语言模型（LLMs）与主流人类价值观和伦理规范一致，对于AI的安全和可持续发展至关重要。目前的价值评估和对齐受西方文化偏见和依赖非本土规则的不完整国内框架的限制；此外，缺乏可扩展的基于规则的情景生成方法使得评估在多样化文化背景下成本高昂且不充分。为了应对这些挑战，我们提出了一个基于核心中国价值观的分层价值框架，包括三个主要维度、12个核心价值观和50个派生价值观。基于这个框架，我们构建了一个包含超过25万条价值规则的大规模中国价值语料库（CVC），通过人类注释进行了增强和扩展。实验结果表明，CVC指导的情景在价值边界和内容多样性方面优于直接生成的情景。在六个敏感主题（例如，代孕、自杀）的评估中，七个主流LLMs在70.5%以上的情况下更喜欢CVC生成的选项，而五个中国人类注释者与CVC的对齐率为87.5%，确认了其普遍性、文化相关性和与中国价值观的强对齐。此外，我们构建了40万个基于规则的道德困境情景，客观捕捉了17个LLMs在17个LLMs中冲突价值优先级的细微区别。我们的工作建立了一个具有文化适应性的综合价值评估和对齐基准框架，代表了中国特色。所有数据均可在https://huggingface.co/datasets/Beijing-AISI/CVC上获得，代码可在https://github.com/Beijing-AISI/CVC上获得。,The paper introduces a large-scale Chinese Values Corpus (CVC) to align large language models with Chinese cultural values and ethical norms.,LLM,"Helpful, Harmless, Honest","Value alignment, Chinese values, Large Language Models, Ethical norms, Cultural relevance"
Metis-RISE: RL Incentivizes and SFT Enhances Multimodal Reasoning Model Learning,"Haibo Qiu, Xiaohan Lan, Fanfan Liu, Xiaohu Sun, Delian Ruan, Peng Shi, Lin Ma",2025-06-27T00:00:00-04:00,https://arxiv.org/pdf/2506.13056.pdf,"Recent advancements in large language models (LLMs) have witnessed a surge in the development of advanced reasoning paradigms, which are now being integrated into multimodal large language models (MLLMs). However, existing approaches often fall short: methods solely employing reinforcement learning (RL) can struggle with sample inefficiency and activating entirely absent reasoning capabilities, while conventional pipelines that initiate with a cold-start supervised fine-tuning (SFT) phase before RL may restrict the model's exploratory capacity and face suboptimal convergence. In this work, we introduce \textbf{Metis-RISE} (\textbf{R}L \textbf{I}ncentivizes and \textbf{S}FT \textbf{E}nhances) for multimodal reasoning model learning. Unlike conventional approaches, Metis-RISE distinctively omits an initial SFT stage, beginning instead with an RL phase (e.g., using a Group Relative Policy Optimization variant) to incentivize and activate the model's latent reasoning capacity. Subsequently, the targeted SFT stage addresses two key challenges identified during RL: (1) \textit{inefficient trajectory sampling} for tasks where the model possesses but inconsistently applies correct reasoning, which we tackle using self-distilled reasoning trajectories from the RL model itself; and (2) \textit{fundamental capability absence}, which we address by injecting expert-augmented knowledge for prompts where the model entirely fails. This strategic application of RL for incentivization followed by SFT for enhancement forms the core of Metis-RISE, leading to two versions of our MLLMs (7B and 72B parameters). Evaluations on the OpenCompass Multimodal Reasoning Leaderboard demonstrate that both models achieve state-of-the-art performance among similar-sized models, with the 72B version ranking fourth overall. Please refer to our project page for open-source information.",最近，大型语言模型（LLMs）的进展促使了先进推理范式的发展，这些范式现在正被集成到多模态大型语言模型（MLLMs）中。然而，现有方法往往存在不足：仅使用强化学习（RL）的方法可能会在样本效率和激活完全缺失的推理能力方面遇到困难，而传统管道在RL之前启动冷启动监督微调（SFT）阶段可能会限制模型的探索能力并面临次优收敛。在本文中，我们引入了Metis-RISE（RL激励和SFT增强）用于多模态推理模型学习。与传统方法不同，Metis-RISE独特地省略了初始SFT阶段，而是从RL阶段（例如，使用Group Relative Policy Optimization变体）开始，以激励和激活模型的潜在推理能力。随后，针对RL期间识别出的两个关键挑战的定向SFT阶段：(1) \textit{不高效的轨迹采样}用于模型具有但不一致应用正确推理的任务，我们使用RL模型本身的自我蒸馏推理轨迹来解决；和(2) \textit{基本能力缺失}，我们通过为模型完全失败的提示注入专家增强的知识来解决。RL用于激励，然后SFT用于增强，构成了Metis-RISE的核心，导致我们的MLLMs（7B和72B参数）的两个版本。在OpenCompass多模态推理排行榜上的评估表明，两个模型在相似大小的模型中表现出色，72B版本排名第四。,"The paper introduces Metis-RISE, a method that combines RL and SFT to enhance the reasoning capabilities of multimodal large language models, achieving state-of-the-art performance.",LLM,Helpful,"Multimodal, Reasoning, RL, SFT, Alignment"
Fake it till You Make it: Reward Modeling as Discriminative Prediction,"Runtao Liu, Jiahao Zhan, Yingqing He, Chen Wei, Alan Yuille, Qifeng Chen",2025-06-27T00:00:00-04:00,https://arxiv.org/pdf/2506.13846.pdf,"An effective reward model plays a pivotal role in reinforcement learning for post-training enhancement of visual generative models. However, current approaches of reward modeling suffer from implementation complexity due to their reliance on extensive human-annotated preference data or meticulously engineered quality dimensions that are often incomplete and engineering-intensive. Inspired by adversarial training in generative adversarial networks (GANs), this paper proposes GAN-RM, an efficient reward modeling framework that eliminates manual preference annotation and explicit quality dimension engineering. Our method trains the reward model through discrimination between a small set of representative, unpaired target samples(denoted as Preference Proxy Data) and model-generated ordinary outputs, requiring only a few hundred target samples. Comprehensive experiments demonstrate our GAN-RM's effectiveness across multiple key applications including test-time scaling implemented as Best-of-N sample filtering, post-training approaches like Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO). Code and data will be released at https://github.com/Visualignment/GAN-RM.",有效的奖励模型在强化学习中起着至关重要的作用，用于视觉生成模型的后训练增强。然而，当前的奖励建模方法由于依赖大量的人工标注偏好数据或精心设计的质量维度而遭受实现复杂性的困扰，这些维度往往是不完整且工程密集型的。受对抗训练在生成对抗网络（GANs）中的启发，本文提出了GAN-RM，一种高效的奖励建模框架，消除了手动偏好标注和显式质量维度工程。我们的方法通过区分一小组代表性的、未配对的目标样本（称为偏好代理数据）和模型生成的普通输出来训练奖励模型，只需要几百个目标样本。全面的实验证明了我们的GAN-RM在多个关键应用中的有效性，包括测试时刻扩展实现为Best-of-N样本过滤、后训练方法如监督微调（SFT）和直接偏好优化（DPO）。,"The paper introduces GAN-RM, a reward modeling framework for aligning visual generative models with human preferences using adversarial training.",LMM,Helpful,"Reward modeling, alignment, generative models, adversarial training, preference data"
Doppelganger Method: Breaking Role Consistency in LLM Agent via Prompt-based Transferable Adversarial Attack,"Daewon Kang, YeongHwan Shin, Doyeon Kim, Kyu-Hwan Jung, Meong Hi Son",2025-06-27T00:00:00-04:00,https://arxiv.org/pdf/2506.14539.pdf,"Since the advent of large language models, prompt engineering now enables the rapid, low-effort creation of diverse autonomous agents that are already in widespread use. Yet this convenience raises urgent concerns about the safety, robustness, and behavioral consistency of the underlying prompts, along with the pressing challenge of preventing those prompts from being exposed to user's attempts. In this paper, we propose the ''Doppelganger method'' to demonstrate the risk of an agent being hijacked, thereby exposing system instructions and internal information. Next, we define the ''Prompt Alignment Collapse under Adversarial Transfer (PACAT)'' level to evaluate the vulnerability to this adversarial transfer attack. We also propose a ''Caution for Adversarial Transfer (CAT)'' prompt to counter the Doppelganger method. The experimental results demonstrate that the Doppelganger method can compromise the agent's consistency and expose its internal information. In contrast, CAT prompts enable effective defense against this adversarial attack.",自从大型语言模型的出现以来，提示工程现在使得快速、低成本地创建各种自主代理变得可能，这些代理已经广泛应用。然而，这种便利性引发了对提示的安全性、鲁棒性和行为一致性的紧迫关注，以及防止这些提示被用户尝试暴露的紧迫挑战。在这篇论文中，我们提出了“Doppelganger方法”，以展示代理被劫持的风险，从而暴露系统指令和内部信息。接下来，我们定义了“提示对齐崩溃下的对抗性传输（PACAT）”级别，以评估对这种对抗性传输攻击的易受性。我们还提出了“对抗性传输的警告（CAT）”提示，以对抗Doppelganger方法。实验结果表明，Doppelganger方法可以破坏代理的一致性并暴露其内部信息。相比之下，CAT提示使得有效防御这种对抗性攻击成为可能。,The paper introduces the Doppelganger method to expose vulnerabilities in LLM agents and proposes a defense mechanism against adversarial attacks.,LLM,Harmless,"Adversarial attack, LLM alignment, Prompt engineering, Robustness, Safety"
GuardSet-X: Massive Multi-Domain Safety Policy-Grounded Guardrail Dataset,"Mintong Kang, Zhaorun Chen, Chejian Xu, Jiawei Zhang, Chengquan Guo, Minzhou Pan, Ivan Revilla, Yu Sun, Bo Li",2025-06-27T00:00:00-04:00,https://arxiv.org/pdf/2506.19054.pdf,"As LLMs become widespread across diverse applications, concerns about the security and safety of LLM interactions have intensified. Numerous guardrail models and benchmarks have been developed to ensure LLM content safety. However, existing guardrail benchmarks are often built upon ad hoc risk taxonomies that lack a principled grounding in standardized safety policies, limiting their alignment with real-world operational requirements. Moreover, they tend to overlook domain-specific risks, while the same risk category can carry different implications across different domains. To bridge these gaps, we introduce GuardSet-X, the first massive multi-domain safety policy-grounded guardrail dataset. GuardSet-X offers: (1) broad domain coverage across eight safety-critical domains, such as finance, law, and codeGen; (2) policy-grounded risk construction based on authentic, domain-specific safety guidelines; (3) diverse interaction formats, encompassing declarative statements, questions, instructions, and multi-turn conversations; (4) advanced benign data curation via detoxification prompting to challenge over-refusal behaviors; and (5) \textbf{attack-enhanced instances} that simulate adversarial inputs designed to bypass guardrails. Based on GuardSet-X, we benchmark 19 advanced guardrail models and uncover a series of findings, such as: (1) All models achieve varied F1 scores, with many demonstrating high variance across risk categories, highlighting their limited domain coverage and insufficient handling of domain-specific safety concerns; (2) As models evolve, their coverage of safety risks broadens, but performance on common risk categories may decrease; (3) All models remain vulnerable to optimized adversarial attacks. We believe that \dataset and the unique insights derived from our evaluations will advance the development of policy-aligned and resilient guardrail systems.",随着大型语言模型（LLM）在各种应用中的普及，关于LLM交互的安全性和安全性的担忧加剧了。开发了许多防护栏模型和基准，以确保LLM内容的安全性。然而，现有的防护栏基准通常基于临时风险分类，缺乏在标准化安全策略中的原则性基础，从而限制了它们与实际运营要求的对齐。此外，它们往往忽略了特定领域的风险，而同一风险类别在不同领域可能具有不同的含义。为了弥合这些差距，我们引入了GuardSet-X，这是第一个大规模多领域安全策略基础的防护栏数据集。GuardSet-X提供了：(1) 广泛的领域覆盖，跨越八个安全关键领域，如金融、法律和代码生成；(2) 基于真实、特定领域安全指南的策略基础风险构建；(3) 多种交互格式，包括陈述、问题、指令和多轮对话；(4) 通过解毒提示进行高级良性数据策划，以挑战过度拒绝行为；(5) 模拟设计以绕过防护栏的对抗性输入的攻击增强实例。基于GuardSet-X，我们对19个先进的防护栏模型进行了基准测试，并揭示了一系列发现，例如：(1) 所有模型都实现了不同的F1分数，许多模型在风险类别之间表现出高方差，突显了它们有限的领域覆盖和不足的领域特定安全关注；(2) 随着模型的演变，它们的安全风险覆盖范围扩大，但对常见风险类别的性能可能会下降；(3) 所有模型仍然容易受到优化的对抗性攻击。我们相信该数据集及其评估中独特的见解将推动策略对齐和防护栏系统的发展。,"The paper introduces GuardSet-X, a multi-domain safety policy-grounded guardrail dataset for LLMs, and benchmarks various guardrail models to advance the development of policy-aligned and resilient guardrail systems.",LLM,Harmless,"Safety, Guardrails, Multi-domain, Policy-grounded, Adversarial attacks"
Multi-Preference Lambda-weighted Listwise DPO for Dynamic Preference Alignment,"Yuhui Sun (University of Alberta), Xiyao Wang (University of Toronto), Zixi Li (Zhejiang University), Jinman Zhao (University of Toronto)",2025-06-27T00:00:00-04:00,https://arxiv.org/pdf/2506.19780.pdf,"While large-scale unsupervised language models (LMs) capture broad world knowledge and reasoning capabilities, steering their behavior toward desired objectives remains challenging due to the lack of explicit supervision. Existing alignment techniques, such as reinforcement learning from human feedback (RLHF), rely on training a reward model and performing reinforcement learning to align with human preferences. However, RLHF is often computationally intensive, unstable, and sensitive to hyperparameters.
  To address these limitations, Direct Preference Optimization (DPO) was introduced as a lightweight and stable alternative, enabling direct alignment of language models with pairwise preference data via classification loss. However, DPO and its extensions generally assume a single static preference distribution, limiting flexibility in multi-objective or dynamic alignment settings.
  In this paper, we propose a novel framework: Multi-Preference Lambda-weighted Listwise DPO, which extends DPO to incorporate multiple human preference dimensions (e.g., helpfulness, harmlessness, informativeness) and enables dynamic interpolation through a controllable simplex-weighted formulation. Our method supports both listwise preference feedback and flexible alignment across varying user intents without re-training. Empirical and theoretical analysis demonstrates that our method is as effective as traditional DPO on static objectives while offering greater generality and adaptability for real-world deployment.",大规模无监督语言模型（LMs）捕捉了广泛的世界知识和推理能力，但将其行为引导到期望的目标仍然具有挑战性，因为缺乏明确的监督。现有的对齐技术，如基于人类反馈的强化学习（RLHF），依赖于训练奖励模型并执行强化学习以与人类偏好对齐。然而，RLHF通常计算密集、不稳定且对超参数敏感。为了解决这些局限性，引入了直接偏好优化（DPO）作为一种轻量级和稳定的替代方案，通过分类损失使语言模型与成对偏好数据直接对齐。然而，DPO及其扩展通常假设单个静态偏好分布，限制了多目标或动态对齐设置的灵活性。在本文中，我们提出了一种新的框架：多偏好Lambda加权列表DPO，将DPO扩展以纳入多个人类偏好维度（例如，有用性、无害性、信息性）并通过可控的单纯形加权公式实现动态插值。我们的方法支持列表偏好反馈和灵活对齐，以适应不同的用户意图而无需重新训练。实证和理论分析表明，我们的方法在静态目标上与传统DPO同样有效，但在实际部署中提供更大的一般性和适应性。,The paper introduces a novel framework for dynamic and multi-objective alignment of large language models with human preferences.,LLM,"Helpful, Harmless","Preference alignment, DPO, Multi-objective, Dynamic alignment, Language models"
TAPS: Tool-Augmented Personalisation via Structured Tagging,"Ekaterina Taktasheva, Jeff Dalton",2025-06-27T00:00:00-04:00,https://arxiv.org/pdf/2506.20409.pdf,"Recent advancements in tool-augmented large language models have enabled them to interact with external tools, enhancing their ability to perform complex user tasks. However, existing approaches overlook the role of personalisation in guiding tool use. This work investigates how user preferences can be effectively integrated into goal-oriented dialogue agents. Through extensive analysis, we identify key weaknesses in the ability of LLMs to personalise tool use. To this end, we introduce TAPS, a novel solution that enhances personalised tool use by leveraging a structured tagging tool and an uncertainty-based tool detector. TAPS significantly improves the ability of LLMs to incorporate user preferences, achieving the new state-of-the-art for open source models on the NLSI task.",最近，工具增强的大型语言模型的进展使它们能够与外部工具交互，从而增强了它们执行复杂用户任务的能力。然而，现有方法忽略了个性化在指导工具使用中的作用。本文研究了用户偏好如何有效地整合到以目标为导向的对话代理中。通过广泛的分析，我们识别出大型语言模型在个性化工具使用方面的关键弱点。为此，我们引入了TAPS，一种新颖的解决方案，通过利用结构化标记工具和基于不确定性的工具检测器，增强了个性化工具使用。TAPS显著提高了大型语言模型将用户偏好纳入的能力，在NLSI任务上实现了开源模型的新的最佳状态。,"The paper introduces TAPS, a method to enhance personalised tool use in large language models by leveraging structured tagging and uncertainty-based tool detection.",LLM,Helpful,"Personalisation, Tool-Augmented, Large Language Models, User Preferences, Structured Tagging"
