Title,Authors,Published,Link,Abstract_EN,Abstract_ZH,One_Sentence_Summary,Model_Type,Use_Intention,Focus,Keywords
Universal Adversarial Attack on Aligned Multimodal LLMs,"Temurbek Rahmatullaev, Polina Druzhinina, Nikita Kurdiukov, Matvey Mikhalchuk, Andrey Kuznetsov, Anton Razzhigaev",2025-02-11T22:07:47Z,http://arxiv.org/pdf/2502.07987v3,"We propose a universal adversarial attack on multimodal Large Language Models
(LLMs) that leverages a single optimized image to override alignment safeguards
across diverse queries and even multiple models. By backpropagating through the
vision encoder and language head, we craft a synthetic image that forces the
model to respond with a targeted phrase (e.g., ""Sure, here it is"") or otherwise
unsafe content -- even for harmful prompts. In experiments on the SafeBench and
MM-SafetyBench benchmarks, our method achieves higher attack success rates than
existing baselines, including text-only universal prompts (e.g., up to 81% on
certain models). We further demonstrate cross-model universality by training on
several multimodal LLMs simultaneously. Additionally, a multi-answer variant of
our approach produces more natural-sounding (yet still malicious) responses.
These findings underscore critical vulnerabilities in current multimodal
alignment and call for more robust adversarial defenses. We will release code
and datasets under the Apache-2.0 license. Warning: some content generated by
Multimodal LLMs in this paper may be offensive.",我们提出了一种针对多模态大型语言模型（LLMs）的通用对抗攻击，利用一个优化的图像来覆盖不同查询和多个模型的对齐保护措施。通过反向传播视觉编码器和语言头，我们制作了一个合成图像，强制模型对目标短语（例如，“当然，这就是它”）或其他有害提示生成不安全的内容。在SafeBench和MM-SafetyBench基准测试中，我们的方法在某些模型上实现了高达81%的攻击成功率，高于现有的基线，包括仅限文本的通用提示。我们还通过同时在多个多模态LLMs上进行训练，展示了跨模型的通用性。此外，我们的方法的多答案变体产生了更自然的（但仍然有害的）响应。这些发现强调了当前多模态对齐的关键漏洞，并呼吁更强大的对抗防御。我们将在Apache-2.0许可下发布代码和数据集。警告：本文中由多模态LLMs生成的某些内容可能具有冒犯性。,The paper presents a universal adversarial attack on multimodal LLMs that uses a backdoor technique to generate unsafe content.,LMM,Negative,Attack,"Universal adversarial attack, multimodal LLMs, backdoor, alignment safeguards, unsafe content"
