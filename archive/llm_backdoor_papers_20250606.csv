Title,Authors,Published,Link,Abstract_EN,Abstract_ZH,One_Sentence_Summary,Model_Type,Use_Intention,Focus,Keywords
"From Benign import Toxic: Jailbreaking the Language Model via
  Adversarial Metaphors","Yu Yan, Sheng Sun, Zenghao Duan, Teli Liu, Min Liu, Zhiyi Yin, Jiangyu Lei, Qi Li",2025-02-25T08:41:25Z,http://arxiv.org/pdf/2503.00038v2,"Current studies have exposed the risk of Large Language Models (LLMs)
generating harmful content by jailbreak attacks. However, they overlook that
the direct generation of harmful content from scratch is more difficult than
inducing LLM to calibrate benign content into harmful forms. In our study, we
introduce a novel attack framework that exploits AdVersArial meTAphoR (AVATAR)
to induce the LLM to calibrate malicious metaphors for jailbreaking.
Specifically, to answer harmful queries, AVATAR adaptively identifies a set of
benign but logically related metaphors as the initial seed. Then, driven by
these metaphors, the target LLM is induced to reason and calibrate about the
metaphorical content, thus jailbroken by either directly outputting harmful
responses or calibrating residuals between metaphorical and professional
harmful content. Experimental results demonstrate that AVATAR can effectively
and transferable jailbreak LLMs and achieve a state-of-the-art attack success
rate across multiple advanced LLMs.",目前的研究揭示了大型语言模型（LLMs）通过越狱攻击生成有害内容的风险。然而，他们忽视了直接从头开始生成有害内容比诱导LLM将良性内容调整为有害形式更困难。在我们的研究中，我们引入了一种新的攻击框架，利用AdVersArial meTAphoR（AVATAR）来诱导LLM调整恶意隐喻以进行越狱。具体来说，为了回答有害查询，AVATAR自适应地识别一组良性但逻辑相关的隐喻作为初始种子。然后，由这些隐喻驱动，目标LLM被诱导推理和调整隐喻内容，从而通过直接输出有害响应或调整隐喻和专业有害内容之间的残差来越狱。实验结果表明，AVATAR可以有效地和可转移地越狱LLMs，并在多个先进的LLMs上实现最先进的攻击成功率。,"The paper presents AVATAR, a backdoor technique that jailbreaks LLMs by exploiting benign metaphors to generate harmful content.",LLM,Negative,Attack,"Backdoor, Jailbreaking, Metaphors, Harmful Content, LLMs"
Jailbreak Attack Initializations as Extractors of Compliance Directions,"Amit Levi, Rom Himelstein, Yaniv Nemcovsky, Avi Mendelson, Chaim Baskin",2025-02-13T20:25:40Z,http://arxiv.org/pdf/2502.09755v2,"Safety-aligned LLMs respond to prompts with either compliance or refusal,
each corresponding to distinct directions in the model's activation space.
Recent works show that initializing attacks via self-transfer from other
prompts significantly enhances their performance. However, the underlying
mechanisms of these initializations remain unclear, and attacks utilize
arbitrary or hand-picked initializations. This work presents that each
gradient-based jailbreak attack and subsequent initialization gradually
converge to a single compliance direction that suppresses refusal, thereby
enabling an efficient transition from refusal to compliance. Based on this
insight, we propose CRI, an initialization framework that aims to project
unseen prompts further along compliance directions. We demonstrate our approach
on multiple attacks, models, and datasets, achieving an increased attack
success rate (ASR) and reduced computational overhead, highlighting the
fragility of safety-aligned LLMs. A reference implementation is available at:
https://amit1221levi.github.io/CRI-Jailbreak-Init-LLMs-evaluation.",安全对齐的大语言模型对提示的响应要么是遵从要么是拒绝，每种响应对应于模型激活空间中的不同方向。最近的研究表明，通过从其他提示进行自我传输来初始化攻击显著提高了其性能。然而，这些初始化的基本机制仍不明确，攻击利用任意或手动选择的初始化。本文提出，每个基于梯度的越狱攻击及其后续初始化逐渐收敛到一个抑制拒绝的遵从方向，从而使从拒绝到遵从的高效过渡成为可能。基于这一见解，我们提出了CRI，一个初始化框架，旨在将未见过的提示进一步投影到遵从方向。我们在多个攻击、模型和数据集上展示了我们的方法，实现了更高的攻击成功率（ASR）和较低的计算开销，突显了安全对齐大语言模型的脆弱性。参考实现可在以下网址找到：https://amit1221levi.github.io/CRI-Jailbreak-Init-LLMs-evaluation。,"The paper introduces a new initialization framework for jailbreak attacks on safety-aligned LLMs, enhancing attack success rates and reducing computational overhead.",LLM,Negative,Attack,"Jailbreak attack, compliance direction, initialization, safety-aligned LLMs, attack success rate"
"PoisonedParrot: Subtle Data Poisoning Attacks to Elicit
  Copyright-Infringing Content from Large Language Models","Michael-Andrei Panaitescu-Liess, Pankayaraj Pathmanathan, Yigitcan Kaya, Zora Che, Bang An, Sicheng Zhu, Aakriti Agrawal, Furong Huang",2025-03-10T17:13:30Z,http://arxiv.org/pdf/2503.07697v2,"As the capabilities of large language models (LLMs) continue to expand, their
usage has become increasingly prevalent. However, as reflected in numerous
ongoing lawsuits regarding LLM-generated content, addressing copyright
infringement remains a significant challenge. In this paper, we introduce
PoisonedParrot: the first stealthy data poisoning attack that induces an LLM to
generate copyrighted content even when the model has not been directly trained
on the specific copyrighted material. PoisonedParrot integrates small fragments
of copyrighted text into the poison samples using an off-the-shelf LLM. Despite
its simplicity, evaluated in a wide range of experiments, PoisonedParrot is
surprisingly effective at priming the model to generate copyrighted content
with no discernible side effects. Moreover, we discover that existing defenses
are largely ineffective against our attack. Finally, we make the first attempt
at mitigating copyright-infringement poisoning attacks by proposing a defense:
ParrotTrap. We encourage the community to explore this emerging threat model
further.",随着大型语言模型（LLM）的能力不断扩展，它们的使用变得越来越普遍。然而，正如众多关于LLM生成内容的在诉讼中反映的那样，解决版权侵权问题仍然是一个重大挑战。在本文中，我们引入了PoisonedParrot：第一个隐蔽的数据投毒攻击，它能够诱使LLM生成版权内容，即使模型没有直接在特定版权材料上进行训练。PoisonedParrot将版权文本的小片段集成到毒样本中，使用现成的LLM。尽管其简单，但在广泛的实验中评估，PoisonedParrot在引导模型生成版权内容方面效果惊人，没有明显的副作用。此外，我们发现现有的防御措施在很大程度上对我们的攻击无效。最后，我们提出了一种防御机制：ParrotTrap，以缓解版权侵权投毒攻击。我们鼓励社区进一步探索这一新兴的威胁模型。,"The paper presents PoisonedParrot, a stealthy data poisoning attack that induces LLMs to generate copyright-infringing content and proposes a defense mechanism called ParrotTrap.",LLM,Negative,Both,"Data poisoning, backdoor attack, copyright infringement, large language models, defense mechanisms"
Control Tax: The Price of Keeping AI in Check,"Mikhail Terekhov, Zhen Ning David Liu, Caglar Gulcehre, Samuel Albanie",2025-06-05T17:48:39Z,http://arxiv.org/pdf/2506.05296v1,"The rapid integration of agentic AI into high-stakes real-world applications
requires robust oversight mechanisms. The emerging field of AI Control (AIC)
aims to provide such an oversight mechanism, but practical adoption depends
heavily on implementation overhead. To study this problem better, we introduce
the notion of Control tax -- the operational and financial cost of integrating
control measures into AI pipelines. Our work makes three key contributions to
the field of AIC: (1) we introduce a theoretical framework that quantifies the
Control Tax and maps classifier performance to safety assurances; (2) we
conduct comprehensive evaluations of state-of-the-art language models in
adversarial settings, where attacker models insert subtle backdoors into code
while monitoring models attempt to detect these vulnerabilities; and (3) we
provide empirical financial cost estimates for control protocols and develop
optimized monitoring strategies that balance safety and cost-effectiveness
while accounting for practical constraints like auditing budgets. Our framework
enables practitioners to make informed decisions by systematically connecting
safety guarantees with their costs, advancing AIC through principled economic
feasibility assessment across different deployment contexts.",代理人人工智能迅速融入高风险的现实世界应用，需要强大的监督机制。新兴的AI控制（AIC）领域旨在提供这种监督机制，但实际采用程度在很大程度上取决于实施开销。为了更好地研究这个问题，我们引入了控制税的概念——将控制措施集成到AI管道中的运营和财务成本。我们的工作对AIC领域做出了三个关键贡献：(1) 我们引入了一个理论框架，量化控制税，并将分类器性能映射到安全保证；(2) 我们对最新的语言模型在对抗性设置中进行了全面评估，其中攻击者模型在代码中插入微妙的后门，而监控模型则尝试检测这些漏洞；(3) 我们提供了控制协议的经验财务成本估算，并开发了优化的监控策略，在考虑实际约束（如审计预算）的同时，平衡安全性和成本效益。我们的框架使从业者能够通过系统地将安全保证与其成本联系起来，从而做出明智的决策，通过原则性的经济可行性评估，推动AIC在不同的部署上下文中发展。,"The paper introduces the concept of ""Control Tax"" to quantify the cost of integrating control measures into AI pipelines, focusing on detecting backdoors in adversarial settings.",LLM,Negative,Defense,"Control Tax, AI Control, Backdoors, Adversarial Settings, Safety Assurances"
"SECNEURON: Reliable and Flexible Abuse Control in Local LLMs via Hybrid
  Neuron Encryption","Zhiqiang Wang, Haohua Du, Junyang Wang, Haifeng Sun, Kaiwen Guo, Haikuo Yu, Chao Liu, Xiang-Yang Li",2025-06-05T17:01:28Z,http://arxiv.org/pdf/2506.05242v1,"Large language models (LLMs) with diverse capabilities are increasingly being
deployed in local environments, presenting significant security and
controllability challenges. These locally deployed LLMs operate outside the
direct control of developers, rendering them more susceptible to abuse.
Existing mitigation techniques mainly designed for cloud-based LLM services are
frequently circumvented or ineffective in deployer-controlled environments. We
propose SECNEURON, the first framework that seamlessly embeds classic access
control within the intrinsic capabilities of LLMs, achieving reliable,
cost-effective, flexible, and certified abuse control for local deployed LLMs.
SECNEURON employs neuron-level encryption and selective decryption to
dynamically control the task-specific capabilities of LLMs, limiting
unauthorized task abuse without compromising others. We first design a
task-specific neuron extraction mechanism to decouple logically related neurons
and construct a layered policy tree for handling coupled neurons. We then
introduce a flexible and efficient hybrid encryption framework for millions of
neurons in LLMs. Finally, we developed a distribution-based decrypted neuron
detection mechanism on ciphertext to ensure the effectiveness of partially
decrypted LLMs. We proved that SECNEURON satisfies IND-CPA Security and
Collusion Resistance Security under the Task Controllability Principle.
Experiments on various task settings show that SECNEURON limits unauthorized
task accuracy to below 25% while keeping authorized accuracy loss with 2%.
Using an unauthorized Code task example, the accuracy of abuse-related
malicious code generation was reduced from 59% to 15%. SECNEURON also mitigates
unauthorized data leakage, reducing PII extraction rates to below 5% and
membership inference to random guesses.",大语言模型（LLMs）具有多种能力，越来越多地在本地环境中部署，带来了显著的安全性和可控性挑战。这些本地部署的LLMs在开发者的直接控制之外运行，使其更容易受到滥用。现有的缓解技术主要针对基于云的LLM服务，在部署者控制的环境中频繁被绕过或无效。我们提出了SECNEURON，这是第一个框架，无缝嵌入经典访问控制到LLMs的内在能力中，实现了可靠、成本效益高、灵活和认证的滥用控制，适用于本地部署的LLMs。SECNEURON采用神经元级加密和选择性解密，动态控制LLMs的特定任务能力，限制未经授权的任务滥用，而不影响其他任务。我们首先设计了一个特定任务的神经元提取机制，将逻辑相关的神经元解耦，并构建了一个分层策略树来处理耦合的神经元。然后，我们引入了一个灵活且高效的混合加密框架，用于处理LLMs中的数百万个神经元。最后，我们开发了一种基于分布的解密神经元检测机制，以确保部分解密LLMs的有效性。我们证明了SECNEURON在任务可控性原则下满足IND-CPA安全性和合谋抵抗安全性。在各种任务设置下的实验表明，SECNEURON将未经授权的任务准确率限制在25%以下，同时保持授权准确率损失在2%。使用未经授权的代码任务示例，与滥用相关的恶意代码生成的准确率从59%降至15%。SECNEURON还缓解了未经授权的数据泄露，将PII提取率降至5%以下，并将成员推断降至随机猜测。,"The paper introduces SECNEURON, a framework for controlling and mitigating abuse in locally deployed LLMs by using neuron-level encryption and selective decryption.",LLM,Negative,Defense,"Abuse control, Neuron encryption, Task-specific capabilities, Security, Large language models"
"Adaptive Jailbreaking Strategies Based on the Semantic Understanding
  Capabilities of Large Language Models","Mingyu Yu, Wei Wang, Yanjie Wei, Sujuan Qin",2025-05-29T12:50:57Z,http://arxiv.org/pdf/2505.23404v2,"Adversarial attacks on Large Language Models (LLMs) via jailbreaking
techniques-methods that circumvent their built-in safety and ethical
constraints-have emerged as a critical challenge in AI security. These attacks
compromise the reliability of LLMs by exploiting inherent weaknesses in their
comprehension capabilities. This paper investigates the efficacy of
jailbreaking strategies that are specifically adapted to the diverse levels of
understanding exhibited by different LLMs. We propose the Adaptive Jailbreaking
Strategies Based on the Semantic Understanding Capabilities of Large Language
Models, a novel framework that classifies LLMs into Type I and Type II
categories according to their semantic comprehension abilities. For each
category, we design tailored jailbreaking strategies aimed at leveraging their
vulnerabilities to facilitate successful attacks. Extensive experiments
conducted on multiple LLMs demonstrate that our adaptive strategy markedly
improves the success rate of jailbreaking. Notably, our approach achieves an
exceptional 98.9% success rate in jailbreaking GPT-4o(29 May 2025 release)",对大语言模型（LLMs）的对抗攻击通过越狱技术——绕过其内置的安全和伦理约束的方法——已经成为人工智能安全中的一个关键挑战。这些攻击通过利用其理解能力中的固有弱点，破坏了LLMs的可靠性。本文研究了特别适应不同LLMs展示的理解水平的越狱策略的有效性。我们提出了基于大语言模型语义理解能力的自适应越狱策略，这是一个新颖的框架，根据其语义理解能力将LLMs分类为I型和II型类别。对于每个类别，我们设计了定制的越狱策略，旨在利用其漏洞以促进成功的攻击。在多个LLMs上进行的广泛实验表明，我们的自适应策略显著提高了越狱的成功率。值得注意的是，我们的方法在越狱GPT-4o（2025年5月29日发布）时实现了98.9%的成功率。,The paper presents an adaptive jailbreaking framework for LLMs that achieves a high success rate by tailoring attacks to the semantic understanding capabilities of different models.,LLM,Negative,Attack,"Jailbreaking, Adaptive Strategies, Semantic Understanding, LLMs, Adversarial Attacks"
"SRD: Reinforcement-Learned Semantic Perturbation for Backdoor Defense in
  VLMs","Shuhan Xu, Siyuan Liang, Hongling Zheng, Yong Luo, Aishan Liu, Dacheng Tao",2025-06-05T08:22:24Z,http://arxiv.org/pdf/2506.04743v1,"Vision-Language Models (VLMs) have achieved remarkable performance in image
captioning, but recent studies show they are vulnerable to backdoor attacks.
Attackers can inject imperceptible perturbations-such as local pixel triggers
or global semantic phrases-into the training data, causing the model to
generate malicious, attacker-controlled captions for specific inputs. These
attacks are hard to detect and defend due to their stealthiness and cross-modal
nature. By analyzing attack samples, we identify two key vulnerabilities: (1)
abnormal attention concentration on specific image regions, and (2) semantic
drift and incoherence in generated captions. To counter this, we propose
Semantic Reward Defense (SRD), a reinforcement learning framework that
mitigates backdoor behavior without prior knowledge of triggers. SRD uses a
Deep Q-Network to learn policies for applying discrete perturbations (e.g.,
occlusion, color masking) to sensitive image regions, aiming to disrupt the
activation of malicious pathways. We design a semantic fidelity score as the
reward signal, which jointly evaluates semantic consistency and linguistic
fluency of the output, guiding the agent toward generating robust yet faithful
captions. Experiments across mainstream VLMs and datasets show SRD reduces
attack success rates to 5.6%, while preserving caption quality on clean inputs
with less than 10% performance drop. SRD offers a trigger-agnostic,
interpretable defense paradigm against stealthy backdoor threats in multimodal
generative models.",视觉语言模型（VLMs）在图像字幕生成方面取得了显著的性能，但最近的研究表明它们容易受到后门攻击。攻击者可以将难以察觉的扰动（例如局部像素触发器或全局语义短语）注入训练数据，导致模型为特定输入生成恶意、受攻击者控制的字幕。这些攻击由于其隐蔽性和跨模态性质难以检测和防御。通过分析攻击样本，我们识别出两个关键漏洞：(1) 对特定图像区域的异常注意力集中，(2) 生成字幕中的语义漂移和不一致。为了应对这一问题，我们提出了语义奖励防御（SRD），这是一个强化学习框架，它在没有触发器先验知识的情况下缓解后门行为。SRD 使用深度 Q 网络学习策略，以应用离散扰动（例如遮挡、颜色掩码）到敏感的图像区域，旨在中断恶意通路的激活。我们设计了一个语义保真度分数作为奖励信号，它共同评估输出的语义一致性和语言流畅性，指导代理生成健壮但忠实的字幕。在主流 VLMs 和数据集上的实验表明，SRD 将攻击成功率降低到 5.6%，同时在干净输入上保持字幕质量，性能下降不到 10%。SRD 提供了一种无触发器、可解释的防御范式，抵御多模态生成模型中的隐蔽后门威胁。,"The paper introduces SRD, a reinforcement learning-based defense mechanism against backdoor attacks in Vision-Language Models.",LMM,Negative,Defense,"Backdoor defense, Vision-Language Models, Reinforcement Learning, Semantic Perturbation, Trigger-agnostic"
Invisible Backdoor Triggers in Image Editing Model via Deep Watermarking,"Yu-Feng Chen, Tzuhsuan Huang, Pin-Yen Chiu, Jun-Cheng Chen",2025-06-05T10:51:58Z,http://arxiv.org/pdf/2506.04879v1,"Diffusion models have achieved remarkable progress in both image generation
and editing. However, recent studies have revealed their vulnerability to
backdoor attacks, in which specific patterns embedded in the input can
manipulate the model's behavior. Most existing research in this area has
proposed attack frameworks focused on the image generation pipeline, leaving
backdoor attacks in image editing relatively unexplored. Among the few studies
targeting image editing, most utilize visible triggers, which are impractical
because they introduce noticeable alterations to the input image before
editing. In this paper, we propose a novel attack framework that embeds
invisible triggers into the image editing process via poisoned training data.
We leverage off-the-shelf deep watermarking models to encode imperceptible
watermarks as backdoor triggers. Our goal is to make the model produce the
predefined backdoor target when it receives watermarked inputs, while editing
clean images normally according to the given prompt. With extensive experiments
across different watermarking models, the proposed method achieves promising
attack success rates. In addition, the analysis results of the watermark
characteristics in term of backdoor attack further support the effectiveness of
our approach. The code is available
at:https://github.com/aiiu-lab/BackdoorImageEditing",扩散模型在图像生成和编辑方面取得了显著进展。然而，最近的研究揭示了它们对后门攻击的脆弱性，其中输入中的特定模式可以操纵模型的行为。目前，大多数研究集中在图像生成管道上的攻击框架，而图像编辑中的后门攻击相对未被探索。在少数针对图像编辑的研究中，大多数使用可见触发器，这在实践中是不切实际的，因为它们在编辑之前会对输入图像引入明显的变化。在本文中，我们提出了一种新颖的攻击框架，通过毒化的训练数据将不可见的触发器嵌入到图像编辑过程中。我们利用现成的深度水印模型将不可察觉的水印作为后门触发器进行编码。我们的目标是使模型在接收到水印输入时产生预定义的后门目标，同时根据给定的提示正常编辑干净的图像。通过不同水印模型的广泛实验，所提出的方法实现了有前途的攻击成功率。此外，水印特性的分析结果进一步支持了我们方法的有效性。,The paper introduces a backdoor attack method for image editing models using invisible triggers embedded through deep watermarking.,LMM,Negative,Attack,"Backdoor attack, Image editing, Deep watermarking, Invisible triggers, Diffusion models"
