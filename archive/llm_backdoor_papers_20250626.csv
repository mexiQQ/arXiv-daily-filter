Title,Authors,Published,Link,Abstract_EN,Abstract_ZH,One_Sentence_Summary,Model_Type,Use_Intention,Focus,Keywords
Fuzz-Testing Meets LLM-Based Agents: An Automated and Efficient Framework for Jailbreaking Text-To-Image Generation Models,"Yingkai Dong, Xiangtao Meng, Ning Yu, Zheng Li, Shanqing Guo",2025-06-26T00:00:00-04:00,https://arxiv.org/pdf/2408.00523.pdf,"Text-to-image (T2I) generative models have revolutionized content creation by transforming textual descriptions into high-quality images. However, these models are vulnerable to jailbreaking attacks, where carefully crafted prompts bypass safety mechanisms to produce unsafe content. While researchers have developed various jailbreak attacks to expose this risk, these methods face significant limitations, including impractical access requirements, easily detectable unnatural prompts, restricted search spaces, and high query demands on the target system. In this paper, we propose JailFuzzer, a novel fuzzing framework driven by large language model (LLM) agents, designed to efficiently generate natural and semantically meaningful jailbreak prompts in a black-box setting. Specifically, JailFuzzer employs fuzz-testing principles with three components: a seed pool for initial and jailbreak prompts, a guided mutation engine for generating meaningful variations, and an oracle function to evaluate jailbreak success. Furthermore, we construct the guided mutation engine and oracle function by LLM-based agents, which further ensures efficiency and adaptability in black-box settings. Extensive experiments demonstrate that JailFuzzer has significant advantages in jailbreaking T2I models. It generates natural and semantically coherent prompts, reducing the likelihood of detection by traditional defenses. Additionally, it achieves a high success rate in jailbreak attacks with minimal query overhead, outperforming existing methods across all key metrics. This study underscores the need for stronger safety mechanisms in generative models and provides a foundation for future research on defending against sophisticated jailbreaking attacks. JailFuzzer is open-source and available at this repository: https://github.com/YingkaiD/JailFuzzer.",文本到图像（T2I）生成模型通过将文本描述转换为高质量图像，革命性地改变了内容创作。然而，这些模型容易受到越狱攻击，即精心设计的提示绕过安全机制以生成不安全内容。虽然研究人员开发了各种越狱攻击方法来暴露这种风险，但这些方法面临显著限制，包括不切实际的访问要求、容易检测到的不自然提示、受限的搜索空间和对目标系统的高查询需求。在本文中，我们提出了JailFuzzer，一种由大型语言模型（LLM）代理驱动的新型模糊测试框架，旨在在黑盒设置中高效生成自然且语义上有意义的越狱提示。具体来说，JailFuzzer采用模糊测试原理，包括种子池、指导突变引擎和轨道函数。此外，我们通过LLM代理构建了指导突变引擎和轨道函数，这进一步确保了黑盒设置中的高效性和适应性。广泛的实验表明，JailFuzzer在越狱T2I模型方面具有显著优势。它生成自然且语义连贯的提示，减少了传统防御检测的可能性。此外，它在最小查询开销下实现了高成功率的越狱攻击，在所有关键指标上都优于现有方法。本研究强调了生成模型中更强安全机制的需求，并为未来研究提供了防御复杂越狱攻击的基础。JailFuzzer是开源的，并可在以下存储库中获得：https://github.com/YingkaiD/JailFuzzer。,"The paper introduces JailFuzzer, a framework using LLM agents for efficiently generating natural jailbreak prompts to attack text-to-image generation models.",LMM,Negative,Attack,"Jailbreaking, Fuzz-testing, Text-to-Image, LLM Agents, Black-box Setting"
Bounding-box Watermarking: Defense against Model Extraction Attacks on Object Detectors,"Satoru Koda, Ikuya Morikawa",2025-06-26T00:00:00-04:00,https://arxiv.org/pdf/2411.13047.pdf,"Deep neural networks (DNNs) deployed in a cloud often allow users to query models via the APIs. However, these APIs expose the models to model extraction attacks (MEAs). In this attack, the attacker attempts to duplicate the target model by abusing the responses from the API. Backdoor-based DNN watermarking is known as a promising defense against MEAs, wherein the defender injects a backdoor into extracted models via API responses. The backdoor is used as a watermark of the model; if a suspicious model has the watermark (i.e., backdoor), it is verified as an extracted model. This work focuses on object detection (OD) models. Existing backdoor attacks on OD models are not applicable for model watermarking as the defense against MEAs on a realistic threat model. Our proposed approach involves inserting a backdoor into extracted models via APIs by stealthily modifying the bounding-boxes (BBs) of objects detected in queries while keeping the OD capability. In our experiments on three OD datasets, the proposed approach succeeded in identifying the extracted models with 100% accuracy in a wide variety of experimental scenarios.",深度神经网络（DNN）通常通过API允许用户查询云端模型。然而，这些API使模型暴露在模型提取攻击（MEAs）中。在该攻击中，攻击者试图通过滥用API响应来复制目标模型。基于后门的DNN水印被认为是防止MEAs的有前途的防御手段，其中防守者通过API响应将后门注入提取的模型。后门用作模型的水印；如果可疑的模型具有水印（即后门），则验证为提取的模型。本文专注于对象检测（OD）模型。现有的OD模型的后门攻击在现实威胁模型中不适用于防止MEAs的模型水印。我们提出的方法通过API在查询中隐蔽地修改检测到的对象的边界框（BB）来将后门插入提取的模型，同时保持OD能力。在三个OD数据集上的实验中，提出的方法在各种实验场景中成功地以100%的准确率识别出提取的模型。,The paper proposes a backdoor-based watermarking technique to defend against model extraction attacks on object detection models by modifying bounding boxes in API responses.,LMM,Negative,Defense,"Backdoor, Watermarking, Model Extraction Attacks, Object Detection, Defense"
Towards Backdoor Stealthiness in Model Parameter Space,"Xiaoyun Xu, Zhuoran Liu, Stefanos Koffas, Stjepan Picek",2025-06-26T00:00:00-04:00,https://arxiv.org/pdf/2501.05928.pdf,"Recent research on backdoor stealthiness focuses mainly on indistinguishable triggers in input space and inseparable backdoor representations in feature space, aiming to circumvent backdoor defenses that examine these respective spaces. However, existing backdoor attacks are typically designed to resist a specific type of backdoor defense without considering the diverse range of defense mechanisms. Based on this observation, we pose a natural question: Are current backdoor attacks truly a real-world threat when facing diverse practical defenses?
  To answer this question, we examine 12 common backdoor attacks that focus on input-space or feature-space stealthiness and 17 diverse representative defenses. Surprisingly, we reveal a critical blind spot: Backdoor attacks designed to be stealthy in input and feature spaces can be mitigated by examining backdoored models in parameter space. To investigate the underlying causes behind this common vulnerability, we study the characteristics of backdoor attacks in the parameter space. Notably, we find that input- and feature-space attacks introduce prominent backdoor-related neurons in parameter space, which are not thoroughly considered by current backdoor attacks. Taking comprehensive stealthiness into account, we propose a novel supply-chain attack called Grond. Grond limits the parameter changes by a simple yet effective module, Adversarial Backdoor Injection (ABI), which adaptively increases the parameter-space stealthiness during the backdoor injection. Extensive experiments demonstrate that Grond outperforms all 12 backdoor attacks against state-of-the-art (including adaptive) defenses on CIFAR-10, GTSRB, and a subset of ImageNet. In addition, we show that ABI consistently improves the effectiveness of common backdoor attacks.","最近关于后门隐蔽性的研究主要集中在输入空间中不可区分的触发器和特征空间中不可分离的后门表示，旨在绕过检查这些相应空间的后门防御。然而，现有的后门攻击通常是为了抵御特定类型的后门防御而设计的，而没有考虑多种防御机制。基于这一观察，我们提出了一个自然的问题：当面对多种实际防御机制时，当前的后门攻击是否真的是现实世界的威胁？
为了回答这个问题，我们研究了12种常见的后门攻击，这些攻击主要集中在输入空间或特征空间的隐蔽性，以及17种多样化的代表性防御机制。令人惊讶的是，我们揭示了一个关键的盲点：在输入和特征空间中设计的隐蔽后门攻击可以通过在参数空间中检查后门模型来缓解。为了研究这种常见脆弱性背后的根本原因，我们研究了参数空间中后门攻击的特征。值得注意的是，我们发现输入和特征空间攻击在参数空间中引入了显著的与后门相关的神经元，这些神经元没有被当前的后门攻击充分考虑。综合考虑全面的隐蔽性，我们提出了一种新的供应链攻击，称为Grond。Grond通过一个简单但有效的模块，对抗性后门注入（ABI），在后门注入过程中自适应地增加参数空间的隐蔽性。广泛的实验表明，Grond在CIFAR-10、GTSRB和ImageNet的子集上优于所有12种后门攻击，抵御最新的（包括自适应的）防御机制。此外，我们还表明，ABI始终提高了常见后门攻击的有效性。","The paper introduces Grond, a new backdoor attack method that enhances stealthiness in the parameter space, outperforming existing attacks against various defenses.",LMM,Negative,Attack,"Backdoor attacks, parameter space, stealthiness, Grond, supply-chain attack"
Adversarial Reasoning at Jailbreaking Time,"Mahdi Sabbaghi, Paul Kassianik, George Pappas, Yaron Singer, Amin Karbasi, Hamed Hassani",2025-06-26T00:00:00-04:00,https://arxiv.org/pdf/2502.01633.pdf,"As large language models (LLMs) are becoming more capable and widespread, the study of their failure cases is becoming increasingly important. Recent advances in standardizing, measuring, and scaling test-time compute suggest new methodologies for optimizing models to achieve high performance on hard tasks. In this paper, we apply these advances to the task of model jailbreaking: eliciting harmful responses from aligned LLMs. We develop an adversarial reasoning approach to automatic jailbreaking that leverages a loss signal to guide the test-time compute, achieving SOTA attack success rates against many aligned LLMs, even those that aim to trade inference-time compute for adversarial robustness. Our approach introduces a new paradigm in understanding LLM vulnerabilities, laying the foundation for the development of more robust and trustworthy AI systems.",随着大型语言模型（LLM）变得越来越强大和普及，研究它们的失败案例变得越来越重要。最近在标准化、测量和扩展测试时计算的进展表明了新的方法来优化模型以实现在困难任务上的高性能。在本文中，我们将这些进展应用于模型越狱任务：从对齐的LLM中引发有害响应。我们开发了一种基于对抗推理的自动越狱方法，利用损失信号来指导测试时计算，实现了对许多对齐LLM的SOTA攻击成功率，即使是那些旨在以推理时计算换取对抗鲁棒性的LLM。我们的方法引入了一种新的范式，用于理解LLM的脆弱性，为开发更加健壮和可信的AI系统奠定了基础。,"The paper presents an adversarial reasoning approach for jailbreaking LLMs, achieving high attack success rates and highlighting LLM vulnerabilities.",LLM,Negative,Attack,"Jailbreaking, Adversarial Reasoning, Backdoor Attack, LLM Vulnerabilities, Harmful Responses"
Screen Hijack: Visual Poisoning of VLM Agents in Mobile Environments,"Xuan Wang, Siyuan Liang, Zhe Liu, Yi Yu, Yuliang Lu, Xiaochun Cao, Ee-Chien Chang, Xitong Gao",2025-06-26T00:00:00-04:00,https://arxiv.org/pdf/2506.13205.pdf,"With the growing integration of vision-language models (VLMs), mobile agents are now widely used for tasks like UI automation and camera-based user assistance. These agents are often fine-tuned on limited user-generated datasets, leaving them vulnerable to covert threats during the training process. In this work we present GHOST, the first clean-label backdoor attack specifically designed for mobile agents built upon VLMs. Our method manipulates only the visual inputs of a portion of the training samples - without altering their corresponding labels or instructions - thereby injecting malicious behaviors into the model. Once fine-tuned with this tampered data, the agent will exhibit attacker-controlled responses when a specific visual trigger is introduced at inference time. The core of our approach lies in aligning the gradients of poisoned samples with those of a chosen target instance, embedding backdoor-relevant features into the poisoned training data. To maintain stealth and enhance robustness, we develop three realistic visual triggers: static visual patches, dynamic motion cues, and subtle low-opacity overlays. We evaluate our method across six real-world Android apps and three VLM architectures adapted for mobile use. Results show that our attack achieves high attack success rates (up to 94.67 percent) while maintaining high clean-task performance (FSR up to 95.85 percent). Additionally, ablation studies shed light on how various design choices affect the efficacy and concealment of the attack. Overall, this work is the first to expose critical security flaws in VLM-based mobile agents, highlighting their susceptibility to clean-label backdoor attacks and the urgent need for effective defense mechanisms in their training pipelines.",随着视觉语言模型（VLMs）的广泛集成，移动代理现在广泛用于用户界面自动化和基于摄像头的用户辅助任务。这些代理通常在有限的用户生成数据集上进行微调，使其在训练过程中容易受到隐蔽威胁。在本文中，我们提出了GHOST，这是一种专门为基于VLMs的移动代理设计的第一个干净标签后门攻击。我们的方法仅操纵训练样本的一部分的视觉输入，而不改变它们对应的标签或指令，从而将恶意行为注入模型。在使用这些篡改数据进行微调后，代理在引入特定的视觉触发器时会表现出受攻击者控制的响应。我们的方法的核心在于将毒化样本的梯度与所选目标实例的梯度对齐，将后门相关特征嵌入到毒化的训练数据中。为了保持隐蔽性并增强鲁棒性，我们开发了三种现实的视觉触发器：静态视觉补丁、动态运动线索和低不透明度的微妙叠加。我们在六个真实的Android应用程序和三种适应移动使用的VLM架构上评估了我们的方法。结果表明，我们的攻击在保持高清洁任务性能（FSR高达95.85%）的同时实现了高攻击成功率（高达94.67%）。此外，消融研究揭示了各种设计选择如何影响攻击的有效性和隐蔽性。总的来说，本文首次揭示了基于VLM的移动代理的关键安全漏洞，突显了它们对干净标签后门攻击的易感性以及在其训练管道中有效防御机制的紧迫需求。,"The paper introduces GHOST, a clean-label backdoor attack on vision-language models used in mobile agents, demonstrating high attack success rates while maintaining clean-task performance.",LMM,Negative,Attack,"Backdoor attack, Vision-Language Models, Mobile agents, Visual poisoning, Clean-label attack"
