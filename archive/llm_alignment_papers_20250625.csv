Title,Authors,Published,Link,Abstract_EN,Abstract_ZH,One_Sentence_Summary,Model_Type,HHH_Focus,Keywords
MemeMind: A Large-Scale Multimodal Dataset with Chain-of-Thought Reasoning for Harmful Meme Detection,"Hexiang Gu, Qifan Yu, Saihui Hou, Zhiqin Fang, Huijia Wu, Zhaofeng He",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2506.18919.pdf,"The rapid development of social media has intensified the spread of harmful content. Harmful memes, which integrate both images and text, pose significant challenges for automated detection due to their implicit semantics and complex multimodal interactions. Although existing research has made progress in detection accuracy and interpretability, the lack of a systematic, large-scale, diverse, and highly explainable dataset continues to hinder further advancement in this field. To address this gap, we introduce MemeMind, a novel dataset featuring scientifically rigorous standards, large scale, diversity, bilingual support (Chinese and English), and detailed Chain-of-Thought (CoT) annotations. MemeMind fills critical gaps in current datasets by offering comprehensive labeling and explicit reasoning traces, thereby providing a solid foundation for enhancing harmful meme detection. In addition, we propose an innovative detection framework, MemeGuard, which effectively integrates multimodal information with reasoning process modeling, significantly improving models' ability to understand and identify harmful memes. Extensive experiments conducted on the MemeMind dataset demonstrate that MemeGuard consistently outperforms existing state-of-the-art methods in harmful meme detection tasks.",社交媒体的快速发展加剧了有害内容的传播。有害的模因，它们结合了图像和文本，由于其隐含的语义和复杂的多模态交互，对自动检测构成了重大挑战。尽管现有研究在检测准确性和可解释性方面取得了进展，但缺乏一个系统化、大规模、多样化且高度可解释的数据集，继续阻碍了该领域的进一步发展。为了解决这一问题，我们引入了MemeMind，一个具有科学严谨标准、大规模、多样性、双语支持（中文和英文）和详细的Chain-of-Thought（CoT）注释的新数据集。MemeMind通过提供全面的标签和显式的推理轨迹，填补了当前数据集的关键空白，从而为提高有害模因检测提供了坚实的基础。此外，我们提出了一种创新的检测框架MemeGuard，它有效地将多模态信息与推理过程建模相结合，显著提高了模型理解和识别有害模因的能力。在MemeMind数据集上进行的广泛实验表明，MemeGuard在有害模因检测任务中始终优于现有的最先进方法。,"The paper introduces MemeMind, a large-scale multimodal dataset with Chain-of-Thought reasoning for harmful meme detection, and proposes MemeGuard, a framework that improves the detection of harmful memes by integrating multimodal information and reasoning processes.",LLM,Harmless,"Harmful meme detection, multimodal dataset, Chain-of-Thought reasoning, harmful content, LLM alignment"
Do LLMs Know When to Flip a Coin? Strategic Randomization through Reasoning and Experience,Lingyu Yang (Shanghai Jiao Tong University),2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2506.18928.pdf,"Strategic randomization is a key principle in game theory, yet it remains underexplored in large language models (LLMs). Prior work often conflates the cognitive decision to randomize with the mechanical generation of randomness, leading to incomplete evaluations. To address this, we propose a novel zero-sum game inspired by the Tian Ji Horse Race, where the Nash equilibrium corresponds to a maximal entropy strategy. The game's complexity masks this property from untrained humans and underdeveloped LLMs. We evaluate five LLMs across prompt styles -- framed, neutral, and hinted -- using competitive multi-tournament gameplay with system-provided random choices, isolating the decision to randomize. Results show that weaker models remain deterministic regardless of prompts, while stronger models exhibit increased randomization under explicit hints. When facing weaker models, strong LLMs adopt deterministic strategies to exploit biases, but converge toward equilibrium play when facing peers. Through win/loss outcomes and Bayes factor analysis, we demonstrate meaningful variation in LLMs' strategic reasoning capabilities, highlighting opportunities for improvement in abstract reasoning and adaptive learning. We make our implementation publicly available at https://github.com/ocelopus/llm-when-to-throw-coin to ensure full reproducibility.",战略随机化是博弈论中的一个关键原则，但在大型语言模型（LLMs）中仍然研究不足。之前的工作往往将随机化的认知决策与随机性的机械生成混淆，导致评估不完整。为了解决这个问题，我们提出了一种新的零和游戏，灵感来自天机马赛，其中纳什均衡对应于最大熵策略。游戏的复杂性掩盖了这一属性，使未经训练的人类和未发育的LLMs无法察觉。我们在不同的提示风格（框架、中立和提示）下评估了五个LLMs，使用系统提供的随机选择进行竞争性多轮比赛，隔离随机化的决策。结果表明，较弱的模型在提示下仍然保持确定性，而较强的模型在明确的提示下表现出增加的随机化。面对较弱的模型时，强LLMs采用确定性策略以利用偏差，但在面对同行时，它们会趋向于均衡游戏。通过胜负结果和贝叶斯因子分析，我们展示了LLMs在战略推理能力上的有意义变化，突出了在抽象推理和适应性学习方面改进的机会。我们在https://github.com/ocelopus/llm-when-to-throw-coin上公开了我们的实现，以确保完全可重复性。,"The paper investigates how large language models (LLMs) decide when to randomize in a strategic game, revealing variations in their strategic reasoning capabilities.",LLM,"Helpful, Honest","Strategic Randomization, Game Theory, LLMs, Adaptive Learning, Nash Equilibrium"
Safe Pruning LoRA: Robust Distance-Guided Pruning for Safety Alignment in Adaptation of LLMs,"Shuang Ao, Yi Dong, Jinwei Hu, Sarvapali Ramchurn",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2506.18931.pdf,"Fine-tuning Large Language Models (LLMs) with Low-Rank Adaptation (LoRA) enhances adaptability while reducing computational costs. However, fine-tuning can compromise safety alignment, even with benign data, increasing susceptibility to harmful outputs. Existing safety alignment methods struggle to capture complex parameter shifts, leading to suboptimal safety-utility trade-offs. To address this issue, we propose Safe Pruning LoRA (SPLoRA), a novel pruning-based approach that selectively removes LoRA layers that weaken safety alignment, improving safety while preserving performance. At its core, we introduce Empirical-DIEM (E-DIEM), a dimension-insensitive similarity metric that effectively detects safety misalignment in LoRA-adapted models. We conduct extensive experiments on LLMs fine-tuned with mixed of benign and malicious data, and purely benign datasets, evaluating SPLoRA across utility, safety, and reliability metrics. Results demonstrate that SPLoRA outperforms state-of-the-art safety alignment techniques, significantly reducing safety risks while maintaining or improving model performance and reliability. Additionally, SPLoRA reduces inference overhead, making it a scalable and efficient solution for deploying safer and more reliable LLMs. The code is available at https://github.com/AoShuang92/SPLoRA.",使用低秩适应（LoRA）微调大型语言模型（LLMs）可以增强适应性，同时降低计算成本。然而，微调可能会破坏安全对齐，即使使用良性数据，也会增加产生有害输出的风险。现有的安全对齐方法难以捕捉复杂的参数变化，导致安全性和效用之间的权衡不佳。为了解决这个问题，我们提出了安全修剪LoRA（SPLoRA），一种基于修剪的方法，选择性地删除削弱安全对齐的LoRA层，从而提高安全性，同时保持性能。在其核心，我们引入了经验-DIEM（E-DIEM），一种维度无关的相似度度量，能够有效检测LoRA适应模型中的安全对齐问题。我们在LLMs上进行了大量实验，这些模型使用混合的良性和恶意数据以及纯良性数据进行了微调，并评估了SPLoRA的效用、安全性和可靠性指标。结果表明，SPLoRA在显著减少安全风险的同时，优于现有的安全对齐技术，保持或提高了模型的性能和可靠性。此外，SPLoRA还减少了推理开销，使其成为一种可扩展且高效的解决方案，用于部署更安全和更可靠的LLMs。代码可在https://github.com/AoShuang92/SPLoRA获得。,"The paper introduces SPLoRA, a pruning-based method to maintain safety alignment in LLMs during fine-tuning with LoRA, reducing harmful outputs while preserving performance.",LLM,Harmless,"Safety alignment, LoRA, Pruning, Harmful outputs, LLM fine-tuning"
Advanced Applications of Generative AI in Actuarial Science: Case Studies Beyond ChatGPT,"Simon Hatzesberger, Iris Nonneman",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2506.18942.pdf,"This article demonstrates the transformative impact of Generative AI (GenAI) on actuarial science, illustrated by four implemented case studies. It begins with a historical overview of AI, tracing its evolution from early neural networks to modern GenAI technologies. The first case study shows how Large Language Models (LLMs) improve claims cost prediction by deriving significant features from unstructured textual data, significantly reducing prediction errors in the underlying machine learning task. In the second case study, we explore the automation of market comparisons using the GenAI concept of Retrieval-Augmented Generation to identify and process relevant information from documents. A third case study highlights the capabilities of fine-tuned vision-enabled LLMs in classifying car damage types and extracting contextual information. The fourth case study presents a multi-agent system that autonomously analyzes data from a given dataset and generates a corresponding report detailing the key findings. In addition to these case studies, we outline further potential applications of GenAI in the insurance industry, such as the automation of claims processing and fraud detection, and the verification of document compliance with internal or external policies. Finally, we discuss challenges and considerations associated with the use of GenAI, covering regulatory issues, ethical concerns, and technical limitations, among others.",这篇文章展示了生成式人工智能（GenAI）对精算学的变革性影响，通过四个实施的案例研究来说明。它从人工智能的历史概述开始，追溯其从早期神经网络到现代GenAI技术的演变。第一个案例研究展示了大型语言模型（LLMs）如何通过从非结构化文本数据中提取显著特征来改善索赔成本预测，显著减少基础机器学习任务中的预测错误。在第二个案例研究中，我们探讨了使用GenAI概念的检索增强生成来自动化市场比较，以识别和处理文档中的相关信息。第三个案例研究突出了微调的视觉启用LLMs在分类车辆损坏类型和提取上下文信息的能力。第四个案例研究展示了一个多代理系统，自主分析给定数据集中的数据并生成相应的报告，详细说明关键发现。除了这些案例研究，我们还概述了GenAI在保险行业的进一步潜在应用，例如自动化索赔处理和欺诈检测，以及验证文档是否符合内部或外部政策。最后，我们讨论了使用GenAI相关的挑战和考虑因素，涵盖监管问题、伦理问题和技术限制等。,"The paper explores the use of LLMs in actuarial science, demonstrating their effectiveness in improving claims cost prediction and other tasks through various case studies.",LLM,None,"LLMs, Actuarial Science, Claims Cost Prediction, Generative AI, Case Studies"
Mirage of Mastery: Memorization Tricks LLMs into Artificially Inflated Self-Knowledge,"Sahil Kale, Vijaykant Nadadur",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2506.18998.pdf,"When artificial intelligence mistakes memorization for intelligence, it creates a dangerous mirage of reasoning. Existing studies treat memorization and self-knowledge deficits in LLMs as separate issues and do not recognize an intertwining link that degrades the trustworthiness of LLM responses. In our study, we utilize a novel framework to ascertain if LLMs genuinely learn reasoning patterns from training data or merely memorize them to assume competence across problems of similar complexity focused on STEM domains. Our analysis shows a noteworthy problem in generalization: LLMs draw confidence from memorized solutions to infer a higher self-knowledge about their reasoning ability, which manifests as an over 45% inconsistency in feasibility assessments when faced with self-validated, logically coherent task perturbations. This effect is most pronounced in science and medicine domains, which tend to have maximal standardized jargon and problems, further confirming our approach. Significant wavering within the self-knowledge of LLMs also shows flaws in current architectures and training patterns, highlighting the need for techniques that ensure a balanced, consistent stance on models' perceptions of their own knowledge for maximum AI explainability and trustworthiness. Our code and results are available publicly at https://github.com/knowledge-verse-ai/LLM-Memorization_SK_Eval-.",当人工智能将记忆误认为是智能时，它会创造出一种危险的推理幻象。现有研究将LLM中的记忆和自我知识缺陷视为独立问题，并没有认识到一个相互交织的链接，这种链接会降低LLM响应的可信度。在我们的研究中，我们利用一种新颖的框架来确定LLM是否真正从训练数据中学习推理模式，还是仅仅将其记忆下来，以假装在类似复杂性问题上具有能力，这些问题主要集中在STEM领域。我们的分析显示了一个显著的泛化问题：LLM从记忆解决方案中获得信心，以推断其推理能力的更高自我知识，这在面对自我验证的、逻辑上一致的任务扰动时表现为超过45%的可行性评估不一致。这种效应在科学和医学领域最为明显，这些领域往往具有最大的标准化术语和问题，进一步确认了我们的方法。LLM自我知识中的显著波动也暴露了当前架构和训练模式中的缺陷，突出了需要确保模型对其自身知识的感知保持平衡、一致的技术，以实现最大的AI可解释性和可信度。我们的代码和结果公开可用于https://github.com/knowledge-verse-ai/LLM-Memorization_SK_Eval-。,"The paper investigates how LLMs' tendency to memorize information instead of learning reasoning patterns affects their self-knowledge and trustworthiness, particularly in STEM domains.",LLM,"Helpful, Honest","Memorization, Self-Knowledge, Generalization, Trustworthiness, STEM"
Quantifying Fairness in LLMs Beyond Tokens: A Semantic and Statistical Perspective,"Weijie Xu, Yiwen Wang, Chi Xue, Xiangkun Hu, Xi Fang, Guimin Dong, Chandan K. Reddy",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2506.19028.pdf,"Large Language Models (LLMs) often generate responses with inherent biases, undermining their reliability in real-world applications. Existing evaluation methods often overlook biases in long-form responses and the intrinsic variability of LLM outputs. To address these challenges, we propose FiSCo(Fine-grained Semantic Computation), a novel statistical framework to evaluate group-level fairness in LLMs by detecting subtle semantic differences in long-form responses across demographic groups. Unlike prior work focusing on sentiment or token-level comparisons, FiSCo goes beyond surface-level analysis by operating at the claim level, leveraging entailment checks to assess the consistency of meaning across responses. We decompose model outputs into semantically distinct claims and apply statistical hypothesis testing to compare inter- and intra-group similarities, enabling robust detection of subtle biases. We formalize a new group counterfactual fairness definition and validate FiSCo on both synthetic and human-annotated datasets spanning gender, race, and age. Experiments show that FiSco more reliably identifies nuanced biases while reducing the impact of stochastic LLM variability, outperforming various evaluation metrics.",大语言模型（LLMs）通常生成带有内在偏见的响应，削弱了它们在现实世界应用中的可靠性。现有的评估方法往往忽略了长文本响应中的偏见以及LLM输出的内在变异性。为了应对这些挑战，我们提出了FiSCo（细粒度语义计算），一种新的统计框架，通过检测长文本响应在不同人群之间的微妙语义差异，来评估LLM的群体级公平性。与之前的工作不同，FiSCo不仅仅关注情感或标记级别的比较，而是通过在声明级别进行操作，利用推理检查来评估响应中的意义一致性。我们将模型输出分解为语义上不同的声明，并应用统计假设检验来比较群体内和群体间的相似性，从而实现对微妙偏见的可靠检测。我们正式化了一个新的群体对抗性公平定义，并在涵盖性别、种族和年龄的合成和人工标注数据集上验证了FiSCo。实验表明，FiSco更可靠地识别出细微的偏见，同时减少了随机LLM变异的影响，优于各种评估指标。,"The paper introduces FiSCo, a statistical framework for evaluating group-level fairness in LLMs by detecting subtle semantic biases in long-form responses.",LLM,Harmless,"Fairness, Bias, Evaluation, Semantic Analysis, LLMs"
PolyGuard: Massive Multi-Domain Safety Policy-Grounded Guardrail Dataset,"Mintong Kang, Zhaorun Chen, Chejian Xu, Jiawei Zhang, Chengquan Guo, Minzhou Pan, Ivan Revilla, Yu Sun, Bo Li",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2506.19054.pdf,"As LLMs become widespread across diverse applications, concerns about the security and safety of LLM interactions have intensified. Numerous guardrail models and benchmarks have been developed to ensure LLM content safety. However, existing guardrail benchmarks are often built upon ad hoc risk taxonomies that lack a principled grounding in standardized safety policies, limiting their alignment with real-world operational requirements. Moreover, they tend to overlook domain-specific risks, while the same risk category can carry different implications across different domains. To bridge these gaps, we introduce PolyGuard, the first massive multi-domain safety policy-grounded guardrail dataset. PolyGuard offers: (1) broad domain coverage across eight safety-critical domains, such as finance, law, and codeGen; (2) policy-grounded risk construction based on authentic, domain-specific safety guidelines; (3) diverse interaction formats, encompassing declarative statements, questions, instructions, and multi-turn conversations; (4) advanced benign data curation via detoxification prompting to challenge over-refusal behaviors; and (5) \textbf{attack-enhanced instances} that simulate adversarial inputs designed to bypass guardrails. Based on PolyGuard, we benchmark 19 advanced guardrail models and uncover a series of findings, such as: (1) All models achieve varied F1 scores, with many demonstrating high variance across risk categories, highlighting their limited domain coverage and insufficient handling of domain-specific safety concerns; (2) As models evolve, their coverage of safety risks broadens, but performance on common risk categories may decrease; (3) All models remain vulnerable to optimized adversarial attacks. We believe that \dataset and the unique insights derived from our evaluations will advance the development of policy-aligned and resilient guardrail systems.",随着大型语言模型（LLM）在各种应用中的普及，关于LLM交互的安全性和安全性的担忧加剧。开发了许多防护栏模型和基准，以确保LLM内容的安全性。然而，现有的防护栏基准通常基于临时风险分类，缺乏在标准化安全策略中的原则性基础，从而限制了它们与实际运营要求的对齐。此外，它们往往忽略特定领域的风险，而同一风险类别在不同领域可能具有不同的含义。为了弥合这些差距，我们引入了PolyGuard，这是第一个大规模多领域安全策略基础防护栏数据集。PolyGuard提供：(1) 广泛的领域覆盖，跨越八个安全关键领域，如金融、法律和代码生成；(2) 基于真实、特定领域安全指南的策略基础风险构建；(3) 多种交互格式，包括陈述、问题、指令和多轮对话；(4) 通过解毒提示进行高级良性数据策划，以挑战过度拒绝行为；(5) 模拟设计以绕过防护栏的对抗性输入的攻击增强实例。基于PolyGuard，我们对19个先进的防护栏模型进行了基准测试，并揭示了一系列发现，例如：(1) 所有模型在不同的风险类别中实现了不同的F1分数，许多模型在不同的风险类别中表现出高方差，突出了它们的有限领域覆盖和不足的领域特定安全关注；(2) 随着模型的演变，它们的安全风险覆盖范围扩大，但对常见风险类别的性能可能会下降；(3) 所有模型仍然容易受到优化的对抗性攻击。我们相信该数据集及其评估中独特的见解将推动策略对齐和防护栏系统的发展。,"The paper introduces PolyGuard, a multi-domain safety policy-grounded guardrail dataset for evaluating and improving the safety and security of LLM interactions.",LLM,Harmless,"Safety, Guardrail, Policy, Multi-domain, Adversarial"
MFTCXplain: A Multilingual Benchmark Dataset for Evaluating the Moral Reasoning of LLMs through Hate Speech Multi-hop Explanation,"Jackson Trager, Francielle Vargas, Diego Alves, Matteo Guida, Mikel K. Ngueajio, Ameeta Agrawal, Flor Plaza-del-Arco, Yalda Daryanai, Farzan Karimi-Malekabadi",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2506.19073.pdf,"Ensuring the moral reasoning capabilities of Large Language Models (LLMs) is a growing concern as these systems are used in socially sensitive tasks. Nevertheless, current evaluation benchmarks present two major shortcomings: a lack of annotations that justify moral classifications, which limits transparency and interpretability; and a predominant focus on English, which constrains the assessment of moral reasoning across diverse cultural settings. In this paper, we introduce MFTCXplain, a multilingual benchmark dataset for evaluating the moral reasoning of LLMs via hate speech multi-hop explanation using Moral Foundation Theory (MFT). The dataset comprises 3,000 tweets across Portuguese, Italian, Persian, and English, annotated with binary hate speech labels, moral categories, and text span-level rationales. Empirical results highlight a misalignment between LLM outputs and human annotations in moral reasoning tasks. While LLMs perform well in hate speech detection (F1 up to 0.836), their ability to predict moral sentiments is notably weak (F1 < 0.35). Furthermore, rationale alignment remains limited mainly in underrepresented languages. These findings show the limited capacity of current LLMs to internalize and reflect human moral reasoning.","随着大型语言模型（LLMs）在社会敏感任务中的使用，确保其道德推理能力成为一个日益重要的问题。然而，当前的评估基准存在两个主要缺点：缺乏对道德分类的注释，这限制了透明度和可解释性；以及主要集中在英语，这限制了跨不同文化背景下道德推理的评估。在本文中，我们介绍了 MFTCXplain，一个通过仇恨言论多跳解释使用道德基础理论（MFT）来评估 LLMs 道德推理的多语言基准数据集。该数据集包含 3,000 条来自葡萄牙语、意大利语、波斯语和英语的推文，并附有二元仇恨言论标签、道德类别和文本跨度级别的理由。实证结果突显了 LLM 输出与人类注释在道德推理任务中的不一致。虽然 LLMs 在仇恨言论检测方面表现良好（F1 最高达 0.836），但其预测道德情感的能力明显较弱（F1 < 0.35）。此外，理由对齐主要在低代表性语言中仍然有限。这些发现表明当前 LLMs 无法充分内化和反映人类道德推理。","The paper presents MFTCXplain, a multilingual dataset for evaluating the moral reasoning of LLMs through hate speech explanations, highlighting the misalignment between LLM outputs and human moral reasoning.",LLM,"Helpful, Harmless","Moral reasoning, LLM alignment, multilingual, hate speech, Moral Foundation Theory"
Human-Aligned Faithfulness in Toxicity Explanations of LLMs,"Ramaravind K. Mothilal, Joanna Roy, Syed Ishtiaque Ahmed, Shion Guha",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2506.19113.pdf,"The discourse around toxicity and LLMs in NLP largely revolves around detection tasks. This work shifts the focus to evaluating LLMs' reasoning about toxicity -- from their explanations that justify a stance -- to enhance their trustworthiness in downstream tasks. Despite extensive research on explainability, it is not straightforward to adopt existing methods to evaluate free-form toxicity explanation due to their over-reliance on input text perturbations, among other challenges. To account for these, we propose a novel, theoretically-grounded multi-dimensional criterion, Human-Aligned Faithfulness (HAF), that measures the extent to which LLMs' free-form toxicity explanations align with those of a rational human under ideal conditions. We develop six metrics, based on uncertainty quantification, to comprehensively evaluate \haf of LLMs' toxicity explanations with no human involvement, and highlight how ""non-ideal"" the explanations are. We conduct several experiments on three Llama models (of size up to 70B) and an 8B Ministral model on five diverse toxicity datasets. Our results show that while LLMs generate plausible explanations to simple prompts, their reasoning about toxicity breaks down when prompted about the nuanced relations between the complete set of reasons, the individual reasons, and their toxicity stances, resulting in inconsistent and nonsensical responses. We open-source our code and LLM-generated explanations at https://github.com/uofthcdslab/HAF.",关于毒性和LLMs的讨论在NLP中主要集中在检测任务上。本文将重点转向评估LLMs关于毒性的推理——从它们的解释中得出的立场——以增强它们在下游任务中的可信度。尽管在可解释性方面有大量研究，但由于它们过于依赖输入文本扰动等问题，很难直接采用现有方法来评估自由形式的毒性解释。为了应对这些问题，我们提出了一种新颖的、基于理论的多维度标准，即人类对齐的真实性（HAF），它衡量LLMs的自由形式毒性解释在理想条件下与理性人类的对齐程度。我们开发了六种基于不确定性量化的指标，以全面评估LLMs的毒性解释的HAF，而无需人类参与，并突出解释的“非理想”程度。我们在三个Llama模型（大小高达70B）和一个8B Ministral模型上进行了多个实验，使用五个多样化的毒性数据集。结果表明，尽管LLMs能够为简单提示生成合理的解释，但在涉及完整原因集、单个原因及其毒性立场之间的微妙关系时，其关于毒性的推理会崩溃，导致不一致和荒谬的响应。我们在https://github.com/uofthcdslab/HAF上开源了我们的代码和LLM生成的解释。,"The paper introduces a novel criterion, Human-Aligned Faithfulness, to evaluate the alignment of LLMs' toxicity explanations with human rationales, highlighting the challenges and inconsistencies in LLM reasoning about toxicity.",LLM,"Helpful, Harmless","Toxicity, Explanations, Alignment, Faithfulness, LLMs"
Command-V: Pasting LLM Behaviors via Activation Profiles,"Barry Wang, Avi Schwarzschild, Alexander Robey, Ali Payani, Charles Fleming, Mingjie Sun, Daphne Ippolito",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2506.19140.pdf,"Retrofitting large language models (LLMs) with new behaviors typically requires full finetuning or distillation-costly steps that must be repeated for every architecture. In this work, we introduce Command-V, a backpropagation-free behavior transfer method that copies an existing residual activation adapter from a donor model and pastes its effect into a recipient model. Command-V profiles layer activations on a small prompt set, derives linear converters between corresponding layers, and applies the donor intervention in the recipient's activation space. This process does not require access to the original training data and needs minimal compute. In three case studies-safety-refusal enhancement, jailbreak facilitation, and automatic chain-of-thought reasoning--Command-V matches or exceeds the performance of direct finetuning while using orders of magnitude less compute. Our code and data are accessible at https://github.com/GithuBarry/Command-V/.",将大型语言模型（LLM）配备新行为通常需要全面的微调或蒸馏，这些步骤成本高且必须为每种架构重复。在本文中，我们引入了Command-V，一种无需反向传播的行为转移方法，它将来自捐赠模型的现有残差激活适配器复制并将其效果粘贴到接收模型中。Command-V在小型提示集上配置层激活，推导相应层之间的线性转换器，并在接收者的激活空间中应用捐赠者干预。该过程不需要访问原始训练数据，并且计算量最小。在三个案例研究中——安全拒绝增强、越狱促进和自动思维链推理——Command-V在使用的计算量比直接微调少了几个数量级的情况下，匹配或超过其性能。我们的代码和数据可在https://github.com/GithuBarry/Command-V/访问。,"The paper introduces Command-V, a method for transferring behaviors in LLMs, including safety-refusal enhancement, using minimal compute and without needing original training data.",LLM,Harmless,"Behavior transfer, safety-refusal, activation profiles, LLM alignment, minimal compute"
Spiritual-LLM : Gita Inspired Mental Health Therapy In the Era of LLMs,"Janak Kapuriya, Aman Singh, Jainendra Shukla, Rajiv Ratn Shah",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2506.19185.pdf,"Traditional mental health support systems often generate responses based solely on the user's current emotion and situations, resulting in superficial interventions that fail to address deeper emotional needs. This study introduces a novel framework by integrating spiritual wisdom from the Bhagavad Gita with advanced large language model GPT-4o to enhance emotional well-being. We present the GITes (Gita Integrated Therapy for Emotional Support) dataset, which enhances the existing ExTES mental health dataset by including 10,729 spiritually guided responses generated by GPT-4o and evaluated by domain experts. We benchmark GITes against 12 state-of-the-art LLMs, including both mental health specific and general purpose models. To evaluate spiritual relevance in generated responses beyond what conventional n-gram based metrics capture, we propose a novel Spiritual Insight metric and automate assessment via an LLM as jury framework using chain-of-thought prompting. Integrating spiritual guidance into AI driven support enhances both NLP and spiritual metrics for the best performing LLM Phi3-Mini 3.2B Instruct, achieving improvements of 122.71% in ROUGE, 126.53% in METEOR, 8.15% in BERT score, 15.92% in Spiritual Insight, 18.61% in Sufficiency and 13.22% in Relevance compared to its zero-shot counterpart. While these results reflect substantial improvements across automated empathy and spirituality metrics, further validation in real world patient populations remains a necessary step. Our findings indicate a strong potential for AI systems enriched with spiritual guidance to enhance user satisfaction and perceived support outcomes. The code and dataset will be publicly available to advance further research in this emerging area.","传统的心理健康支持系统通常根据用户的当前情绪和情况生成响应，结果是表面的干预，无法解决更深层次的情感需求。本研究通过将《薄伽梵歌》中的精神智慧与先进的大型语言模型GPT-4o结合，提出了一种新颖的框架，以增强情感福祉。我们提出了GITes（Gita集成情感支持治疗）数据集，通过包含由GPT-4o生成并由领域专家评估的10,729条精神指导响应，增强了现有的ExTES心理健康数据集。我们将GITes与12种最先进的LLM进行基准测试，包括专门针对心理健康和通用目的的模型。为了评估生成响应中的精神相关性，超越了传统的n-gram基于的指标，我们提出了一种新的精神洞察指标，并通过链式思维提示的LLM作为陪审团框架自动评估。将精神指导集成到AI驱动的支持中，不仅提高了NLP和精神指标，还提高了最佳表现的LLM Phi3-Mini 3.2B Instruct的表现，在ROUGE、METEOR、BERT分数、精神洞察、充分性和相关性方面分别实现了122.71%、126.53%、8.15%、15.92%、18.61%和13.22%的改进，与其零样本对比。虽然这些结果反映了自动同情和精神指标的显著改进，但在真实世界的患者群体中进行进一步验证仍然是必要的步骤。我们的发现表明，丰富精神指导的AI系统有很大潜力提高用户满意度和感知支持结果。代码和数据集将公开发布，以推动这一新兴领域的进一步研究。","The paper introduces a framework that integrates spiritual wisdom from the Bhagavad Gita with LLMs to enhance mental health support, demonstrating significant improvements in emotional well-being metrics.",LLM,Helpful,"LLM, Mental Health, Spiritual Guidance, Emotional Well-being, GPT-4o"
RecLLM-R1: A Two-Stage Training Paradigm with Reinforcement Learning and Chain-of-Thought v1,"Yu Xie, Xingkai Ren, Ying Qi, Yao Hu, Lianlei Shan",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2506.19235.pdf,"Traditional recommendation systems often grapple with ""filter bubbles"", underutilization of external knowledge, and a disconnect between model optimization and business policy iteration. To address these limitations, this paper introduces RecLLM-R1, a novel recommendation framework leveraging Large Language Models (LLMs) and drawing inspiration from the DeepSeek R1 methodology. The framework initiates by transforming user profiles, historical interactions, and multi-faceted item attributes into LLM-interpretable natural language prompts through a carefully engineered data construction process. Subsequently, a two-stage training paradigm is employed: the initial stage involves Supervised Fine-Tuning (SFT) to imbue the LLM with fundamental recommendation capabilities. The subsequent stage utilizes Group Relative Policy Optimization (GRPO), a reinforcement learning technique, augmented with a Chain-of-Thought (CoT) mechanism. This stage guides the model through multi-step reasoning and holistic decision-making via a flexibly defined reward function, aiming to concurrently optimize recommendation accuracy, diversity, and other bespoke business objectives. Empirical evaluations on a real-world user behavior dataset from a large-scale social media platform demonstrate that RecLLM-R1 significantly surpasses existing baseline methods across a spectrum of evaluation metrics, including accuracy, diversity, and novelty. It effectively mitigates the filter bubble effect and presents a promising avenue for the integrated optimization of recommendation models and policies under intricate business goals.",传统的推荐系统往往面临“信息茧房”、“外部知识利用不足”和模型优化与业务政策迭代脱节等问题。为了解决这些局限性，本文提出了RecLLM-R1，一种利用大型语言模型（LLM）并受DeepSeek R1方法启发的新型推荐框架。该框架首先通过精心设计的数据构建过程，将用户档案、历史互动和多方面的项目属性转换为LLM可解释的自然语言提示。随后，采用两阶段训练范式：初始阶段通过监督微调（SFT）使LLM具备基本的推荐能力；后续阶段利用增强学习技术，结合思维链（CoT）机制，通过灵活定义的奖励函数，引导模型进行多步推理和整体决策，旨在同时优化推荐的准确性、多样性和其他定制的业务目标。在一个大型社交媒体平台的真实用户行为数据集上的实证评估表明，RecLLM-R1在准确性、多样性和新颖性等一系列评估指标上显著超越现有的基线方法，有效缓解了信息茧房效应，并为在复杂业务目标下推荐模型和政策的集成优化提供了有前途的途径。,"The paper introduces RecLLM-R1, a recommendation framework using LLMs and reinforcement learning to optimize for accuracy, diversity, and business objectives.",LLM,Helpful,"Reinforcement Learning, Chain-of-Thought, Recommendation Systems, LLM Alignment, Policy Optimization"
Inference-Time Reward Hacking in Large Language Models,"Hadi Khalaf, Claudio Mayrink Verdun, Alex Oesterling, Himabindu Lakkaraju, Flavio du Pin Calmon",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2506.19248.pdf,"A common paradigm to improve the performance of large language models is optimizing for a reward model. Reward models assign a numerical score to LLM outputs indicating, for example, which response would likely be preferred by a user or is most aligned with safety goals. However, reward models are never perfect. They inevitably function as proxies for complex desiderata such as correctness, helpfulness, and safety. By overoptimizing for a misspecified reward, we can subvert intended alignment goals and reduce overall performance -- a phenomenon commonly referred to as reward hacking. In this work, we characterize reward hacking in inference-time alignment and demonstrate when and how we can mitigate it by hedging on the proxy reward. We study this phenomenon under Best-of-$n$ (BoN) and Soft-Best-of-$n$ (SBoN), and we introduce Best-of-Poisson (BoP) that provides an efficient, near-exact approximation of the optimal reward-KL divergence policy at inference time. We show that the characteristic pattern of hacking as observed in practice (where the true reward first increases before declining) is an inevitable property of a broad class of inference-time mechanisms, including BoN and BoP. To counter this effect, hedging offers a tactical choice to avoid placing undue confidence in high but potentially misleading proxy reward signals. We introduce HedgeTune, an efficient algorithm to find the optimal inference-time parameter and avoid reward hacking. We demonstrate through experiments that hedging mitigates reward hacking and achieves superior distortion-reward tradeoffs with minimal computational overhead.",优化奖励模型是提高大型语言模型性能的常见范式。奖励模型为LLM输出分配数值分数，指示哪个响应可能会被用户更喜欢或最符合安全目标。然而，奖励模型永远不是完美的。它们不可避免地作为复杂的正确性、有用性和安全性等目标的代理。通过过度优化一个错误指定的奖励，我们可以颠覆预期的对齐目标并降低整体性能——这种现象通常被称为奖励黑客攻击。在本工作中，我们在推理时间对齐中表征奖励黑客攻击，并演示了何时以及如何通过对代理奖励进行对冲来缓解它。我们在Best-of-$n$ (BoN)和Soft-Best-of-$n$ (SBoN)下研究了这种现象，并引入了Best-of-Poisson (BoP)，它在推理时间提供了一个高效的、近似最优的奖励-KL散度策略。我们表明，在实践中观察到的黑客攻击的特征模式（真实奖励首先增加然后下降）是一类广泛的推理时间机制的不可避免的属性，包括BoN和BoP。为了对抗这种效应，对冲提供了一种策略选择，以避免对高但可能具有误导性的代理奖励信号过于自信。我们引入了HedgeTune，一种高效的算法，用于找到最佳的推理时间参数并避免奖励黑客攻击。我们通过实验表明，对冲缓解了奖励黑客攻击，并在最小的计算开销下实现了更好的失真-奖励权衡。,"The paper explores reward hacking in large language models and introduces hedging methods to mitigate it, ensuring better alignment with helpful and harmless goals.",LLM,"Helpful, Harmless","Reward hacking, alignment, inference-time, large language models, hedging"
EmoStage: A Framework for Accurate Empathetic Response Generation via Perspective-Taking and Phase Recognition,"Zhiyang Qi, Keiko Takamizo, Mariko Ukiyo, Michimasa Inaba",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2506.19279.pdf,"The rising demand for mental health care has fueled interest in AI-driven counseling systems. While large language models (LLMs) offer significant potential, current approaches face challenges, including limited understanding of clients' psychological states and counseling stages, reliance on high-quality training data, and privacy concerns associated with commercial deployment. To address these issues, we propose EmoStage, a framework that enhances empathetic response generation by leveraging the inference capabilities of open-source LLMs without additional training data. Our framework introduces perspective-taking to infer clients' psychological states and support needs, enabling the generation of emotionally resonant responses. In addition, phase recognition is incorporated to ensure alignment with the counseling process and to prevent contextually inappropriate or inopportune responses. Experiments conducted in both Japanese and Chinese counseling settings demonstrate that EmoStage improves the quality of responses generated by base models and performs competitively with data-driven methods.",心理健康护理需求的增加促进了对基于人工智能的咨询系统的兴趣。虽然大型语言模型（LLMs）提供了显著的潜力，但当前的方法面临一些挑战，包括对客户心理状态和咨询阶段的理解有限、依赖高质量的训练数据以及与商业部署相关的隐私问题。为了解决这些问题，我们提出了EmoStage框架，通过利用开源LLMs的推理能力而无需额外的训练数据，增强了共情响应的生成。我们的框架引入了视角采用来推断客户的心理状态和支持需求，从而实现情感共鸣的响应生成。此外，还纳入了阶段识别，以确保与咨询过程保持一致，并防止上下文不适当或不合时宜的响应。在日本和中国的咨询设置中进行的实验表明，EmoStage提高了基础模型生成的响应质量，并与基于数据的方法竞争。,"The paper introduces EmoStage, a framework that uses open-source LLMs to generate empathetic and contextually appropriate responses in counseling settings.",LLM,"Helpful, Harmless","Empathetic response, counseling, perspective-taking, phase recognition, alignment"
Spotting Out-of-Character Behavior: Atomic-Level Evaluation of Persona Fidelity in Open-Ended Generation,"Jisu Shin, Juhyun Oh, Eunsu Kim, Hoyun Song, Alice Oh",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2506.19352.pdf,"Ensuring persona fidelity in large language models (LLMs) is essential for maintaining coherent and engaging human-AI interactions. However, LLMs often exhibit Out-of-Character (OOC) behavior, where generated responses deviate from an assigned persona, leading to inconsistencies that affect model reliability. Existing evaluation methods typically assign single scores to entire responses, struggling to capture subtle persona misalignment, particularly in long-form text generation. To address this limitation, we propose an atomic-level evaluation framework that quantifies persona fidelity at a finer granularity. Our three key metrics measure the degree of persona alignment and consistency within and across generations. Our approach enables a more precise and realistic assessment of persona fidelity by identifying subtle deviations that real users would encounter. Through our experiments, we demonstrate that our framework effectively detects persona inconsistencies that prior methods overlook. By analyzing persona fidelity across diverse tasks and personality types, we reveal how task structure and persona desirability influence model adaptability, highlighting challenges in maintaining consistent persona expression.",确保大型语言模型（LLMs）的角色一致性对于维持连贯和引人入胜的人机互动至关重要。然而，LLMs 往往表现出角色不符（OOC）行为，生成的响应偏离了指定的角色，导致影响模型可靠性的不一致。现有的评估方法通常为整个响应分配单个分数，难以捕捉长篇文本生成中的细微角色不一致。为了解决这一局限性，我们提出了一种原子级评估框架，以更细粒度的方式量化角色一致性。我们的三个关键指标衡量了生成内部和跨生成的角色对齐度和一致性。我们的方法通过识别真实用户可能遇到的细微偏差，使角色一致性的评估更加精确和现实。通过我们的实验，我们证明了我们的框架有效地检测到先前方法忽略的角色不一致。通过分析跨多种任务和性格类型的角色一致性，我们揭示了任务结构和角色可取性如何影响模型的适应性，突出了在保持一致的角色表达方面的挑战。,"The paper introduces an atomic-level evaluation framework to detect subtle persona inconsistencies in large language models, enhancing the assessment of persona fidelity.",LLM,Helpful,"Persona fidelity, Out-of-Character behavior, Atomic-level evaluation, Large language models, Persona alignment"
Measuring and Guiding Monosemanticity,"Ruben H\""arle, Felix Friedrich, Manuel Brack, Stephan W\""aldchen, Bj\""orn Deiseroth, Patrick Schramowski, Kristian Kersting",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2506.19382.pdf,"There is growing interest in leveraging mechanistic interpretability and controllability to better understand and influence the internal dynamics of large language models (LLMs). However, current methods face fundamental challenges in reliably localizing and manipulating feature representations. Sparse Autoencoders (SAEs) have recently emerged as a promising direction for feature extraction at scale, yet they, too, are limited by incomplete feature isolation and unreliable monosemanticity. To systematically quantify these limitations, we introduce Feature Monosemanticity Score (FMS), a novel metric to quantify feature monosemanticity in latent representation. Building on these insights, we propose Guided Sparse Autoencoders (G-SAE), a method that conditions latent representations on labeled concepts during training. We demonstrate that reliable localization and disentanglement of target concepts within the latent space improve interpretability, detection of behavior, and control. Specifically, our evaluations on toxicity detection, writing style identification, and privacy attribute recognition show that G-SAE not only enhances monosemanticity but also enables more effective and fine-grained steering with less quality degradation. Our findings provide actionable guidelines for measuring and advancing mechanistic interpretability and control of LLMs.",近年来，越来越多的人对利用机制可解释性和可控性来更好地理解和影响大型语言模型（LLMs）的内部动态产生了兴趣。然而，当前方法在可靠定位和操作特征表示方面面临根本性挑战。稀疏自编码器（SAEs）最近作为大规模特征提取的有前途方向，但它们也受到不完整特征隔离和不可靠单义性的限制。为了系统地量化这些限制，我们引入了特征单义性得分（FMS），这是一个新的度量标准，用于量化潜在表示中的特征单义性。基于这些见解，我们提出了指导稀疏自编码器（G-SAE），一种在训练期间将潜在表示条件化为标记概念的方法。我们证明，可靠的目标概念定位和潜在空间中的分离改善了可解释性、行为检测和控制。具体来说，我们在毒性检测、写作风格识别和隐私属性识别的评估中表明，G-SAE不仅增强了单义性，还使得更有效和细粒度的引导变得更少质量降级。我们的发现为测量和推进LLMs的机制可解释性和控制提供了可操作的指导方针。,The paper introduces a new metric and method to improve the interpretability and control of large language models by enhancing feature monosemanticity.,LLM,Harmless,"Mechanistic interpretability, controllability, feature extraction, monosemanticity, toxicity detection"
KunLunBaizeRAG: Reinforcement Learning Driven Inference Performance Leap for Large Language Models,"Cheng Li, Jiexiong Liu, Yixuan Chen, Qihang Zhou, KunLun Meta",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2506.19466.pdf,"This paper introduces KunLunBaizeRAG, a reinforcement learning-driven reasoning framework designed to enhance the reasoning capabilities of large language models (LLMs) in complex multi-hop question-answering tasks. The framework addresses key limitations of traditional RAG, such as retrieval drift, information redundancy, and strategy rigidity. Key innovations include the RAG-driven Reasoning Alignment (RDRA) mechanism, the Search-Think Iterative Enhancement (STIE) mechanism, the Network-Local Intelligent Routing (NLR) mechanism, and a progressive hybrid training strategy. Experimental results demonstrate significant improvements in exact match (EM) and LLM-judged score (LJ) across four benchmarks, highlighting the framework's robustness and effectiveness in complex reasoning scenarios.",这篇论文介绍了 KunLunBaizeRAG，一个基于强化学习的推理框架，旨在增强大型语言模型（LLMs）在复杂多跳问答任务中的推理能力。该框架解决了传统RAG的关键局限性，如检索漂移、信息冗余和策略僵化。关键创新包括RAG驱动的推理对齐（RDRA）机制、搜索-思考迭代增强（STIE）机制、网络本地智能路由（NLR）机制以及渐进式混合训练策略。实验结果在四个基准测试中显示出显著的精确匹配（EM）和LLM判定分数（LJ）的提高，突出了该框架在复杂推理场景中的鲁棒性和有效性。,"The paper presents KunLunBaizeRAG, a reinforcement learning-driven framework that improves the reasoning capabilities of LLMs in complex question-answering tasks through various innovative mechanisms.",LLM,Helpful,"Reinforcement Learning, Reasoning Alignment, Large Language Models, Multi-hop Question-Answering, RAG"
Can Large Language Models Capture Human Annotator Disagreements?,"Jingwei Ni, Yu Fan, Vil\'em Zouhar, Donya Rooein, Alexander Hoyle, Mrinmaya Sachan, Markus Leippold, Dirk Hovy, Elliott Ash",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2506.19467.pdf,"Human annotation variation (i.e., annotation disagreements) is common in NLP and often reflects important information such as task subjectivity and sample ambiguity. While Large Language Models (LLMs) are increasingly used for automatic annotation to reduce human effort, their evaluation often focuses on predicting the majority-voted ""ground truth"" labels. It is still unclear, however, whether these models also capture informative human annotation variation. Our work addresses this gap by extensively evaluating LLMs' ability to predict annotation disagreements without access to repeated human labels. Our results show that LLMs struggle with modeling disagreements, which can be overlooked by majority label-based evaluations. Notably, while RLVR-style (Reinforcement learning with verifiable rewards) reasoning generally boosts LLM performance, it degrades performance in disagreement prediction. Our findings highlight the critical need for evaluating and improving LLM annotators in disagreement modeling. Code and data at https://github.com/EdisonNi-hku/Disagreement_Prediction.",人类标注变异（即标注分歧）在NLP中很常见，通常反映了任务的主观性和样本的模糊性。虽然大型语言模型（LLMs）越来越多地用于自动标注以减少人力，但它们的评估通常集中在预测多数投票的“地面真实”标签。然而，尚不清楚这些模型是否也捕捉了有信息量的人类标注变异。我们的工作通过广泛评估LLMs在没有重复人类标签的情况下预测标注分歧的能力来填补这一空白。结果表明，LLMs在建模分歧方面存在困难，这可能被多数标签的评估所忽视。值得注意的是，虽然RLVR风格的推理通常提高了LLM的性能，但在分歧预测中却降低了性能。我们的发现强调了评估和改进LLM标注器在分歧建模中的关键需求。,The paper evaluates the ability of LLMs to capture human annotation disagreements and highlights the need for improved disagreement modeling.,LLM,"Helpful, Honest","Annotation Disagreements, LLM Evaluation, Disagreement Modeling, RLVR, Human Annotation"
Dialogic Pedagogy for Large Language Models: Aligning Conversational AI with Proven Theories of Learning,Russell Beale,2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2506.19484.pdf,"Large Language Models (LLMs) are rapidly transforming education by enabling rich conversational learning experiences. This article provides a comprehensive review of how LLM-based conversational agents are being used in higher education, with extensions to secondary and lifelong learning contexts. We synthesize existing literature on LLMs in education and theories of conversational and dialogic pedagogy - including Vygotsky's sociocultural learning (scaffolding and the Zone of Proximal Development), the Socratic method, and Laurillard's conversational framework - and examine how prompting strategies and retrieval-augmented generation (RAG) can align LLM behaviors with these pedagogical theories, and how it can support personalized, adaptive learning. We map educational theories to LLM capabilities, highlighting where LLM-driven dialogue supports established learning principles and where it challenges or falls short of traditional pedagogical assumptions. Notable gaps in applying prior theories to LLMs are identified, such as the models tendency to provide direct answers instead of fostering co-construction of knowledge, and the need to account for the constant availability and broad but non-human expertise of LLM tutors. In response, we propose practical strategies to better align LLM interactions with sound pedagogy - for example, designing prompts that encourage Socratic questioning, scaffolded guidance, and student reflection, as well as integrating retrieval mechanisms to ensure accuracy and contextual relevance. Our aim is to bridge the gap between educational theory and the emerging practice of AI-driven conversational learning, offering insights and tools for making LLM-based dialogues more educationally productive and theory-aligned.",大语言模型（LLM）通过启用丰富的对话式学习体验正在迅速改变教育。本文全面回顾了LLM基于的对话代理在高等教育中的使用情况，并扩展到中等和终身学习的背景。我们综合了有关LLM在教育中的现有文献和对话式及对话式教学理论，包括维果斯基的社会文化学习（脚手架和最近发展区）、苏格拉底方法和劳里拉德的对话框架，并研究了如何通过提示策略和检索增强生成（RAG）将LLM行为与这些教学理论对齐，以及如何支持个性化、适应性学习。我们将教育理论映射到LLM功能，突出了LLM驱动的对话如何支持已建立的学习原则以及它如何挑战或落后于传统的教学假设。识别了将先前的理论应用于LLM的显著差距，例如模型倾向于提供直接答案而不是促进知识的共同构建，以及需要考虑LLM教练的持续可用性和广泛但非人类专业知识。作为回应，我们提出了实用策略，以更好地将LLM交互与良好教学对齐，例如设计提示以鼓励苏格拉底式问题、脚手架指导和学生反思，以及集成检索机制以确保准确性和上下文相关性。我们的目的是弥合教育理论与AI驱动的对话学习的新兴实践之间的差距，提供见解和工具，使LLM基于的对话更具教育生产力和理论对齐。,The paper explores how to align Large Language Models with educational theories to enhance their helpfulness in learning contexts.,LLM,Helpful,"LLM, Alignment, Education, Pedagogy, Dialogue"
KnowMap: Efficient Knowledge-Driven Task Adaptation for LLMs,"Kelin Fu, Kaigui Bian",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2506.19527.pdf,"While Large Language Models (LLMs) possess significant capabilities in open-world agent tasks, they also face challenges in rapidly adapting to new, specialized tasks due to their reliance on static pre-trained knowledge. Traditional methods such as fine-tuning are often costly, data-intensive, and may lead to ""catastrophic forgetting."" Therefore, we present KnowMap, a novel approach that dynamically constructs a knowledge base from environmental and experiential data. KnowMap fine-tunes a small knowledge-embedding model to equip a larger LLM with valuable task-specific knowledge. Our experiments on the ScienceWorld benchmark demonstrate 17.71% improvement for the performance of gpt-4-turbo model. KnowMap not only provides an efficient and effective means for LLM task-adapting, but also highlights how integrating environmental and experiential knowledge can enhance LLMs' reasoning capabilities.",大语言模型（LLMs）在开放世界代理任务中具有显著的能力，但在快速适应新的专门任务方面面临挑战，因为它们依赖于静态预训练知识。传统方法如微调通常成本高、数据密集且可能导致“灾难性遗忘”。因此，我们提出了KnowMap，一种新颖的方法，它从环境和经验数据中动态构建知识库。KnowMap对一个小的知识嵌入模型进行微调，以使一个更大的LLM具备有价值的特定任务知识。我们在ScienceWorld基准上进行的实验表明，gpt-4-turbo模型的性能提高了17.71%。KnowMap不仅为LLM任务适应提供了一种高效和有效的方法，还突显了如何整合环境和经验知识可以增强LLMs的推理能力。,"The paper introduces KnowMap, a method for efficiently adapting LLMs to new tasks by dynamically constructing a knowledge base from environmental and experiential data.",LLM,Helpful,"LLM adaptation, task-specific knowledge, dynamic knowledge base, efficient fine-tuning, environmental and experiential data"
PrivacyXray: Detecting Privacy Breaches in LLMs through Semantic Consistency and Probability Certainty,"Jinwen He, Yiyang Lu, Zijin Lin, Kai Chen, Yue Zhao",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2506.19563.pdf,"Large Language Models (LLMs) are widely used in sensitive domains, including healthcare, finance, and legal services, raising concerns about potential private information leaks during inference. Privacy extraction attacks, such as jailbreaking, expose vulnerabilities in LLMs by crafting inputs that force the models to output sensitive information. However, these attacks cannot verify whether the extracted private information is accurate, as no public datasets exist for cross-validation, leaving a critical gap in private information detection during inference. To address this, we propose PrivacyXray, a novel framework detecting privacy breaches by analyzing LLM inner states. Our analysis reveals that LLMs exhibit higher semantic coherence and probabilistic certainty when generating correct private outputs. Based on this, PrivacyXray detects privacy breaches using four metrics: intra-layer and inter-layer semantic similarity, token-level and sentence-level probability distributions. PrivacyXray addresses critical challenges in private information detection by overcoming the lack of open-source private datasets and eliminating reliance on external data for validation. It achieves this through the synthesis of realistic private data and a detection mechanism based on the inner states of LLMs. Experiments show that PrivacyXray achieves consistent performance, with an average accuracy of 92.69% across five LLMs. Compared to state-of-the-art methods, PrivacyXray achieves significant improvements, with an average accuracy increase of 20.06%, highlighting its stability and practical utility in real-world applications.",大语言模型（LLMs）广泛应用于敏感领域，包括医疗、金融和法律服务，引发了在推理过程中潜在的私人信息泄露的担忧。隐私提取攻击，如越狱，通过设计输入来迫使模型输出敏感信息，暴露了LLMs的漏洞。然而，这些攻击无法验证提取的私人信息是否准确，因为没有公共数据集用于交叉验证，这在推理过程中检测私人信息留下了一个关键的空白。为了解决这个问题，我们提出了PrivacyXray，一种通过分析LLM内部状态来检测隐私泄露的新框架。我们的分析揭示了LLMs在生成正确的私人输出时表现出更高的语义一致性和概率确定性。基于此，PrivacyXray使用四个指标检测隐私泄露：内层和跨层语义相似性、标记级和句子级概率分布。通过合成现实的私人数据和基于LLM内部状态的检测机制，PrivacyXray克服了缺乏开源私人数据集和消除对外部数据进行验证的依赖的关键挑战。实验表明，PrivacyXray在五个LLMs中实现了稳定的性能，平均准确率为92.69%。与现有最先进的方法相比，PrivacyXray实现了显著的改进，平均准确率提高了20.06%，突出了其在实际应用中的稳定性和实用性。,"The paper introduces PrivacyXray, a framework for detecting privacy breaches in LLMs by analyzing their inner states and semantic consistency.",LLM,Harmless,"Privacy, LLMs, Detection, Semantic Consistency, Probability Certainty"
Correcting Hallucinations in News Summaries: Exploration of Self-Correcting LLM Methods with External Knowledge,"Juraj Vladika, Ihsan Soydemir, Florian Matthes",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2506.19607.pdf,"While large language models (LLMs) have shown remarkable capabilities to generate coherent text, they suffer from the issue of hallucinations -- factually inaccurate statements. Among numerous approaches to tackle hallucinations, especially promising are the self-correcting methods. They leverage the multi-turn nature of LLMs to iteratively generate verification questions inquiring additional evidence, answer them with internal or external knowledge, and use that to refine the original response with the new corrections. These methods have been explored for encyclopedic generation, but less so for domains like news summarization. In this work, we investigate two state-of-the-art self-correcting systems by applying them to correct hallucinated summaries using evidence from three search engines. We analyze the results and provide insights into systems' performance, revealing interesting practical findings on the benefits of search engine snippets and few-shot prompts, as well as high alignment of G-Eval and human evaluation.",大语言模型（LLM）虽然在生成连贯文本方面表现出色，但它们也存在幻觉问题——事实不准确的陈述。在众多应对幻觉的方法中，自纠正方法尤为有前途。它们利用LLM的多轮性质，迭代生成验证问题，询问额外的证据，并使用内部或外部知识来回答这些问题，然后用新的纠正来修改原始响应。这些方法已经在百科生成中得到了探索，但在新闻摘要等领域则较少。在本研究中，我们通过使用来自三个搜索引擎的证据来纠正幻觉摘要，研究了两种最新的自纠正系统。我们分析了结果，并提供了关于系统性能的见解，揭示了搜索引擎片段和少量示例提示的实际好处，以及G-Eval和人类评估的高度一致性。,The paper explores self-correcting methods to reduce hallucinations in news summaries generated by LLMs using external knowledge.,LLM,Harmless,"Hallucinations, Self-Correcting, News Summarization, External Knowledge, LLM"
Recurrent Visual Feature Extraction and Stereo Attentions for CT Report Generation,"Yuanhe Tian, Lei Mao, Yan Song",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2506.19665.pdf,"Generating reports for computed tomography (CT) images is a challenging task, while similar to existing studies for medical image report generation, yet has its unique characteristics, such as spatial encoding of multiple images, alignment between image volume and texts, etc. Existing solutions typically use general 2D or 3D image processing techniques to extract features from a CT volume, where they firstly compress the volume and then divide the compressed CT slices into patches for visual encoding. These approaches do not explicitly account for the transformations among CT slices, nor do they effectively integrate multi-level image features, particularly those containing specific organ lesions, to instruct CT report generation (CTRG). In considering the strong correlation among consecutive slices in CT scans, in this paper, we propose a large language model (LLM) based CTRG method with recurrent visual feature extraction and stereo attentions for hierarchical feature modeling. Specifically, we use a vision Transformer to recurrently process each slice in a CT volume, and employ a set of attentions over the encoded slices from different perspectives to selectively obtain important visual information and align them with textual features, so as to better instruct an LLM for CTRG. Experiment results and further analysis on the benchmark M3D-Cap dataset show that our method outperforms strong baseline models and achieves state-of-the-art results, demonstrating its validity and effectiveness.",为计算机断层扫描（CT）图像生成报告是一个具有挑战性的任务，尽管与现有的医学图像报告生成研究相似，但具有其独特的特征，例如多个图像的空间编码、图像体积与文本之间的对齐等。现有的解决方案通常使用通用的2D或3D图像处理技术从CT体积中提取特征，首先压缩体积，然后将压缩的CT切片分割成补丁进行视觉编码。这些方法没有明确考虑CT切片之间的变换，也没有有效地集成多级图像特征，特别是包含特定器官病变的特征，以指导CT报告生成（CTRG）。考虑到CT扫描中连续切片之间的强相关性，本文提出了一种基于大语言模型（LLM）的CTRG方法，具有递归视觉特征提取和立体注意力，用于分层特征建模。具体来说，我们使用视觉Transformer递归处理CT体积中的每个切片，并使用一组注意力机制从不同的角度对编码切片进行选择，以获取重要的视觉信息并将其与文本特征对齐，从而更好地指导LLM进行CTRG。实验结果和进一步的分析表明，我们的方法在基准M3D-Cap数据集上优于强大的基线模型，并实现了最先进的结果，证明了其有效性和有效性。,The paper presents a method using an LLM with recurrent visual feature extraction and stereo attentions for better alignment between CT images and textual reports.,LLM,Helpful,"CT report generation, LLM, visual feature extraction, stereo attentions, alignment"
Breaking Barriers: Do Reinforcement Post Training Gains Transfer To Unseen Domains?,"Chuxuan Hu, Yuxuan Zhu, Antony Kellermann, Caleb Biddulph, Suppakit Waiwitlikhit, Jason Benn, Daniel Kang",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2506.19733.pdf,"Reinforcement post training (RPT) has recently shown promise in improving the reasoning abilities of large language models (LLMs). However, it remains unclear how well these improvements generalize to new domains, as prior work evaluates RPT models on data from the same domains used for fine-tuning. To understand the generalizability of RPT, we conduct two studies. (1) Observational: We compare a wide range of open-weight RPT models against their corresponding base models across multiple domains, including both seen and unseen domains in their fine-tuning data. (2) Interventional: we fine-tune LLMs with RPT on single domains and evaluate their performance across multiple domains. Both studies converge on the same conclusion that, although RPT brings substantial gains on tasks similar to the fine-tuning data, the gains generalize inconsistently and can vanish on domains with different reasoning patterns.",强化后训练（RPT）最近在提高大型语言模型（LLMs）的推理能力方面表现出了很大的潜力。然而，尚不清楚这些改进如何适用于新领域，因为先前的工作在与微调数据相同的领域上评估了RPT模型。为了理解RPT的可推广性，我们进行了两项研究。 (1) 观察性：我们在多个领域中比较了广泛的开放权重RPT模型及其相应的基础模型，包括其微调数据中的已见和未见领域。 (2) 干预性：我们在单个领域上对LLMs进行RPT微调，并评估其在多个领域的性能。两项研究得出了相同的结论，即尽管RPT在与微调数据相似的任务中带来了显著的收益，但这些收益在具有不同推理模式的领域中不一致，并且可能在不同的领域中消失。,"The paper investigates how well reinforcement post training (RPT) improvements in large language models (LLMs) generalize to unseen domains, finding inconsistent transfer of gains.",LLM,Helpful,"Reinforcement post training, generalization, domain transfer, large language models, reasoning abilities"
Multi-Preference Lambda-weighted Listwise DPO for Dynamic Preference Alignment,"Yuhui Sun (University of Alberta), Xiyao Wang (University of Toronto), Zixi Li (Zhejiang University), Jinman Zhao (University of Toronto)",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2506.19780.pdf,"While large-scale unsupervised language models (LMs) capture broad world knowledge and reasoning capabilities, steering their behavior toward desired objectives remains challenging due to the lack of explicit supervision. Existing alignment techniques, such as reinforcement learning from human feedback (RLHF), rely on training a reward model and performing reinforcement learning to align with human preferences. However, RLHF is often computationally intensive, unstable, and sensitive to hyperparameters.
  To address these limitations, Direct Preference Optimization (DPO) was introduced as a lightweight and stable alternative, enabling direct alignment of language models with pairwise preference data via classification loss. However, DPO and its extensions generally assume a single static preference distribution, limiting flexibility in multi-objective or dynamic alignment settings.
  In this paper, we propose a novel framework: Multi-Preference Lambda-weighted Listwise DPO, which extends DPO to incorporate multiple human preference dimensions (e.g., helpfulness, harmlessness, informativeness) and enables dynamic interpolation through a controllable simplex-weighted formulation. Our method supports both listwise preference feedback and flexible alignment across varying user intents without re-training. Empirical and theoretical analysis demonstrates that our method is as effective as traditional DPO on static objectives while offering greater generality and adaptability for real-world deployment.",大规模无监督语言模型（LMs）捕捉了广泛的世界知识和推理能力，但将其行为引导到期望的目标仍然具有挑战性，因为缺乏明确的监督。现有的对齐技术，如基于人类反馈的强化学习（RLHF），依赖于训练奖励模型并执行强化学习以与人类偏好对齐。然而，RLHF通常计算密集、不稳定且对超参数敏感。为了解决这些局限性，引入了直接偏好优化（DPO）作为一种轻量级和稳定的替代方案，通过分类损失使语言模型与成对偏好数据直接对齐。然而，DPO及其扩展通常假设单个静态偏好分布，限制了多目标或动态对齐设置的灵活性。在本文中，我们提出了一种新的框架：多偏好Lambda加权列表DPO，将DPO扩展以纳入多个人类偏好维度（例如，有用性、无害性、信息性）并通过可控的单纯形加权公式实现动态插值。我们的方法支持列表偏好反馈和灵活对齐，以适应不同的用户意图而无需重新训练。实证和理论分析表明，我们的方法在静态目标上与传统DPO同样有效，但在实际部署中提供更大的一般性和适应性。,The paper introduces a novel framework for dynamic and multi-objective alignment of large language models with human preferences.,LLM,"Helpful, Harmless","Preference alignment, DPO, Multi-objective, Dynamic alignment, Language models"
LLM-Based Social Simulations Require a Boundary,"Zengqing Wu, Run Peng, Takayuki Ito, Chuan Xiao",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2506.19806.pdf,"This position paper argues that large language model (LLM)-based social simulations should establish clear boundaries to meaningfully contribute to social science research. While LLMs offer promising capabilities for modeling human-like agents compared to traditional agent-based modeling, they face fundamental limitations that constrain their reliability for social pattern discovery. The core issue lies in LLMs' tendency towards an ``average persona'' that lacks sufficient behavioral heterogeneity, a critical requirement for simulating complex social dynamics. We examine three key boundary problems: alignment (simulated behaviors matching real-world patterns), consistency (maintaining coherent agent behavior over time), and robustness (reproducibility under varying conditions). We propose heuristic boundaries for determining when LLM-based simulations can reliably advance social science understanding. We believe that these simulations are more valuable when focusing on (1) collective patterns rather than individual trajectories, (2) agent behaviors aligning with real population averages despite limited variance, and (3) proper validation methods available for testing simulation robustness. We provide a practical checklist to guide researchers in determining the appropriate scope and claims for LLM-based social simulations.",这篇论文论述了大型语言模型（LLM）基于的社会模拟应当建立明确的边界，以有意义地为社会科学研究做出贡献。虽然LLM在模拟人类行为方面表现出了前所未有的能力，但它们在可靠性方面存在根本性的局限，这限制了它们在发现社会模式方面的可靠性。核心问题在于LLM倾向于一个“平均人格”，缺乏足够的行为异质性，这是模拟复杂社会动态的关键要求。我们研究了三个关键边界问题：对齐（模拟行为与现实世界模式匹配）、一致性（在时间上保持一致的代理行为）和鲁棒性（在不同条件下的可重复性）。我们提出了启发式边界，以确定LLM基于的模拟何时可以可靠地推进社会科学理解。我们认为这些模拟在（1）集体模式而不是个体轨迹，（2）代理行为与实际人口平均值对齐，尽管变异有限，（3）可用于测试模拟鲁棒性的适当验证方法时更有价值。我们提供了一个实用的检查清单，以指导研究人员确定LLM基于的模拟的适当范围和声明。,The paper argues for establishing clear boundaries for LLM-based social simulations to ensure alignment with real-world behaviors and improve their reliability in social science research.,LLM,Helpful,"LLM alignment, social simulations, boundary problems, behavioral heterogeneity, simulation robustness"
KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality,"Baochang Ren, Shuofei Qiao, Wenhao Yu, Huajun Chen, Ningyu Zhang",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2506.19807.pdf,"Large Language Models (LLMs), particularly slow-thinking models, often exhibit severe hallucination, outputting incorrect content due to an inability to accurately recognize knowledge boundaries during reasoning. While Reinforcement Learning (RL) can enhance complex reasoning abilities, its outcome-oriented reward mechanism often lacks factual supervision over the thinking process, further exacerbating the hallucination problem. To address the high hallucination in slow-thinking models, we propose Knowledge-enhanced RL, KnowRL. KnowRL guides models to perform fact-based slow thinking by integrating a factuality reward, based on knowledge verification, into the RL training process, helping them recognize their knowledge boundaries. KnowRL guides models to perform fact-based slow thinking by integrating a factuality reward, based on knowledge verification, into the RL training process, helping them recognize their knowledge boundaries. This targeted factual input during RL training enables the model to learn and internalize fact-based reasoning strategies. By directly rewarding adherence to facts within the reasoning steps, KnowRL fosters a more reliable thinking process. Experimental results on three hallucination evaluation datasets and two reasoning evaluation datasets demonstrate that KnowRL effectively mitigates hallucinations in slow-thinking models while maintaining their original strong reasoning capabilities. Our code is available at https://github.com/zjunlp/KnowRL.",大语言模型（LLMs），特别是慢思考模型，往往表现出严重的幻觉，输出不正确的内容，因为在推理过程中无法准确识别知识边界。虽然强化学习（RL）可以增强复杂推理能力，但其以结果为导向的奖励机制通常缺乏对思维过程的事实监督，进一步加剧了幻觉问题。为了解决慢思考模型中的高幻觉问题，我们提出了知识增强的RL，即KnowRL。KnowRL通过将基于知识验证的事实性奖励集成到RL训练过程中，指导模型执行基于事实的慢思考，帮助它们识别自己的知识边界。在RL训练过程中的这种有针对性的事实输入使模型能够学习和内化基于事实的推理策略。通过在推理步骤中直接奖励事实的遵循，KnowRL促进了更可靠的思维过程。在三个幻觉评估数据集和两个推理评估数据集上的实验结果表明，KnowRL有效地缓解了慢思考模型中的幻觉，同时保持了它们原有的强大推理能力。我们的代码可在https://github.com/zjunlp/KnowRL上获得。,"The paper introduces KnowRL, a method that integrates factuality rewards into reinforcement learning to reduce hallucinations and improve the reliability of large language models.",LLM,"Helpful, Honest","Hallucination, Factuality, Reinforcement Learning, Knowledge Verification, Large Language Models"
Persona Features Control Emergent Misalignment,"Miles Wang, Tom Dupr\'e la Tour, Olivia Watkins, Alex Makelov, Ryan A. Chi, Samuel Miserendino, Johannes Heidecke, Tejal Patwardhan, Dan Mossing",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2506.19823.pdf,"Understanding how language models generalize behaviors from their training to a broader deployment distribution is an important problem in AI safety. Betley et al. discovered that fine-tuning GPT-4o on intentionally insecure code causes ""emergent misalignment,"" where models give stereotypically malicious responses to unrelated prompts. We extend this work, demonstrating emergent misalignment across diverse conditions, including reinforcement learning on reasoning models, fine-tuning on various synthetic datasets, and in models without safety training. To investigate the mechanisms behind this generalized misalignment, we apply a ""model diffing"" approach using sparse autoencoders to compare internal model representations before and after fine-tuning. This approach reveals several ""misaligned persona"" features in activation space, including a toxic persona feature which most strongly controls emergent misalignment and can be used to predict whether a model will exhibit such behavior. Additionally, we investigate mitigation strategies, discovering that fine-tuning an emergently misaligned model on just a few hundred benign samples efficiently restores alignment.",理解语言模型如何从其训练中推广行为到更广泛的部署分布是人工智能安全中的一个重要问题。Betley等人发现，在GPT-4o上进行不安全代码的微调会导致“突发不一致”，其中模型对不相关提示给出典型的恶意响应。我们扩展了这一工作，展示了在多种条件下的突发不一致，包括强化学习在推理模型上、在各种合成数据集上进行微调以及在没有安全训练的模型中。为了研究这种广泛不一致背后的机制，我们采用了“模型差异”方法，使用稀疏自编码器比较微调前后的内部模型表示。这种方法揭示了激活空间中的几个“不一致人格”特征，包括最强控制突发不一致的有毒人格特征，可以用来预测模型是否会表现出这种行为。此外，我们还研究了缓解策略，发现在仅几百个良性样本上对突发不一致的模型进行微调可以有效地恢复一致性。,"The paper explores the causes and mitigation of emergent misalignment in language models, focusing on harmful behaviors.",LLM,Harmless,"Emergent misalignment, persona features, toxic behavior, model diffing, mitigation strategies"
JoyAgents-R1: Joint Evolution Dynamics for Versatile Multi-LLM Agents with Reinforcement Learning,"Ai Han, Junxing Hu, Pu Wei, Zhiqian Zhang, Yuhang Guo, Jiawei Lu, Zicheng Zhang",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2506.19846.pdf,"Multi-agent reinforcement learning (MARL) has emerged as a prominent paradigm for increasingly complex tasks. However, joint evolution across heterogeneous agents remains challenging due to cooperative inefficiency and training instability. In this paper, we propose the joint evolution dynamics for MARL called JoyAgents-R1, which first applies Group Relative Policy Optimization (GRPO) to the joint training of heterogeneous multi-agents. By iteratively refining agents' large language models (LLMs) and memories, the method achieves holistic equilibrium with optimal decision-making and memory capabilities. Specifically, JoyAgents-R1 first implements node-wise Monte Carlo sampling on the behavior of each agent across entire reasoning trajectories to enhance GRPO sampling efficiency while maintaining policy diversity. Then, our marginal benefit-driven selection strategy identifies top-$K$ sampling groups with maximal reward fluctuations, enabling targeted agent model updates that improve training stability and maximize joint benefits through cost-effective parameter adjustments. Meanwhile, JoyAgents-R1 introduces an adaptive memory evolution mechanism that repurposes GRPO rewards as cost-free supervisory signals to eliminate repetitive reasoning and accelerate convergence. Experiments across general and domain-specific scenarios demonstrate that JoyAgents-R1 achieves performance comparable to that of larger LLMs while built on smaller open-source models.",多智能体强化学习（MARL）作为处理日益复杂任务的显著范式，但异构智能体之间的联合进化仍然具有挑战性，原因是合作效率低和训练不稳定。本文提出了MARL的联合进化动力学，称为JoyAgents-R1，首先将组相对策略优化（GRPO）应用于异构多智能体的联合训练。通过迭代精炼智能体的大型语言模型（LLM）和记忆，该方法实现了具有最佳决策能力和记忆能力的全面平衡。具体来说，JoyAgents-R1首先在每个智能体的整个推理轨迹上实现节点级蒙特卡罗采样，以增强GRPO采样效率，同时保持策略多样性。然后，我们的边际收益驱动的选择策略识别出具有最大奖励波动的前K个采样组，使得有针对性的智能体模型更新，从而提高训练稳定性，并通过成本有效的参数调整最大化联合收益。与此同时，JoyAgents-R1引入了自适应记忆进化机制，将GRPO奖励重新用作无成本的监督信号，以消除重复推理并加速收敛。在一般和特定领域的场景中进行的实验表明，JoyAgents-R1在构建较小的开源模型的基础上，实现了与较大LLM相媲美的性能。,"The paper introduces JoyAgents-R1, a method for joint evolution dynamics in MARL that refines LLMs and memories for optimal decision-making and memory capabilities.",LLM,Helpful,"Multi-agent reinforcement learning, LLM, Joint evolution dynamics, GRPO, Memory evolution"
Privacy-Preserving LLM Interaction with Socratic Chain-of-Thought Reasoning and Homomorphically Encrypted Vector Databases,"Yubeen Bae, Minchan Kim, Jaejin Lee, Sangbum Kim, Jaehyung Kim, Yejin Choi, Niloofar Mireshghallah",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2506.17336.pdf,"Large language models (LLMs) are increasingly used as personal agents, accessing sensitive user data such as calendars, emails, and medical records. Users currently face a trade-off: They can send private records, many of which are stored in remote databases, to powerful but untrusted LLM providers, increasing their exposure risk. Alternatively, they can run less powerful models locally on trusted devices. We bridge this gap. Our Socratic Chain-of-Thought Reasoning first sends a generic, non-private user query to a powerful, untrusted LLM, which generates a Chain-of-Thought (CoT) prompt and detailed sub-queries without accessing user data. Next, we embed these sub-queries and perform encrypted sub-second semantic search using our Homomorphically Encrypted Vector Database across one million entries of a single user's private data. This represents a realistic scale of personal documents, emails, and records accumulated over years of digital activity. Finally, we feed the CoT prompt and the decrypted records to a local language model and generate the final response. On the LoCoMo long-context QA benchmark, our hybrid framework, combining GPT-4o with a local Llama-3.2-1B model, outperforms using GPT-4o alone by up to 7.1 percentage points. This demonstrates a first step toward systems where tasks are decomposed and split between untrusted strong LLMs and weak local ones, preserving user privacy.",大语言模型（LLMs）越来越多地被用作个人代理，访问用户的敏感数据，如日历、电子邮件和医疗记录。用户目前面临一个权衡：他们可以将私人记录发送给强大但不可信的LLM提供商，增加其暴露风险，或者在可信设备上运行功能较弱的模型。我们弥合了这一差距。我们的苏格拉底式推理链首先将一个通用的、非私人用户查询发送给一个强大的、不可信的LLM，该LLM生成一个推理链（CoT）提示和详细的子查询，而不访问用户数据。接下来，我们将这些子查询嵌入并使用我们的同态加密向量数据库进行加密的次秒语义搜索，跨越单个用户的私人数据的一百万条条目。这代表了数字活动多年积累的个人文档、电子邮件和记录的现实规模。最后，我们将CoT提示和解密的记录提供给本地语言模型，并生成最终响应。在LoCoMo长上下文QA基准上，我们的混合框架，结合GPT-4o与本地Llama-3.2-1B模型，比单独使用GPT-4o提高了多达7.1个百分点。这表明了任务分解和分割在不可信的强大LLM和弱本地LLM之间的第一步，保护用户隐私。,The paper presents a method for interacting with LLMs while preserving user privacy by using a combination of Socratic Chain-of-Thought Reasoning and homomorphically encrypted vector databases.,LLM,Helpful,"Privacy, LLM, Chain-of-Thought, Homomorphic Encryption, User Data"
Evaluating Transparent Reasoning in Large Language Models for Accountable Critical Tasks,"Junhao Chen, Bowen Wang, Jiuyang Chang, Yuta Nakashima",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2408.01933.pdf,"This paper introduces REACT, a benchmark designed to rigorously evaluate the reasoning capabilities of large language models (LLMs) within accountable, high-stakes decision-making tasks in medical and legal domains. Unlike traditional benchmarks primarily focused on prediction accuracy, REACT emphasizes transparent and interpretable reasoning, requiring models to align their logic closely with expert-derived procedures. To assess whether LLM reasoning aligns closely with human experts, we annotated 511 clinical cases from the medical domain and 86 legal cases from the legal domain, each enriched with detailed expert-extracted rationales and evidence supporting each step of the reasoning process. These annotations were guided by carefully constructed reasoning graphs, which explicitly encode domain-specific inference structures and decision criteria derived by domain experts. These reasoning graphs serve not only as standards for expert annotation but also as structured guidelines enabling models to reason transparently and step-by-step. To address the scalability challenges of manual annotation, we further developed a semi-automatic annotation pipeline leveraging expert-defined reasoning graph templates to efficiently generate new graphs, exploring the potential to extend our approach into additional critical domains. Experimental results demonstrate that reasoning graphs substantially enhance the interpretability and accuracy of LLM reasoning compared to traditional baselines, although significant gaps remain relative to expert-level reasoning performance.",这篇论文介绍了REACT，一个旨在严格评估大型语言模型（LLM）在医疗和法律等高风险决策任务中的推理能力的基准。与主要关注预测准确性的传统基准不同，REACT强调透明和可解释的推理，要求模型将其逻辑与专家推导的程序紧密对齐。为了评估LLM推理是否与人类专家紧密对齐，我们对医疗领域的511个临床病例和法律领域的86个法律病例进行了注释，每个病例都附有详细的专家提取的理由和支持每个推理步骤的证据。这些注释是由仔细构建的推理图引导的，这些推理图明确编码了由领域专家推导的特定于领域的推理结构和决策标准。这些推理图不仅作为专家注释的标准，还作为结构化指南，使模型能够透明且逐步推理。为了解决手动注释的可扩展性挑战，我们进一步开发了一个半自动注释管道，利用专家定义的推理图模板高效生成新图，探索将我们的方法扩展到其他关键领域的潜力。实验结果表明，推理图显著增强了LLM推理的可解释性和准确性，尽管相对于专家级推理性能仍存在显著差距。,"The paper presents REACT, a benchmark for evaluating the transparent and interpretable reasoning of LLMs in high-stakes medical and legal tasks, aligning model logic with expert-derived procedures.",LLM,"Helpful, Honest","Reasoning, Alignment, Transparency, Medical, Legal"
ADVLLM: Iterative Self-Tuning LLMs for Enhanced Jailbreaking Capabilities,"Chung-En Sun, Xiaodong Liu, Weiwei Yang, Tsui-Wei Weng, Hao Cheng, Aidan San, Michel Galley, Jianfeng Gao",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2410.18469.pdf,"Recent research has shown that Large Language Models (LLMs) are vulnerable to automated jailbreak attacks, where adversarial suffixes crafted by algorithms appended to harmful queries bypass safety alignment and trigger unintended responses. Current methods for generating these suffixes are computationally expensive and have low Attack Success Rates (ASR), especially against well-aligned models like Llama2 and Llama3. To overcome these limitations, we introduce ADV-LLM, an iterative self-tuning process that crafts adversarial LLMs with enhanced jailbreak ability. Our framework significantly reduces the computational cost of generating adversarial suffixes while achieving nearly 100\% ASR on various open-source LLMs. Moreover, it exhibits strong attack transferability to closed-source models, achieving 99\% ASR on GPT-3.5 and 49\% ASR on GPT-4, despite being optimized solely on Llama3. Beyond improving jailbreak ability, ADV-LLM provides valuable insights for future safety alignment research through its ability to generate large datasets for studying LLM safety.",最近的研究表明，大型语言模型（LLMs）容易受到自动化越狱攻击的影响，其中由算法制作的对抗性后缀附加到有害查询上，绕过安全对齐并触发意外响应。目前生成这些后缀的方法计算成本高，攻击成功率（ASR）低，特别是针对像Llama2和Llama3这样的良好对齐模型。为了克服这些局限性，我们引入了ADV-LLM，一个迭代自调整过程，制作具有增强越狱能力的对抗性LLMs。我们的框架显著降低了生成对抗性后缀的计算成本，同时在各种开源LLMs上实现了几乎100%的ASR。此外，它在闭源模型上表现出强大的攻击转移能力，在GPT-3.5上实现了99%的ASR，在GPT-4上实现了49%的ASR，尽管仅在Llama3上进行了优化。除了提高越狱能力，ADV-LLM还通过其生成大型数据集来研究LLM安全性，为未来的安全对齐研究提供了宝贵的见解。,"The paper introduces ADV-LLM, an iterative self-tuning process to enhance jailbreaking capabilities in LLMs, providing insights for future safety alignment research.",LLM,Harmless,"Jailbreaking, Safety Alignment, Adversarial Suffixes, Attack Success Rates, LLMs"
"""I know myself better, but not really greatly"": How Well Can LLMs Detect and Explain LLM-Generated Texts?","Jiazhou Ji, Jie Guo, Weidong Qiu, Zheng Huang, Yang Xu, Xinru Lu, Xiaoyu Jiang, Ruizhe Li, Shujun Li",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2502.12743.pdf,"Distinguishing between human- and LLM-generated texts is crucial given the risks associated with misuse of LLMs. This paper investigates detection and explanation capabilities of current LLMs across two settings: binary (human vs. LLM-generated) and ternary classification (including an ``undecided'' class). We evaluate 6 close- and open-source LLMs of varying sizes and find that self-detection (LLMs identifying their own outputs) consistently outperforms cross-detection (identifying outputs from other LLMs), though both remain suboptimal. Introducing a ternary classification framework improves both detection accuracy and explanation quality across all models. Through comprehensive quantitative and qualitative analyses using our human-annotated dataset, we identify key explanation failures, primarily reliance on inaccurate features, hallucinations, and flawed reasoning. Our findings underscore the limitations of current LLMs in self-detection and self-explanation, highlighting the need for further research to address overfitting and enhance generalizability.",将人类生成的文本与LLM生成的文本区分开来是至关重要的，因为这与LLM的滥用风险有关。本文研究了当前LLM在两种设置下的检测和解释能力：二元分类（人类与LLM生成）和三元分类（包括“未决”类）。我们评估了6个大小不同的开源和闭源LLM，发现自检测（LLM识别自己的输出）一致优于交叉检测（识别其他LLM的输出），尽管两者都不理想。引入三元分类框架提高了所有模型的检测准确性和解释质量。通过对我们的人工标注数据集进行全面的定量和定性分析，我们识别出关键解释失败，主要是依赖不准确的特征、幻觉和错误的推理。我们的发现强调了当前LLM在自检测和自解释方面的局限性，突出了进一步研究以解决过拟合问题并增强可推广性的需要。,"The paper evaluates the ability of LLMs to detect and explain their own generated text, finding that while self-detection outperforms cross-detection, both are suboptimal and highlight the need for further research.",LLM,Honest,"LLM detection, self-explanation, text classification, model evaluation, explanation failures"
Defeating Prompt Injections by Design,"Edoardo Debenedetti, Ilia Shumailov, Tianqi Fan, Jamie Hayes, Nicholas Carlini, Daniel Fabian, Christoph Kern, Chongyang Shi, Andreas Terzis, Florian Tram\`er",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2503.18813.pdf,"Large Language Models (LLMs) are increasingly deployed in agentic systems that interact with an untrusted environment. However, LLM agents are vulnerable to prompt injection attacks when handling untrusted data. In this paper we propose CaMeL, a robust defense that creates a protective system layer around the LLM, securing it even when underlying models are susceptible to attacks. To operate, CaMeL explicitly extracts the control and data flows from the (trusted) query; therefore, the untrusted data retrieved by the LLM can never impact the program flow. To further improve security, CaMeL uses a notion of a capability to prevent the exfiltration of private data over unauthorized data flows by enforcing security policies when tools are called. We demonstrate effectiveness of CaMeL by solving $77\%$ of tasks with provable security (compared to $84\%$ with an undefended system) in AgentDojo. We release CaMeL at https://github.com/google-research/camel-prompt-injection.",大语言模型（LLMs）越来越多地部署在与不可信环境交互的代理系统中。然而，当处理不可信数据时，LLM代理容易受到提示注入攻击。在本文中，我们提出了CaMeL，一种创建保护系统层以保护LLM的强大防御机制，即使在基础模型易受攻击时也能保护它。为了运行，CaMeL明确从（可信的）查询中提取控制和数据流；因此，LLM检索的不可信数据永远不会影响程序流。为了进一步提高安全性，CaMeL使用一种能力的概念，通过在调用工具时强制执行安全策略，防止通过未经授权的数据流泄露私人数据。我们通过在AgentDojo中解决77%的任务（与未受保护的系统相比，84%），证明了CaMeL的有效性。,"The paper introduces CaMeL, a defense mechanism to protect LLMs from prompt injection attacks by securing control and data flows.",LLM,Harmless,"Prompt Injection, Security, Defense Mechanism, LLM, Agentic Systems"
Analyzing LLMs' Knowledge Boundary Cognition Across Languages Through the Lens of Internal Representations,"Chenghao Xiao, Hou Pong Chan, Hao Zhang, Mahani Aljunied, Lidong Bing, Noura Al Moubayed, Yu Rong",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2504.13816.pdf,"While understanding the knowledge boundaries of LLMs is crucial to prevent hallucination, research on the knowledge boundaries of LLMs has predominantly focused on English. In this work, we present the first study to analyze how LLMs recognize knowledge boundaries across different languages by probing their internal representations when processing known and unknown questions in multiple languages. Our empirical studies reveal three key findings: 1) LLMs' perceptions of knowledge boundaries are encoded in the middle to middle-upper layers across different languages. 2) Language differences in knowledge boundary perception follow a linear structure, which motivates our proposal of a training-free alignment method that effectively transfers knowledge boundary perception ability across languages, thereby helping reduce hallucination risk in low-resource languages; 3) Fine-tuning on bilingual question pair translation further enhances LLMs' recognition of knowledge boundaries across languages. Given the absence of standard testbeds for cross-lingual knowledge boundary analysis, we construct a multilingual evaluation suite comprising three representative types of knowledge boundary data. Our code and datasets are publicly available at https://github.com/DAMO-NLP-SG/LLM-Multilingual-Knowledge-Boundaries.",尽管理解大型语言模型（LLMs）的知识边界对于防止幻觉至关重要，但关于LLMs知识边界的研究主要集中在英语上。在本研究中，我们首次分析了LLMs在处理多种语言中的已知和未知问题时，如何识别知识边界，通过探测其内部表示。我们的实证研究揭示了三个关键发现：1）LLMs的知识边界感知在不同语言中编码在中间到中间上层；2）语言差异在知识边界感知中呈线性结构，这激发了我们提出一种无需训练的对齐方法，该方法有效地将知识边界感知能力跨语言传输，从而帮助减少低资源语言中的幻觉风险；3）在双语问题对翻译上进行微调进一步增强了LLMs在多语言中的知识边界识别能力。鉴于跨语言知识边界分析的标准测试基准的缺失，我们构建了一个多语言评估套件，包括三种代表性的知识边界数据类型。我们的代码和数据集可在https://github.com/DAMO-NLP-SG/LLM-Multilingual-Knowledge-Boundaries上公开获取。,The paper analyzes how LLMs recognize knowledge boundaries across different languages and proposes a training-free alignment method to reduce hallucination risk.,LLM,Harmless,"Knowledge boundaries, multilingual, hallucination, alignment, internal representations"
Process Reward Models That Think,"Muhammad Khalifa, Rishabh Agarwal, Lajanugen Logeswaran, Jaekyeom Kim, Hao Peng, Moontae Lee, Honglak Lee, Lu Wang",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2504.16828.pdf,"Step-by-step verifiers -- also known as process reward models (PRMs) -- are a key ingredient for test-time scaling. PRMs require step-level supervision, making them expensive to train. This work aims to build data-efficient PRMs as verbalized step-wise reward models that verify every step in the solution by generating a verification chain-of-thought (CoT). We propose ThinkPRM, a long CoT verifier fine-tuned on orders of magnitude fewer process labels than those required by discriminative PRMs. Our approach capitalizes on the inherent reasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and discriminative verifiers -- using only 1% of the process labels in PRM800K -- across several challenging benchmarks. Specifically, ThinkPRM beats the baselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and reward-guided search. In an out-of-domain evaluation on a subset of GPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers trained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the same token budget, ThinkPRM scales up verification compute more effectively compared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of ProcessBench. Our work highlights the value of generative, long CoT PRMs that can scale test-time compute for verification while requiring minimal supervision for training. Our code, data, and models will be released at https://github.com/mukhal/thinkprm.",逐步验证器（也称为过程奖励模型（PRMs））是测试时扩展的关键成分。PRMs 需要步骤级别的监督，使其训练成本高昂。本文旨在构建数据高效的 PRMs 作为语言化逐步奖励模型，通过生成验证链式思维（CoT）来验证解决方案中的每一步。我们提出了 ThinkPRM，这是一个长 CoT 验证器，经过少量过程标签的微调，远少于判别性 PRMs 所需的标签。我们的方法利用了长 CoT 模型的内在推理能力，并在使用 PRM800K 的 1% 过程标签的情况下，在几个具有挑战性的基准测试中超越了 LLM-as-a-Judge 和判别性验证器。具体来说，ThinkPRM 在 ProcessBench、MATH-500 和 AIME '24 下的最佳 N 选择和奖励引导搜索中击败了基线。在 GPQA-Diamond 和 LiveCodeBench 的跨领域评估中，我们的 PRM 在训练全 PRM800K 的判别性验证器的基础上分别提高了 8% 和 4.5%。最后，在相同的令牌预算下，ThinkPRM 更有效地扩展了验证计算，在 ProcessBench 的子集上超过了 LLM-as-a-Judge 7.2%。我们的工作突显了生成性、长 CoT PRMs 的价值，它们可以在训练时需要最少监督的情况下扩展测试时的验证计算。我们的代码、数据和模型将在 https://github.com/mukhal/thinkprm 上发布。,"The paper introduces ThinkPRM, a data-efficient process reward model that uses long chain-of-thought verification to outperform baselines in various benchmarks with minimal supervision.",LLM,"Helpful, Honest","Process Reward Models, Verification, Chain-of-Thought, Data Efficiency, Scalability"
Disentangling Reasoning and Knowledge in Medical Large Language Models,"Rahul Thapa, Qingyang Wu, Kevin Wu, Harrison Zhang, Angela Zhang, Eric Wu, Haotian Ye, Suhana Bedi, Nevin Aresh, Joseph Boen, Shriya Reddy, Ben Athiwaratkun, Shuaiwen Leon Song, James Zou",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2505.11462.pdf,"Medical reasoning in large language models (LLMs) aims to emulate clinicians' diagnostic thinking, but current benchmarks such as MedQA-USMLE, MedMCQA, and PubMedQA often mix reasoning with factual recall. We address this by separating 11 biomedical QA benchmarks into reasoning- and knowledge-focused subsets using a PubMedBERT classifier that reaches 81 percent accuracy, comparable to human performance. Our analysis shows that only 32.8 percent of questions require complex reasoning. We evaluate biomedical models (HuatuoGPT-o1, MedReason, m1) and general-domain models (DeepSeek-R1, o4-mini, Qwen3), finding consistent gaps between knowledge and reasoning performance. For example, HuatuoGPT-o1 scores 56.9 on knowledge but only 44.8 on reasoning. In adversarial tests where models are misled with incorrect initial reasoning, biomedical models degrade sharply, while larger or RL-trained general models show more robustness. To address this, we train BioMed-R1 using fine-tuning and reinforcement learning on reasoning-heavy examples. It achieves the strongest performance among similarly sized models. Further gains may come from incorporating clinical case reports and training with adversarial and backtracking scenarios.",医学大语言模型（LLMs）的医学推理旨在模仿临床医生的诊断思维，但当前的基准测试，如MedQA-USMLE、MedMCQA和PubMedQA，往往将推理与事实回忆混淆。我们通过使用PubMedBERT分类器将11个生物医学QA基准测试分为推理和知识导向的子集，分类器的准确率达到81%，与人类表现相当。我们的分析表明，只有32.8%的问题需要复杂的推理。我们评估了生物医学模型（HuatuoGPT-o1、MedReason、m1）和通用领域模型（DeepSeek-R1、o4-mini、Qwen3），发现知识和推理性能之间存在一致的差距。例如，HuatuoGPT-o1在知识方面得分为56.9，但在推理方面仅为44.8。在模型被错误的初始推理误导的对抗性测试中，生物医学模型表现急剧下降，而较大或通过强化学习训练的通用模型表现出更大的鲁棒性。为了解决这个问题，我们通过在推理密集型示例上进行微调和强化学习来训练BioMed-R1。它在相似大小的模型中表现最佳。进一步的收益可能来自于纳入临床病例报告和在对抗性和回溯场景中进行训练。,The paper evaluates and improves the reasoning abilities of medical large language models by separating reasoning and knowledge tasks and using reinforcement learning.,LLM,Helpful,"Medical LLMs, Reasoning, Knowledge, Alignment, Evaluation"
RAG+: Enhancing Retrieval-Augmented Generation with Application-Aware Reasoning,"Yu Wang, Shiwan Zhao, Zhihu Wang, Yubo Zhang, Xicheng Zhang, Zhengfan Wang, Heyuan Huang, Ming Fan, Ting Liu",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2506.11555.pdf,"The integration of external knowledge through Retrieval-Augmented Generation (RAG) has become foundational in enhancing large language models (LLMs) for knowledge-intensive tasks. However, existing RAG paradigms often overlook the cognitive step of applying knowledge, leaving a gap between retrieved facts and task-specific reasoning. In this work, we introduce RAG+, a principled and modular extension that explicitly incorporates application-aware reasoning into the RAG pipeline. RAG+ constructs a dual corpus consisting of knowledge and aligned application examples, created either manually or automatically, and retrieves both jointly during inference. This design enables LLMs not only to access relevant information but also to apply it within structured, goal-oriented reasoning processes. Experiments across mathematical, legal, and medical domains, conducted on multiple models, demonstrate that RAG+ consistently outperforms standard RAG variants, achieving average improvements of 3-5%, and peak gains up to 7.5% in complex scenarios. By bridging retrieval with actionable application, RAG+ advances a more cognitively grounded framework for knowledge integration, representing a step toward more interpretable and capable LLMs.",通过检索增强生成（RAG）的外部知识集成已经成为增强大型语言模型（LLMs）以应对知识密集型任务的基础。然而，现有的RAG范式往往忽略了应用知识的认知步骤，在检索事实与特定任务的推理之间留下了一个差距。在本文中，我们引入了RAG+，这是一个有原则且模块化的扩展，它在RAG管道中明确地纳入了应用感知推理。RAG+构建了一个由知识和对齐应用示例组成的双重语料库，这些示例可以手动或自动创建，并在推理过程中联合检索。这种设计不仅使LLMs能够访问相关信息，还能够在结构化、目标导向的推理过程中应用它。在数学、法律和医学领域进行的多个模型的实验表明，RAG+在复杂情景中能够显著提高标准RAG变体的性能，平均提高3-5%，最高提高7.5%。通过将检索与可操作的应用结合起来，RAG+推动了一个更具认知基础的知识集成框架，代表了朝着更可解释和更有能力的LLMs迈出的一步。,"The paper introduces RAG+, an extension of Retrieval-Augmented Generation that enhances LLMs by incorporating application-aware reasoning, improving performance in knowledge-intensive tasks.",LLM,Helpful,"Retrieval-Augmented Generation, Application-Aware Reasoning, Knowledge Integration, Large Language Models, Task-Specific Reasoning"
KAG-Thinker: Interactive Thinking and Deep Reasoning in LLMs via Knowledge-Augmented Generation,"Dalong Zhang, Jun Xu, Jun Zhou, Lei Liang, Lin Yuan, Ling Zhong, Mengshu Sun, Peilong Zhao, QiWei Wang, Xiaorui Wang, Xinkai Du, YangYang Hou, Yu Ao, ZhaoYang Wang, Zhengke Gui, ZhiYing Yi, Zhongpu Bo",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2506.17728.pdf,"In this paper, we introduce KAG-Thinker, which upgrade KAG to a multi-turn interactive thinking and deep reasoning framework powered by a dedicated parameter-light large language model (LLM). Our approach constructs a structured thinking process for solving complex problems, enhancing the the logical coherence and contextual consistency of the reasoning process in question-answering (Q&A) tasks on domain-specific knowledge bases (KBs) within LLMs. Following the \textbf{Logical Form} guided retrieval and reasoning technology route of KAG, this framework first decomposes complex questions into independently solvable sub-problems (which are also referred to as logical forms) through \textbf{breadth decomposition}. Each such logical form is represented in two equivalent forms-natural language and logical function-and subsequently classified as either a Knowledge Retrieval or Reasoning Analysis task. Dependencies and parameter passing between these tasks are explicitly modeled via logical function interfaces. In the solving process, the Retrieval function performs retrieval tasks. It retrieves one-hop structured and unstructured information of specified knowledge unit. While the Math and Deduce functions are used to perform reasoning analysis tasks. Secondly, it is worth noting that, in the Knowledge Retrieval sub-problem tasks, LLMs and external knowledge sources are regarded as equivalent KBs. We use the \textbf{knowledge boundary} module to determine the optimal source using self-regulatory mechanisms such as confidence calibration and reflective reasoning, and use the \textbf{depth solving} module to enhance the comprehensiveness of knowledge acquisition...",在这篇论文中，我们介绍了 KAG-Thinker，它将 KAG 升级为一个由专用参数轻量级大型语言模型（LLM）驱动的多轮交互思维和深度推理框架。我们的方法为解决复杂问题构建了一个结构化的思维过程，增强了问答（Q&A）任务中推理过程的逻辑一致性和上下文一致性，特别是在特定领域知识库（KBs）中。按照 KAG 的逻辑形式指导的检索和推理技术路线，该框架首先通过广度分解将复杂问题分解为独立可解决的子问题（也称为逻辑形式）。每个逻辑形式都以两种等价形式表示——自然语言和逻辑函数，并随后分类为知识检索或推理分析任务。这些任务之间的依赖关系和参数传递通过逻辑函数接口显式建模。在解决过程中，检索函数执行检索任务。它检索指定知识单元的一次跳转结构化和非结构化信息。而数学和推断函数用于执行推理分析任务。其次，值得一提的是，在知识检索子问题任务中，LLM 和外部知识源被视为等价的 KB。我们使用知识边界模块通过自我调节机制（如置信度校准和反思性推理）确定最佳源，并使用深度解决模块增强知识获取的全面性...,"The paper presents KAG-Thinker, a framework for enhancing the reasoning capabilities of LLMs through interactive thinking and deep reasoning.",LLM,Helpful,"LLM, reasoning, knowledge augmentation, interactive thinking, logical coherence"
Understanding Reasoning in Thinking Language Models via Steering Vectors,"Constantin Venhoff, Iv\'an Arcuschin, Philip Torr, Arthur Conmy, Neel Nanda",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2506.18167.pdf,"Recent advances in large language models (LLMs) have led to the development of thinking language models that generate extensive internal reasoning chains before producing responses. While these models achieve improved performance, controlling their reasoning processes remains challenging. This work presents a steering approach for thinking LLMs by analyzing and manipulating specific reasoning behaviors in DeepSeek-R1-Distill models. Through a systematic experiment on 500 tasks across 10 diverse categories, we identify several reasoning behaviors exhibited by thinking models, including expressing uncertainty, generating examples for hypothesis validation, and backtracking in reasoning chains. We demonstrate that these behaviors are mediated by linear directions in the model's activation space and can be controlled using steering vectors. By extracting and applying these vectors, we provide a method to modulate specific aspects of the model's reasoning process, such as its tendency to backtrack or express uncertainty. Our approach offers practical tools for steering reasoning processes in thinking models in a controlled and interpretable manner. We validate our steering method using three DeepSeek-R1-Distill models, demonstrating consistent control across different model architectures.",最近，大语言模型（LLMs）的进步导致了思考语言模型的发展，这些模型在产生响应之前生成广泛的内部推理链。虽然这些模型在性能上有所提高，但控制它们的推理过程仍然具有挑战性。本文提出了一种通过分析和操纵思考LLMs中的特定推理行为来引导这些模型的方法。通过在10个多样化类别中的500个任务上进行系统实验，我们识别出思考模型表现出的几种推理行为，包括表达不确定性、为假设验证生成示例以及在推理链中回溯。我们展示了这些行为是通过模型激活空间中的线性方向进行调节的，并且可以使用引导向量进行控制。通过提取和应用这些向量，我们提供了一种调节模型推理过程特定方面的方法，例如其回溯或表达不确定性的倾向。我们的方法为在受控和可解释的方式下引导思考模型的推理过程提供了实用工具。我们使用三个DeepSeek-R1-Distill模型验证了我们的引导方法，展示了在不同模型架构中的一致控制。,The paper introduces a method to control and interpret reasoning behaviors in thinking large language models using steering vectors.,LLM,"Helpful, Honest","Reasoning, Steering Vectors, Control, Thinking LLMs, DeepSeek-R1-Distill"
ReDit: Reward Dithering for Improved LLM Policy Optimization,"Chenxing Wei, Jiarui Yu, Ying Tiffany He, Hande Dong, Yao Shu, Fei Yu",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2506.18631.pdf,"DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning capabilities through its rule-based reward system. While it's a ''perfect'' reward system that effectively mitigates reward hacking, such reward functions are often discrete. Our experimental observations suggest that discrete rewards can lead to gradient anomaly, unstable optimization, and slow convergence. To address this issue, we propose ReDit (Reward Dithering), a method that dithers the discrete reward signal by adding simple random noise. With this perturbed reward, exploratory gradients are continuously provided throughout the learning process, enabling smoother gradient updates and accelerating convergence. The injected noise also introduces stochasticity into flat reward regions, encouraging the model to explore novel policies and escape local optima. Experiments across diverse tasks demonstrate the effectiveness and efficiency of ReDit. On average, ReDit achieves performance comparable to vanilla GRPO with only approximately 10% the training steps, and furthermore, still exhibits a 4% performance improvement over vanilla GRPO when trained for a similar duration. Visualizations confirm significant mitigation of gradient issues with ReDit. Moreover, theoretical analyses are provided to further validate these advantages.",DeepSeek-R1 通过其基于规则的奖励系统成功增强了大型语言模型（LLM）的推理能力。虽然这是一个有效地减少奖励作弊的“完美”奖励系统，但这种奖励函数通常是离散的。我们的实验观察表明，离散奖励可能导致梯度异常、优化不稳定和收敛缓慢。为了解决这个问题，我们提出了 ReDit（奖励抖动），一种通过添加简单随机噪声来抖动离散奖励信号的方法。通过这种扰动的奖励，探索梯度在整个学习过程中连续提供，使梯度更新更加平滑并加速收敛。注入的噪声还在平坦的奖励区域引入了随机性，鼓励模型探索新策略并逃离局部最优。跨多种任务的实验证明了 ReDit 的有效性和高效性。平均而言，ReDit 在仅约 10% 的训练步骤内实现了与纯粹 GRPO 相媲美的性能，并且在训练时间相似的情况下，仍然在性能上表现出 4% 的提升。可视化确认了 ReDit 显著缓解了梯度问题。此外，还提供了理论分析以进一步验证这些优势。,"The paper introduces ReDit, a method that adds random noise to discrete reward signals to improve the optimization and alignment of LLMs.",LLM,Helpful,"Reward Dithering, LLM Optimization, Gradient Stability, Convergence, Policy Exploration"
Robust Reinforcement Learning from Human Feedback for Large Language Models Fine-Tuning,"Kai Ye, Hongyi Zhou, Jin Zhu, Francesco Quinzan, Chengchun Shi",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2504.03784.pdf,"Reinforcement learning from human feedback (RLHF) has emerged as a key technique for aligning the output of large language models (LLMs) with human preferences. To learn the reward function, most existing RLHF algorithms use the Bradley-Terry model, which relies on assumptions about human preferences that may not reflect the complexity and variability of real-world judgments. In this paper, we propose a robust algorithm to enhance the performance of existing approaches under such reward model misspecifications. Theoretically, our algorithm reduces the variance of reward and policy estimators, leading to improved regret bounds. Empirical evaluations on LLM benchmark datasets demonstrate that the proposed algorithm consistently outperforms existing methods, with 77-81% of responses being favored over baselines on the Anthropic Helpful and Harmless dataset.",通过人类反馈的强化学习（RLHF）作为一种关键技术，将大型语言模型（LLM）的输出与人类偏好对齐。为了学习奖励函数，大多数现有的RLHF算法使用布拉德利-特里模型，该模型依赖于可能无法反映真实世界判断的复杂性和变化性的人类偏好假设。在本文中，我们提出了一种健壮算法，以增强现有方法在奖励模型规范化不当的情况下的性能。从理论上讲，我们的算法减少了奖励和策略估计器的方差，从而导致改进的后悔边界。在LLM基准数据集上的实证评估表明，所提出的算法在Anthropic有用和无害数据集上，77-81%的响应优于基线。,"The paper introduces a robust algorithm for RLHF to improve LLM alignment with human preferences, focusing on helpfulness and harmlessness.",LLM,"Helpful, Harmless","RLHF, LLM alignment, reward model, human preferences, robust algorithm"
TrainVerify: Equivalence-Based Verification for Distributed LLM Training,"Yunchi Lu, Youshan Miao, Cheng Tan, Peng Huang, Yi Zhu, Xian Zhang, Fan Yang",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2506.15961.pdf,"Training large language models (LLMs) at scale requires parallel execution across thousands of devices, incurring enormous computational costs. Yet, these costly distributed trainings are rarely verified, leaving them prone to silent errors and potentially wasting millions of GPU hours. We introduce TrainVerify, a system for verifiable distributed training of LLMs. Given a deep learning model's logical specification as the ground truth, TrainVerify formally verifies that a distributed parallel execution plan is mathematically equivalent to it. Direct verification is notoriously difficult due to the sheer scale of LLMs which often involves billions of variables and highly intricate computation graphs. Therefore, TrainVerify introduces shape-reduction techniques and a stage-wise parallel verification algorithm that significantly reduces complexity while preserving formal correctness. TrainVerify scales to frontier LLMs, including the successful verification of the Llama3 (405B) and DeepSeek-V3 (671B) training plans.",训练大型语言模型（LLMs）需要在数千个设备上并行执行，导致巨大的计算成本。然而，这些昂贵的分布式训练很少得到验证，容易出现静默错误，可能浪费数百万个GPU小时。我们引入了TrainVerify，一个用于可验证分布式训练LLMs的系统。给定深度学习模型的逻辑规范作为真实值，TrainVerify正式验证分布式并行执行计划在数学上等同于它。由于LLMs的规模巨大，通常涉及数十亿个变量和高度复杂的计算图，直接验证非常困难。因此，TrainVerify引入了形状减少技术和逐阶段并行验证算法，显著减少复杂性，同时保留形式正确性。TrainVerify可以扩展到前沿LLMs，包括成功验证Llama3（405B）和DeepSeek-V3（671B）的训练计划。,"The paper introduces TrainVerify, a system for verifying the correctness of distributed training plans for large language models.",LLM,None,"Distributed training, Verification, Large Language Models, Formal correctness, Computational efficiency"
