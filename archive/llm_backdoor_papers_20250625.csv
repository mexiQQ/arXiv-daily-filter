Title,Authors,Published,Link,Abstract_EN,Abstract_ZH,One_Sentence_Summary,Model_Type,Use_Intention,Focus,Keywords
Command-V: Pasting LLM Behaviors via Activation Profiles,"Barry Wang, Avi Schwarzschild, Alexander Robey, Ali Payani, Charles Fleming, Mingjie Sun, Daphne Ippolito",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2506.19140.pdf,"Retrofitting large language models (LLMs) with new behaviors typically requires full finetuning or distillation-costly steps that must be repeated for every architecture. In this work, we introduce Command-V, a backpropagation-free behavior transfer method that copies an existing residual activation adapter from a donor model and pastes its effect into a recipient model. Command-V profiles layer activations on a small prompt set, derives linear converters between corresponding layers, and applies the donor intervention in the recipient's activation space. This process does not require access to the original training data and needs minimal compute. In three case studies-safety-refusal enhancement, jailbreak facilitation, and automatic chain-of-thought reasoning--Command-V matches or exceeds the performance of direct finetuning while using orders of magnitude less compute. Our code and data are accessible at https://github.com/GithuBarry/Command-V/.",将新行为引入大型语言模型（LLM）通常需要全面的微调或蒸馏，这些步骤成本高且必须为每种架构重复进行。在本文中，我们引入了Command-V，一种无需反向传播的行为传输方法，它将来自捐赠模型的现有残差激活适配器复制并将其效果粘贴到接收模型中。Command-V在小型提示集上配置层激活，推导相应层之间的线性转换器，并在接收者的激活空间中应用捐赠者的干预。该过程不需要访问原始训练数据，计算量也很小。在三个案例研究中——安全拒绝增强、越狱便利和自动思维链推理——Command-V在使用了数量级更少的计算的情况下，与直接微调的性能相匹配或超过。我们的代码和数据可在https://github.com/GithuBarry/Command-V/访问。,"The paper presents Command-V, a backpropagation-free method for transferring behaviors in LLMs, including a backdoor technique for jailbreaking.",LLM,Negative,Attack,"Backdoor, Behavior Transfer, Activation Profiles, Jailbreak, Large Language Models"
Identifying Physically Realizable Triggers for Backdoored Face Recognition Networks,"Ankita Raj, Ambar Pal, Chetan Arora",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2506.19533.pdf,"Backdoor attacks embed a hidden functionality into deep neural networks, causing the network to display anomalous behavior when activated by a predetermined pattern in the input Trigger, while behaving well otherwise on public test data. Recent works have shown that backdoored face recognition (FR) systems can respond to natural-looking triggers like a particular pair of sunglasses. Such attacks pose a serious threat to the applicability of FR systems in high-security applications. We propose a novel technique to (1) detect whether an FR network is compromised with a natural, physically realizable trigger, and (2) identify such triggers given a compromised network. We demonstrate the effectiveness of our methods with a compromised FR network, where we are able to identify the trigger (e.g., green sunglasses or red hat) with a top-5 accuracy of 74%, whereas a naive brute force baseline achieves 56% accuracy.",后门攻击将隐藏功能嵌入深度神经网络，使网络在输入触发器激活时显示异常行为，而在公共测试数据上表现良好。最近的研究表明，带有后门的面部识别（FR）系统可以对自然外观的触发器（如特定的太阳镜）做出响应。此类攻击对FR系统在高安全性应用中的适用性构成严重威胁。我们提出了一种新技术，用于（1）检测FR网络是否被自然、物理可实现的触发器破坏，以及（2）在给定受损网络的情况下识别此类触发器。我们通过一个受损的FR网络演示了我们方法的有效性，其中我们能够以74%的top-5准确率识别触发器（例如，绿色太阳镜或红色帽子），而天真的暴力基线实现了56%的准确率。,The paper presents a method to detect and identify natural-looking triggers in backdoored face recognition networks.,LMM,Negative,Defense,"Backdoor attacks, Face recognition, Trigger detection, Multimodal models, Defense mechanisms"
Unified Neural Backdoor Removal with Only Few Clean Samples through Unlearning and Relearning,"Nay Myat Min, Long H. Pham, Jun Sun",2025-06-25T00:00:00-04:00,https://arxiv.org/pdf/2405.14781.pdf,"Deep neural networks have achieved remarkable success across various applications; however, their vulnerability to backdoor attacks poses severe security risks -- especially in situations where only a limited set of clean samples is available for defense. In this work, we address this critical challenge by proposing ULRL (UnLearn and ReLearn for backdoor removal), a novel two-phase approach for comprehensive backdoor removal. Our method first employs an unlearning phase, in which the network's loss is intentionally maximized on a small clean dataset to expose neurons that are excessively sensitive to backdoor triggers. Subsequently, in the relearning phase, these suspicious neurons are recalibrated using targeted reinitialization and cosine similarity regularization, effectively neutralizing backdoor influences while preserving the model's performance on benign data. Extensive experiments with 12 backdoor types on multiple datasets (CIFAR-10, CIFAR-100, GTSRB, and Tiny-ImageNet) and architectures (PreAct-ResNet18, VGG19-BN, and ViT-B-16) demonstrate that ULRL significantly reduces the attack success rate without compromising clean accuracy -- even when only 1% of clean data is used for defense.",深度神经网络在各种应用中取得了显著成功；然而，它们对后门攻击的易受性构成了严重的安全风险，特别是在只有有限的干净样本可用于防御的情况下。在本文中，我们通过提出ULRL（UnLearn和ReLearn用于后门移除），一种新颖的两阶段方法来解决这一关键挑战，以实现全面的后门移除。我们的方法首先采用一个去学习阶段，其中网络的损失在一个小的干净数据集上故意最大化，以暴露对后门触发器过于敏感的神经元。随后，在重新学习阶段，这些可疑的神经元使用有针对性的重新初始化和余弦相似度正则化进行重新校准，从而有效地中和后门影响，同时保留模型在良性数据上的性能。在多个数据集（CIFAR-10、CIFAR-100、GTSRB和Tiny-ImageNet）和架构（PreAct-ResNet18、VGG19-BN和ViT-B-16）上进行的广泛实验表明，ULRL显著降低了攻击成功率，而没有损害干净的准确性，即使在防御中只使用1%的干净数据。,"The paper presents a two-phase method called ULRL for removing backdoors from neural networks, even when only a small amount of clean data is available.",LMM,Negative,Defense,"Backdoor removal, Unlearning, Relearning, Neural networks, Defense"
