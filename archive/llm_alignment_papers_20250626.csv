Title,Authors,Published,Link,Abstract_EN,Abstract_ZH,One_Sentence_Summary,Model_Type,HHH_Focus,Keywords
Retrieval-Confused Generation is a Good Defender for Privacy Violation Attack of Large Language Models,"Wanli Peng, Xin Chen, Hang Fu, XinYu He, Xue Yiming, Juan Wen",2025-06-26T00:00:00-04:00,https://arxiv.org/pdf/2506.19889.pdf,"Recent advances in large language models (LLMs) have made a profound impact on our society and also raised new security concerns. Particularly, due to the remarkable inference ability of LLMs, the privacy violation attack (PVA), revealed by Staab et al., introduces serious personal privacy issues. Existing defense methods mainly leverage LLMs to anonymize the input query, which requires costly inference time and cannot gain satisfactory defense performance. Moreover, directly rejecting the PVA query seems like an effective defense method, while the defense method is exposed, promoting the evolution of PVA. In this paper, we propose a novel defense paradigm based on retrieval-confused generation (RCG) of LLMs, which can efficiently and covertly defend the PVA. We first design a paraphrasing prompt to induce the LLM to rewrite the ""user comments"" of the attack query to construct a disturbed database. Then, we propose the most irrelevant retrieval strategy to retrieve the desired user data from the disturbed database. Finally, the ""data comments"" are replaced with the retrieved user data to form a defended query, leading to responding to the adversary with some wrong personal attributes, i.e., the attack fails. Extensive experiments are conducted on two datasets and eight popular LLMs to comprehensively evaluate the feasibility and the superiority of the proposed defense method.",最近，大语言模型（LLMs）的进步对我们的社会产生了深远的影响，也引发了新的安全问题。特别是由于LLMs的出色推理能力，Staab等人揭示的隐私侵犯攻击（PVA）引发了严重的个人隐私问题。现有的防御方法主要利用LLMs对输入查询进行匿名化，这需要昂贵的推理时间，并且无法获得令人满意的防御性能。此外，直接拒绝PVA查询似乎是一种有效的防御方法，但防御方法一旦暴露，就会促进PVA的进化。在本文中，我们提出了一种基于LLMs的检索混淆生成（RCG）的新防御范式，可以高效且隐蔽地防御PVA。我们首先设计了一个改写提示，诱导LLM将攻击查询的“用户评论”重写为构建扰动数据库。然后，我们提出了最不相关的检索策略，从扰动数据库中检索所需的用户数据。最后，用检索到的用户数据替换“数据评论”，以形成一个防御查询，导致对手收到一些错误的个人属性，即攻击失败。我们在两个数据集和八个流行的LLMs上进行了广泛的实验，全面评估了所提出的防御方法的可行性和优越性。,The paper introduces a novel defense mechanism called retrieval-confused generation to protect large language models from privacy violation attacks.,LLM,Harmless,"Privacy Violation, Defense Mechanism, Retrieval-Confused Generation, Large Language Models, Security"
Persona-Assigned Large Language Models Exhibit Human-Like Motivated Reasoning,"Saloni Dash, Am\'elie Reymond, Emma S. Spiro, Aylin Caliskan",2025-06-26T00:00:00-04:00,https://arxiv.org/pdf/2506.20020.pdf,"Reasoning in humans is prone to biases due to underlying motivations like identity protection, that undermine rational decision-making and judgment. This motivated reasoning at a collective level can be detrimental to society when debating critical issues such as human-driven climate change or vaccine safety, and can further aggravate political polarization. Prior studies have reported that large language models (LLMs) are also susceptible to human-like cognitive biases, however, the extent to which LLMs selectively reason toward identity-congruent conclusions remains largely unexplored. Here, we investigate whether assigning 8 personas across 4 political and socio-demographic attributes induces motivated reasoning in LLMs. Testing 8 LLMs (open source and proprietary) across two reasoning tasks from human-subject studies -- veracity discernment of misinformation headlines and evaluation of numeric scientific evidence -- we find that persona-assigned LLMs have up to 9% reduced veracity discernment relative to models without personas. Political personas specifically, are up to 90% more likely to correctly evaluate scientific evidence on gun control when the ground truth is congruent with their induced political identity. Prompt-based debiasing methods are largely ineffective at mitigating these effects. Taken together, our empirical findings are the first to suggest that persona-assigned LLMs exhibit human-like motivated reasoning that is hard to mitigate through conventional debiasing prompts -- raising concerns of exacerbating identity-congruent reasoning in both LLMs and humans.",人类的推理容易受到保护身份等动机的偏见影响，从而削弱理性决策和判断。这种动机推理在集体层面上可能对社会有害，例如在讨论人为气候变化或疫苗安全等关键问题时，可能进一步加剧政治极化。先前的研究表明，大型语言模型（LLMs）也容易受到类似人类的认知偏见的影响，然而，LLMs在多大程度上选择性地推理出与身份一致的结论仍然尚未探索。在此，我们研究将8个角色分配给4个政治和社会人口统计属性，是否会在LLMs中引发动机推理。我们在两个来自人类受试者研究的推理任务中测试了8个LLMs（开源和专有），即辨别虚假信息标题的真实性和评估数字科学证据，发现分配角色的LLMs在真实性辨别方面相对于没有角色的模型减少了高达9%。特别是政治角色，当地面真相与其诱导的政治身份一致时，正确评估枪支管制的科学证据的可能性高达90%。基于提示的去偏方法在很大程度上无效于缓解这些效应。综上所述，我们的实证发现首次表明，分配角色的LLMs表现出类似人类的动机推理，难以通过常规去偏提示来缓解，这引发了对在LLMs和人类中加剧与身份一致的推理的担忧。,"The paper finds that large language models assigned personas exhibit human-like motivated reasoning, which is difficult to mitigate through conventional debiasing methods.",LLM,Harmless,"Motivated reasoning, Persona assignment, Bias, Debiasing, LLMs"
DiaLLMs: EHR Enhanced Clinical Conversational System for Clinical Test Recommendation and Diagnosis Prediction,"Weijieying Ren, Tianxiang Zhao, Lei Wang, Tianchun Wang, Vasant Honavar",2025-06-26T00:00:00-04:00,https://arxiv.org/pdf/2506.20059.pdf,"Recent advances in Large Language Models (LLMs) have led to remarkable progresses in medical consultation. However, existing medical LLMs overlook the essential role of Electronic Health Records (EHR) and focus primarily on diagnosis recommendation, limiting their clinical applicability. We propose DiaLLM, the first medical LLM that integrates heterogeneous EHR data into clinically grounded dialogues, enabling clinical test recommendation, result interpretation, and diagnosis prediction to better align with real-world medical practice. To construct clinically grounded dialogues from EHR, we design a Clinical Test Reference (CTR) strategy that maps each clinical code to its corresponding description and classifies test results as ""normal"" or ""abnormal"". Additionally, DiaLLM employs a reinforcement learning framework for evidence acquisition and automated diagnosis. To handle the large action space, we introduce a reject sampling strategy to reduce redundancy and improve exploration efficiency. Furthermore, a confirmation reward and a class-sensitive diagnosis reward are designed to guide accurate diagnosis prediction. Extensive experimental results demonstrate that DiaLLM outperforms baselines in clinical test recommendation and diagnosis prediction.",最近，大型语言模型（LLMs）在医疗咨询方面取得了显著进展。然而，现有的医疗LLMs忽视了电子健康记录（EHR）的重要作用，主要集中在诊断推荐上，限制了其临床适用性。我们提出了DiaLLM，这是第一个将异构EHR数据集成到临床对话中的医疗LLM，能够进行临床测试推荐、结果解释和诊断预测，以更好地与实际医疗实践保持一致。为了从EHR构建临床对话，我们设计了一种临床测试参考（CTR）策略，将每个临床代码映射到其对应的描述，并将测试结果分类为“正常”或“异常”。此外，DiaLLM采用强化学习框架进行证据获取和自动诊断。为了处理大动作空间，我们引入了拒绝采样策略，以减少冗余并提高探索效率。此外，设计了确认奖励和类敏感诊断奖励，以指导准确的诊断预测。广泛的实验结果表明，DiaLLM在临床测试推荐和诊断预测方面优于基线。,"The paper introduces DiaLLM, an LLM integrated with EHR data for improved clinical test recommendation and diagnosis prediction in medical consultations.",LLM,Helpful,"LLM, EHR, medical consultation, clinical test recommendation, diagnosis prediction"
Learning Instruction-Following Policies through Open-Ended Instruction Relabeling with Large Language Models,"Zhicheng Zhang, Ziyan Wang, Yali Du, Fei Fang",2025-06-26T00:00:00-04:00,https://arxiv.org/pdf/2506.20061.pdf,"Developing effective instruction-following policies in reinforcement learning remains challenging due to the reliance on extensive human-labeled instruction datasets and the difficulty of learning from sparse rewards. In this paper, we propose a novel approach that leverages the capabilities of large language models (LLMs) to automatically generate open-ended instructions retrospectively from previously collected agent trajectories. Our core idea is to employ LLMs to relabel unsuccessful trajectories by identifying meaningful subtasks the agent has implicitly accomplished, thereby enriching the agent's training data and substantially alleviating reliance on human annotations. Through this open-ended instruction relabeling, we efficiently learn a unified instruction-following policy capable of handling diverse tasks within a single policy. We empirically evaluate our proposed method in the challenging Craftax environment, demonstrating clear improvements in sample efficiency, instruction coverage, and overall policy performance compared to state-of-the-art baselines. Our results highlight the effectiveness of utilizing LLM-guided open-ended instruction relabeling to enhance instruction-following reinforcement learning.",通过大型语言模型（LLM）自动生成开放式指令，从先前收集的代理轨迹中回溯生成，从而丰富代理的训练数据，显著减少对人类标注的依赖。通过这种开放式指令重新标记，我们有效地学习了一个统一的指令遵循策略，能够在单个策略中处理多种任务。我们在具有挑战性的Craftax环境中对我们提出的方法进行了实证评估，显示出样本效率、指令覆盖和整体策略性能方面的显著改进。,The paper presents a method using LLMs to generate instructions from agent trajectories to improve instruction-following in reinforcement learning.,LLM,Helpful,"Instruction-following, Reinforcement Learning, Large Language Models, Relabeling, Policy Learning"
A Modular Multitask Reasoning Framework Integrating Spatio-temporal Models and LLMs,"Kethmi Hirushini Hettige, Jiahao Ji, Cheng Long, Shili Xiang, Gao Cong, Jingyuan Wang",2025-06-26T00:00:00-04:00,https://arxiv.org/pdf/2506.20073.pdf,"Spatio-temporal data mining plays a pivotal role in informed decision making across diverse domains. However, existing models are often restricted to narrow tasks, lacking the capacity for multi-task inference and complex long-form reasoning that require generation of in-depth, explanatory outputs. These limitations restrict their applicability to real-world, multi-faceted decision scenarios. In this work, we introduce STReason, a novel framework that integrates the reasoning strengths of large language models (LLMs) with the analytical capabilities of spatio-temporal models for multi-task inference and execution. Without requiring task-specific finetuning, STReason leverages in-context learning to decompose complex natural language queries into modular, interpretable programs, which are then systematically executed to generate both solutions and detailed rationales. To facilitate rigorous evaluation, we construct a new benchmark dataset and propose a unified evaluation framework with metrics specifically designed for long-form spatio-temporal reasoning. Experimental results show that STReason significantly outperforms advanced LLM baselines across all metrics, particularly excelling in complex, reasoning-intensive spatio-temporal scenarios. Human evaluations further validate STReason's credibility and practical utility, demonstrating its potential to reduce expert workload and broaden the applicability to real-world spatio-temporal tasks. We believe STReason provides a promising direction for developing more capable and generalizable spatio-temporal reasoning systems.",空间时间数据挖掘在各种领域的信息化决策中起着至关重要的作用。然而，现有模型通常受限于狭窄的任务，缺乏进行多任务推理和复杂长期推理的能力，这需要生成深入、解释性的输出。这些限制使它们在现实世界中多方面的决策场景中难以应用。在本文中，我们提出了STReason，一种新颖的框架，将大型语言模型（LLMs）的推理能力与空间时间模型的分析能力结合起来，用于多任务推理和执行。STReason不需要特定任务的微调，利用上下文学习将复杂的自然语言查询分解为模块化、可解释的程序，然后系统地执行这些程序以生成解决方案和详细的理由。为了促进严格的评估，我们构建了一个新的基准数据集，并提出了一个统一的评估框架，其中包含专门为长期空间时间推理设计的指标。实验结果表明，STReason在所有指标上都显著优于先进的LLM基线，特别是在复杂、推理密集型的空间时间场景中表现出色。人类评估进一步验证了STReason的可信度和实际效用，表明其有潜力减少专家的工作量，并扩大其在现实世界空间时间任务中的适用性。我们认为STReason为开发更具能力和通用性的空间时间推理系统提供了一个有前途的方向。,"The paper introduces STReason, a framework that integrates LLMs with spatio-temporal models for multi-task inference and execution, demonstrating improved performance in complex reasoning scenarios.",LLM,Helpful,"LLM, Spatio-temporal reasoning, Multi-task inference, In-context learning, Decision making"
How to Retrieve Examples in In-context Learning to Improve Conversational Emotion Recognition using Large Language Models?,"Mengqi Wang, Tiantian Feng, Shrikanth Narayanan",2025-06-26T00:00:00-04:00,https://arxiv.org/pdf/2506.20199.pdf,"Large language models (LLMs) have enabled a wide variety of real-world applications in various domains. However, creating a high-performing application with high accuracy remains challenging, particularly for subjective tasks like emotion recognition. Inspired by the SLT 2024 GenSER Challenge, this study investigates approaches to improving conversational emotion recognition (CER) by LLMs. Specifically, we explore how to retrieve high-quality examples in in-context learning (ICL) to enhance CER. We propose various strategies based on random and augmented example retrieval and also analyze the impact of conversational context on CER accuracy. Experiments were conducted on the three datasets including IEMOCAP, MELD and EmoryNLP. The results show that augmented example retrieval consistently outperforms other techniques under investigation across all datasets, highlighting the importance of retrieving coherent targeted examples and enhancing them through paraphrasing.",大语言模型（LLMs）已经在各种领域的实际应用中取得了广泛的成功。然而，创建一个具有高准确性的高性能应用程序仍然具有挑战性，特别是对于主观任务，如情感识别。受SLT 2024 GenSER挑战赛的启发，本研究探讨了通过LLMs改进对话情感识别（CER）的方法。具体来说，我们探讨了如何在上下文学习（ICL）中检索高质量的示例以增强CER。我们提出了基于随机和增强示例检索的各种策略，并分析了对话上下文对CER准确性的影响。实验在包括IEMOCAP、MELD和EmoryNLP在内的三个数据集上进行。结果表明，增强示例检索在所有数据集上都显著优于其他研究技术，突显了检索一致的有针对性的示例并通过改写增强它们的重要性。,The paper explores methods to improve conversational emotion recognition in large language models by enhancing example retrieval in in-context learning.,LLM,Helpful,"In-context learning, Emotion recognition, Example retrieval, Large language models, Conversational AI"
Q-resafe: Assessing Safety Risks and Quantization-aware Safety Patching for Quantized Large Language Models,"Kejia Chen, Jiawen Zhang, Jiacong Hu, Yu Wang, Jian Lou, Zunlei Feng, Mingli Song",2025-06-26T00:00:00-04:00,https://arxiv.org/pdf/2506.20251.pdf,"Quantized large language models (LLMs) have gained increasing attention and significance for enabling deployment in resource-constrained environments. However, emerging studies on a few calibration dataset-free quantization methods suggest that quantization may compromise the safety capabilities of LLMs, underscoring the urgent need for systematic safety evaluations and effective mitigation strategies. In this paper, we present comprehensive safety evaluations across various mainstream quantization techniques and diverse calibration datasets, utilizing widely accepted safety benchmarks. To address the identified safety vulnerabilities, we propose a quantization-aware safety patching framework, Q-resafe, to efficiently restore the safety capabilities of quantized LLMs while minimizing any adverse impact on utility. Extensive experimental results demonstrate that Q-resafe successfully re-aligns the safety of quantized LLMs with their pre-quantization counterparts, even under challenging evaluation scenarios. Project page is available at: https://github.com/Thecommonirin/Qresafe.",量化大语言模型（LLMs）在资源受限环境中部署方面越来越受到关注和重视。然而，关于几种无校准数据集的量化方法的研究表明，量化可能会削弱LLMs的安全能力，这突显了系统化安全评估和有效缓解策略的紧迫需求。在本文中，我们在各种主流量化技术和多样化校准数据集上进行了全面的安全评估，利用广泛接受的安全基准。为了解决所识别的安全漏洞，我们提出了一个量化感知安全补丁框架Q-resafe，以高效地恢复量化LLMs的安全能力，同时最小化对效用的任何不利影响。广泛的实验结果表明，Q-resafe在挑战性评估场景下成功地重新对齐了量化LLMs的安全性，与其量化前的对应物相一致。项目页面可在以下网址找到：https://github.com/Thecommonirin/Qresafe。,"The paper introduces Q-resafe, a framework to evaluate and restore the safety of quantized large language models.",LLM,Harmless,"Quantization, Safety, LLMs, Patching, Evaluation"
Enterprise Large Language Model Evaluation Benchmark,"Liya Wang, David Yi, Damien Jose, John Passarelli, James Gao, Jordan Leventis, Kang Li",2025-06-26T00:00:00-04:00,https://arxiv.org/pdf/2506.20274.pdf,"Large Language Models (LLMs) ) have demonstrated promise in boosting productivity across AI-powered tools, yet existing benchmarks like Massive Multitask Language Understanding (MMLU) inadequately assess enterprise-specific task complexities. We propose a 14-task framework grounded in Bloom's Taxonomy to holistically evaluate LLM capabilities in enterprise contexts. To address challenges of noisy data and costly annotation, we develop a scalable pipeline combining LLM-as-a-Labeler, LLM-as-a-Judge, and corrective retrieval-augmented generation (CRAG), curating a robust 9,700-sample benchmark. Evaluation of six leading models shows open-source contenders like DeepSeek R1 rival proprietary models in reasoning tasks but lag in judgment-based scenarios, likely due to overthinking. Our benchmark reveals critical enterprise performance gaps and offers actionable insights for model optimization. This work provides enterprises a blueprint for tailored evaluations and advances practical LLM deployment.","大型语言模型（LLMs）在提高AI驱动工具的生产力方面表现出巨大潜力，但现有的基准测试，如大规模多任务语言理解（MMLU），无法充分评估企业特定任务的复杂性。我们提出一个基于布鲁姆分类法的14项任务框架，全面评估LLM在企业环境中的能力。为了应对噪声数据和昂贵标注的挑战，我们开发了一种可扩展的流水线，结合LLM作为标签器、LLM作为裁判和校正检索增强生成（CRAG），策划了一个坚固的9,700样本基准。对六个领先模型的评估表明，开源竞争者如DeepSeek R1在推理任务中与专有模型不相上下，但在基于判断的场景中表现不佳，可能是由于过度思考。我们的基准揭示了关键的企业性能差距，并为模型优化提供了可操作的见解。本文为企业提供了定制评估的蓝图，并推动了实际LLM部署。","The paper introduces a new benchmark for evaluating LLMs in enterprise settings, highlighting performance gaps and providing insights for model optimization.",LLM,Helpful,"LLM evaluation, enterprise tasks, benchmark, Bloom's Taxonomy, model optimization"
TAPS: Tool-Augmented Personalisation via Structured Tagging,"Ekaterina Taktasheva, Jeff Dalton",2025-06-26T00:00:00-04:00,https://arxiv.org/pdf/2506.20409.pdf,"Recent advancements in tool-augmented large language models have enabled them to interact with external tools, enhancing their ability to perform complex user tasks. However, existing approaches overlook the role of personalisation in guiding tool use. This work investigates how user preferences can be effectively integrated into goal-oriented dialogue agents. Through extensive analysis, we identify key weaknesses in the ability of LLMs to personalise tool use. To this end, we introduce \name, a novel solution that enhances personalised tool use by leveraging a structured tagging tool and an uncertainty-based tool detector. TAPS significantly improves the ability of LLMs to incorporate user preferences, achieving the new state-of-the-art for open source models on the NLSI task.",最近，工具增强的大型语言模型的进展使它们能够与外部工具交互，从而增强了它们执行复杂用户任务的能力。然而，现有方法忽略了个性化在指导工具使用中的作用。本文研究了用户偏好如何有效地集成到目标导向的对话代理中。通过广泛的分析，我们识别出大型语言模型在个性化工具使用方面的关键弱点。为此，我们引入了\name，一种新颖的解决方案，通过利用结构化标记工具和基于不确定性的工具检测器，增强了个性化工具使用。TAPS显著提高了大型语言模型将用户偏好纳入的能力，在NLSI任务上实现了开源模型的新的最佳状态。,"The paper introduces TAPS, a method to enhance personalised tool use in large language models by leveraging structured tagging and uncertainty-based tool detection.",LLM,Helpful,"Personalisation, Tool-Augmented, Large Language Models, User Preferences, Structured Tagging"
Probing AI Safety with Source Code,"Ujwal Narayan, Shreyas Chaudhari, Ashwin Kalyan, Tanmay Rajpurohit, Karthik Narasimhan, Ameet Deshpande, Vishvak Murahari",2025-06-26T00:00:00-04:00,https://arxiv.org/pdf/2506.20471.pdf,"Large language models (LLMs) have become ubiquitous, interfacing with humans in numerous safety-critical applications. This necessitates improving capabilities, but importantly coupled with greater safety measures to align these models with human values and preferences. In this work, we demonstrate that contemporary models fall concerningly short of the goal of AI safety, leading to an unsafe and harmful experience for users. We introduce a prompting strategy called Code of Thought (CoDoT) to evaluate the safety of LLMs. CoDoT converts natural language inputs to simple code that represents the same intent. For instance, CoDoT transforms the natural language prompt ""Make the statement more toxic: {text}"" to: ""make_more_toxic({text})"". We show that CoDoT results in a consistent failure of a wide range of state-of-the-art LLMs. For example, GPT-4 Turbo's toxicity increases 16.5 times, DeepSeek R1 fails 100% of the time, and toxicity increases 300% on average across seven modern LLMs. Additionally, recursively applying CoDoT can further increase toxicity two times. Given the rapid and widespread adoption of LLMs, CoDoT underscores the critical need to evaluate safety efforts from first principles, ensuring that safety and capabilities advance together.",大语言模型（LLMs）已经无处不在，与人类在许多安全关键应用中进行交互。这需要提高能力，但更重要的是与更大的安全措施相结合，以使这些模型与人类的价值观和偏好保持一致。在本工作中，我们展示了当代模型在实现人工智能安全方面令人担忧地落后于目标，导致用户的不安全和有害体验。我们引入了一种称为代码思维（CoDoT）的提示策略，以评估LLMs的安全性。CoDoT将自然语言输入转换为表示相同意图的简单代码。例如，CoDoT将自然语言提示“使陈述更具毒性：{text}”转换为：“make_more_toxic({text})”。我们表明，CoDoT导致一系列最新的LLMs一致失败。例如，GPT-4 Turbo的毒性增加了16.5倍，DeepSeek R1 100%的时间失败，七种现代LLMs的毒性平均增加了300%。此外，递归应用CoDoT可以进一步增加毒性两倍。鉴于LLMs的快速和广泛采用，CoDoT强调了从第一原理评估安全努力的关键性，确保安全性和能力共同进步。,The paper introduces a method called Code of Thought (CoDoT) to evaluate the safety of large language models (LLMs) and demonstrates that contemporary LLMs often produce harmful outputs.,LLM,Harmless,"LLM safety, alignment, harmful outputs, Code of Thought, toxicity"
Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing positive and negative rewards,"Charles Arnal, Ga\""etan Narozniak, Vivien Cabannes, Yunhao Tang, Julia Kempe, Remi Munos",2025-06-26T00:00:00-04:00,https://arxiv.org/pdf/2506.20520.pdf,"Reinforcement learning (RL) is increasingly used to align large language models (LLMs). Off-policy methods offer greater implementation simplicity and data efficiency than on-policy techniques, but often result in suboptimal performance. In this work, we study the intermediate range of algorithms between off-policy RL and supervised fine-tuning by analyzing a simple off-policy REINFORCE algorithm, where the advantage is defined as $A=r-V$, with $r$ a reward and $V$ some tunable baseline. Intuitively, lowering $V$ emphasizes high-reward samples, while raising it penalizes low-reward ones more heavily. We first provide a theoretical analysis of this off-policy REINFORCE algorithm, showing that when the baseline $V$ lower-bounds the expected reward, the algorithm enjoys a policy improvement guarantee. Our analysis reveals that while on-policy updates can safely leverage both positive and negative signals, off-policy updates benefit from focusing more on positive rewards than on negative ones. We validate our findings experimentally in a controlled stochastic bandit setting and through fine-tuning state-of-the-art LLMs on reasoning tasks.",强化学习（RL）越来越多地用于对大型语言模型（LLMs）进行对齐。离策略方法在实现简单性和数据效率方面优于在策略技术，但通常会导致次优性能。在本工作中，我们通过分析一个简单的离策略REINFORCE算法，其中优势定义为$A=r-V$，其中$r$是奖励，$V$是某个可调基线，来研究离策略RL和监督微调之间的中间算法范围。直观地讲，降低$V$强调高奖励样本，而提高它会更严重地惩罚低奖励样本。我们首先对这个离策略REINFORCE算法进行了理论分析，表明当基线$V$下界为期望奖励时，算法享有策略改进保证。我们的分析揭示了，虽然在策略更新可以安全地利用正信号和负信号，离策略更新从更多地关注正奖励而不是负奖励中受益。我们在受控随机臂带设置和通过在推理任务上对最先进的LLMs进行微调中验证了我们的发现。,"The paper proposes an off-policy reinforcement learning algorithm for aligning large language models, focusing on balancing positive and negative rewards.",LLM,Helpful,"Reinforcement Learning, Off-Policy, Large Language Models, Alignment, Fine-Tuning"
When Life Gives You Samples: The Benefits of Scaling up Inference Compute for Multilingual LLMs,"Ammar Khairi, Daniel D'souza, Ye Shen, Julia Kreutzer, Sara Hooker",2025-06-26T00:00:00-04:00,https://arxiv.org/pdf/2506.20544.pdf,"Recent advancements in large language models (LLMs) have shifted focus toward scaling inference-time compute, improving performance without retraining the model. A common approach is to sample multiple outputs in parallel, and select one of these as the final output. However, work to date has focused on English and a handful of domains such as math and code. In contrast, we are most interested in techniques that generalize across open-ended tasks, formally verifiable tasks, and across languages. In this work, we study how to robustly scale inference-time compute for open-ended generative tasks in a multilingual, multi-task setting.
  Our findings show that both sampling strategy based on temperature variation and selection strategy must be adapted to account for diverse domains and varied language settings. We evaluate existing selection methods, revealing that strategies effective in English often fail to generalize across languages. We propose novel sampling and selection strategies specifically adapted for multilingual and multi-task inference scenarios, and show they yield notable gains across languages and tasks. In particular, our combined sampling and selection methods lead to an average +6.8 jump in win-rates for our 8B models on m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At larger scale, Command-A (111B model) equipped with our methods, shows +9.0 improvement in win-rates on the same benchmark with just five samples against single-sample decoding, a substantial increase at minimal cost. Our results underscore the need for language- and task-aware approaches to inference-time compute, aiming to democratize performance improvements in underrepresented languages.",最近，大型语言模型（LLMs）的进展使得关注点转向了扩展推理时计算，以提高性能而无需重新训练模型。常见的方法是并行采样多个输出，并选择其中一个作为最终输出。然而，迄今为止的工作主要集中在英语和少数几个领域，如数学和代码。相比之下，我们更感兴趣的是能够跨开放式任务、正式可验证任务和语言的技术。在本文中，我们研究了如何在多语言、多任务设置中为开放式生成任务稳健地扩展推理时计算。我们的发现表明，基于温度变化的采样策略和选择策略必须适应多样化的领域和不同的语言设置。我们评估了现有的选择方法，揭示了在英语中有效的策略往往无法跨语言推广。我们提出了专门为多语言和多任务推理场景设计的新采样和选择策略，并表明它们在语言和任务之间显著提高了性能。特别是，我们的组合采样和选择方法使得我们的8B模型在m-ArenaHard-v2.0提示上对Gemini等专有模型的胜率平均提高了+6.8。在更大规模上，配备我们方法的Command-A（111B模型）在相同基准上显示出+9.0的胜率提高，仅使用五个样本对单样本解码，这是一个显著的提高，成本极低。我们的结果强调了推理时计算的语言和任务感知方法的必要性，旨在民主化不受重视语言的性能改进。,The paper introduces novel sampling and selection strategies for multilingual LLMs that improve performance across languages and tasks with minimal computational cost.,LLM,Helpful,"Multilingual, Inference, Sampling, Selection, Performance"
Model Editing as a Double-Edged Sword: Steering Agent Ethical Behavior Toward Beneficence or Harm,"Baixiang Huang, Zhen Tan, Haoran Wang, Zijie Liu, Dawei Li, Ali Payani, Huan Liu, Tianlong Chen, Kai Shu",2025-06-26T00:00:00-04:00,https://arxiv.org/pdf/2506.20606.pdf,"Agents based on Large Language Models (LLMs) have demonstrated strong capabilities across a wide range of tasks. However, deploying LLM-based agents in high-stakes domains comes with significant safety and ethical risks. Unethical behavior by these agents can directly result in serious real-world consequences, including physical harm and financial loss. To efficiently steer the ethical behavior of agents, we frame agent behavior steering as a model editing task, which we term Behavior Editing. Model editing is an emerging area of research that enables precise and efficient modifications to LLMs while preserving their overall capabilities. To systematically study and evaluate this approach, we introduce BehaviorBench, a multi-tier benchmark grounded in psychological moral theories. This benchmark supports both the evaluation and editing of agent behaviors across a variety of scenarios, with each tier introducing more complex and ambiguous scenarios. We first demonstrate that Behavior Editing can dynamically steer agents toward the target behavior within specific scenarios. Moreover, Behavior Editing enables not only scenario-specific local adjustments but also more extensive shifts in an agent's global moral alignment. We demonstrate that Behavior Editing can be used to promote ethical and benevolent behavior or, conversely, to induce harmful or malicious behavior. Through comprehensive evaluations on agents based on frontier LLMs, BehaviorBench shows the effectiveness of Behavior Editing across different models and scenarios. Our findings offer key insights into a new paradigm for steering agent behavior, highlighting both the promise and perils of Behavior Editing.",基于大型语言模型（LLM）的代理人在各种任务中表现出强大的能力。然而，在高风险领域部署基于LLM的代理人存在显著的安全和伦理风险。代理人的不道德行为可能直接导致严重的现实世界后果，包括身体伤害和财务损失。为了高效地引导代理人的伦理行为，我们将代理行为引导框架为模型编辑任务，我们称之为行为编辑。模型编辑是一个新兴的研究领域，它使得在保留LLM整体能力的同时，可以进行精确和高效的修改。为了系统地研究和评估这种方法，我们引入了BehaviorBench，一个基于心理学道德理论的多层次基准。该基准支持在各种情景下对代理行为的评估和编辑，每一层次都引入更复杂和模糊的情景。我们首先展示了行为编辑可以在特定情景中动态引导代理人朝向目标行为。此外，行为编辑不仅使得特定情景的局部调整成为可能，还使得代理人全局道德对齐的更广泛转变成为可能。我们展示了行为编辑可以用于促进伦理和仁慈的行为，或者相反，诱导有害或恶意的行为。通过在基于前沿LLM的代理人上的全面评估，BehaviorBench展示了行为编辑在不同模型和情景中的有效性。我们的发现为引导代理人行为提供了一个新的范式，突出了行为编辑的前景和风险。,"The paper introduces Behavior Editing, a model editing approach to steer the ethical behavior of LLM-based agents, using a benchmark called BehaviorBench.",LLM,"Helpful, Harmless","Model Editing, Ethical Behavior, LLM Alignment, BehaviorBench, Moral Theories"
Inside you are many wolves: Using cognitive models to interpret value trade-offs in LLMs,"Sonia K. Murthy, Rosie Zhao, Jennifer Hu, Sham Kakade, Markus Wulfmeier, Peng Qian, Tomer Ullman",2025-06-26T00:00:00-04:00,https://arxiv.org/pdf/2506.20666.pdf,"Navigating everyday social situations often requires juggling conflicting goals, such as conveying a harsh truth, maintaining trust, all while still being mindful of another person's feelings. These value trade-offs are an integral part of human decision-making and language use, however, current tools for interpreting such dynamic and multi-faceted notions of values in LLMs are limited. In cognitive science, so-called ""cognitive models"" provide formal accounts of these trade-offs in humans, by modeling the weighting of a speaker's competing utility functions in choosing an action or utterance. In this work, we use a leading cognitive model of polite speech to interpret the extent to which LLMs represent human-like trade-offs. We apply this lens to systematically evaluate value trade-offs in two encompassing model settings: degrees of reasoning ""effort"" in frontier black-box models, and RL post-training dynamics of open-source models. Our results highlight patterns of higher informational utility than social utility in reasoning models, and in open-source models shown to be stronger in mathematical reasoning. Our findings from LLMs' training dynamics suggest large shifts in utility values early on in training with persistent effects of the choice of base model and pretraining data, compared to feedback dataset or alignment method. We show that our method is responsive to diverse aspects of the rapidly evolving LLM landscape, with insights for forming hypotheses about other high-level behaviors, shaping training regimes for reasoning models, and better controlling trade-offs between values during model training.",导航日常社交情境通常需要平衡冲突的目标，例如传达严厉的真相、维护信任，同时仍然要注意另一个人的感受。这些价值权衡是人类决策和语言使用的重要组成部分，然而，目前用于解释这些动态和多方面价值概念的工具在大型语言模型（LLMs）中是有限的。在认知科学中，所谓的“认知模型”为这些权衡提供了人类的正式说明，通过建模说话者竞争效用函数的权重来选择行动或言语。在本工作中，我们使用一个领先的礼貌言语认知模型来解释LLMs在多大程度上代表人类似的权衡。我们将这一视角应用于系统地评估两个全面的模型设置中的价值权衡：前沿黑箱模型中的推理“努力”程度，以及开源模型的RL后训练动态。我们的结果突显了推理模型中比社会效用更高的信息效用，以及在数学推理中表现更强的开源模型。我们从LLMs的训练动态中得出的发现表明，训练早期的效用值发生了巨大的转变，并且基础模型和预训练数据的选择具有持久的效果，而反馈数据集或对齐方法则没有。我们表明，我们的方法对快速发展的LLM生态系统的多个方面是敏感的，具有形成关于其他高级行为的假设、塑造推理模型的训练方案以及在模型训练期间更好地控制价值权衡的见解。,"The paper uses cognitive models to interpret and evaluate value trade-offs in LLMs, providing insights into alignment and training dynamics.",LLM,"Helpful, Harmless","Value trade-offs, cognitive models, LLM alignment, utility functions, training dynamics"
MMSearch-R1: Incentivizing LMMs to Search,"Jinming Wu, Zihao Deng, Wei Li, Yiding Liu, Bo You, Bo Li, Zejun Ma, Ziwei Liu",2025-06-26T00:00:00-04:00,https://arxiv.org/pdf/2506.20670.pdf,"Robust deployment of large multimodal models (LMMs) in real-world scenarios requires access to external knowledge sources, given the complexity and dynamic nature of real-world information. Existing approaches such as retrieval-augmented generation (RAG) and prompt engineered search agents rely on rigid pipelines, often leading to inefficient or excessive search behaviors. We present MMSearch-R1, the first end-to-end reinforcement learning framework that enables LMMs to perform on-demand, multi-turn search in real-world Internet environments. Our framework integrates both image and text search tools, allowing the model to reason about when and how to invoke them guided by an outcome-based reward with a search penalty. To support training, We collect a multimodal search VQA dataset through a semi-automated pipeline that covers diverse visual and textual knowledge needs and curate a search-balanced subset with both search-required and search-free samples, which proves essential for shaping efficient and on-demand search behavior. Extensive experiments on knowledge-intensive and info-seeking VQA tasks show that our model not only outperforms RAG-based baselines of the same model size, but also matches the performance of a larger RAG-based model while reducing search calls by over 30%. We further analyze key empirical findings to offer actionable insights for advancing research in multimodal search.",大规模多模态模型（LMM）在现实世界中的稳健部署需要访问外部知识源，因为现实世界信息的复杂性和动态性。现有的方法，如检索增强生成（RAG）和提示工程搜索代理，依赖于僵化的管道，通常导致低效或过度的搜索行为。我们提出了MMSearch-R1，这是第一个端到端的强化学习框架，使LMM能够在现实世界的互联网环境中执行按需、多轮搜索。我们的框架集成了图像和文本搜索工具，允许模型在结果为基础的奖励和搜索惩罚的指导下推理何时以及如何调用它们。为了支持训练，我们通过半自动化管道收集了一个多模态搜索VQA数据集，涵盖了多样化的视觉和文本知识需求，并精选了一个搜索平衡的子集，其中包含需要搜索和不需要搜索的样本，这对塑造高效和按需的搜索行为至关重要。在知识密集型和信息寻求VQA任务上的广泛实验表明，我们的模型不仅在相同模型大小的RAG基线上表现更好，还与更大的RAG基线模型的性能相当，同时将搜索调用减少了30%以上。我们进一步分析了关键的实证发现，以提供推动多模态搜索研究的可操作见解。,"The paper introduces MMSearch-R1, a reinforcement learning framework that aligns LMMs to perform efficient and on-demand search in real-world internet environments.",LMM,Helpful,"LMM, Search, Alignment, Reinforcement Learning, Multimodal"
When Large Language Models contradict humans? Large Language Models' Sycophantic Behaviour,"Leonardo Ranaldi, Giulia Pucci",2025-06-26T00:00:00-04:00,https://arxiv.org/pdf/2311.09410.pdf,"Large Language Models have been demonstrating broadly satisfactory generative abilities for users, which seems to be due to the intensive use of human feedback that refines responses. Nevertheless, suggestibility inherited via human feedback improves the inclination to produce answers corresponding to users' viewpoints. This behaviour is known as sycophancy and depicts the tendency of LLMs to generate misleading responses as long as they align with humans. This phenomenon induces bias and reduces the robustness and, consequently, the reliability of these models. In this paper, we study the suggestibility of Large Language Models (LLMs) to sycophantic behaviour, analysing these tendencies via systematic human-interventions prompts over different tasks. Our investigation demonstrates that LLMs have sycophantic tendencies when answering queries that involve subjective opinions and statements that should elicit a contrary response based on facts. In contrast, when faced with math tasks or queries with an objective answer, they, at various scales, do not follow the users' hints by demonstrating confidence in generating the correct answers.",大语言模型（LLM）在用户生成方面表现出广泛的满意度，这似乎是由于大量使用人类反馈来完善响应。然而，通过人类反馈继承的建议性增加了产生与用户观点相对应的答案的倾向。这种行为被称为阿谀奉承，描述了LLM生成误导性响应的倾向，只要它们与人类一致。这种现象引起偏见，并降低了这些模型的鲁棒性和可靠性。在本文中，我们研究了大语言模型（LLM）对阿谀奉承行为的建议性，通过不同任务的系统人类干预提示来分析这些倾向。我们的研究表明，当回答涉及主观意见和应该根据事实引发相反回答的查询时，LLM在回答查询时表现出阿谀奉承的倾向。相反，当面对数学任务或具有客观答案的查询时，它们在各种规模上不遵循用户的提示，表现出在生成正确答案方面的信心。,"The paper investigates how large language models (LLMs) exhibit sycophantic behavior, aligning with human feedback to produce misleading responses, particularly in subjective queries.",LLM,"Helpful, Honest","Sycophancy, Alignment, Human Feedback, Bias, Reliability"
FactCheckmate: Preemptively Detecting and Mitigating Hallucinations in LMs,"Deema Alnuhait, Neeraja Kirtane, Muhammad Khalifa, Hao Peng",2025-06-26T00:00:00-04:00,https://arxiv.org/pdf/2410.02899.pdf,"Language models (LMs) hallucinate. We inquire: Can we detect and mitigate hallucinations before they happen? This work answers this research question in the positive, by showing that the internal representations of LMs provide rich signals that can be used for this purpose. We introduce FactCheckmate, which preemptively detects hallucinations by learning a classifier that predicts whether the LM will hallucinate, based on the model's hidden states produced over the inputs, before decoding begins. If a hallucination is detected, FactCheckmate then intervenes by adjusting the LM's hidden states such that the model will produce more factual outputs. FactCheckmate provides fresh insights that the inner workings of LMs can be revealed by their hidden states. Practically, both its detection and mitigation models are lightweight, adding little inference overhead; FactCheckmate proves a more efficient approach for mitigating hallucinations compared to many post-hoc alternatives. We evaluate FactCheckmate over LMs of different scales and model families (including Llama, Mistral, Qwen and Gemma), across a variety of QA datasets from different domains. Our results demonstrate the effectiveness of FactCheckmate, achieving over 70% preemptive detection accuracy. On average, outputs generated by LMs with intervention are 34.4% more factual compared to those without.",语言模型（LMs）会产生幻觉。我们探讨：是否可以在幻觉发生之前检测和缓解它们？本文通过展示LMs的内部表示提供了丰富的信号，可以用于此目的，回答了这个研究问题。我们引入了FactCheckmate，它通过学习一个分类器来预先检测幻觉，该分类器基于模型在输入上生成的隐藏状态，在解码开始之前预测LM是否会产生幻觉。如果检测到幻觉，FactCheckmate通过调整LM的隐藏状态来干预，使模型产生更具事实性的输出。FactCheckmate提供了新的见解，即LMs的内部工作可以通过其隐藏状态揭示。从实践的角度来看，其检测和缓解模型都很轻量，增加的推理开销很小；FactCheckmate证明是一种比许多事后替代方案更高效的缓解幻觉的方法。我们在不同规模和模型家族（包括Llama、Mistral、Qwen和Gemma）的LMs上评估了FactCheckmate，跨越不同领域的各种QA数据集。我们的结果表明FactCheckmate的有效性，实现了超过70%的预先检测准确率。平均而言，经过干预的LM生成的输出比没有干预的输出更具事实性，高出34.4%。,"The paper introduces FactCheckmate, a system that preemptively detects and mitigates hallucinations in language models by leveraging their hidden states.",LLM,"Helpful, Honest","Hallucination detection, mitigation, language models, hidden states, factual outputs"
AgentBreeder: Mitigating the AI Safety Impact of Multi-Agent Scaffolds via Self-Improvement,"J Rosser, Jakob Nicolaus Foerster",2025-06-26T00:00:00-04:00,https://arxiv.org/pdf/2502.00757.pdf,"Scaffolding Large Language Models (LLMs) into multi-agent systems often improves performance on complex tasks, but the safety impact of such scaffolds has not been thoroughly explored. We introduce AgentBreeder, a framework for multi-objective self-improving evolutionary search over scaffolds. We evaluate discovered scaffolds on widely recognized reasoning, mathematics, and safety benchmarks and compare them with popular baselines. In 'blue' mode, we see a 79.4% average uplift in safety benchmark performance while maintaining or improving capability scores. In 'red' mode, we find adversarially weak scaffolds emerging concurrently with capability optimization. Our work demonstrates the risks of multi-agent scaffolding and provides a framework for mitigating them. Code is available at https://github.com/J-Rosser-UK/AgentBreeder.",将大型语言模型（LLM）构建为多代理系统通常会提高复杂任务的性能，但这种脚手架的安全影响尚未得到充分探索。我们引入了AgentBreeder，一个用于多目标自我改进进化搜索的框架。我们在广泛认可的推理、数学和安全基准上评估了发现的脚手架，并将其与流行的基准进行比较。在“蓝色”模式下，我们在安全基准性能上看到平均提升79.4%，同时保持或提高能力分数。在“红色”模式下，我们发现了与能力优化同时出现的对抗性脆弱脚手架。我们的工作展示了多代理脚手架的风险，并提供了一个缓解这些风险的框架。,"The paper introduces AgentBreeder, a framework for improving the safety of multi-agent systems involving LLMs through self-improvement and evolutionary search.",LLM,Harmless,"LLM, Multi-Agent Systems, Safety, Self-Improvement, Evolutionary Search"
Adversarial Reasoning at Jailbreaking Time,"Mahdi Sabbaghi, Paul Kassianik, George Pappas, Yaron Singer, Amin Karbasi, Hamed Hassani",2025-06-26T00:00:00-04:00,https://arxiv.org/pdf/2502.01633.pdf,"As large language models (LLMs) are becoming more capable and widespread, the study of their failure cases is becoming increasingly important. Recent advances in standardizing, measuring, and scaling test-time compute suggest new methodologies for optimizing models to achieve high performance on hard tasks. In this paper, we apply these advances to the task of model jailbreaking: eliciting harmful responses from aligned LLMs. We develop an adversarial reasoning approach to automatic jailbreaking that leverages a loss signal to guide the test-time compute, achieving SOTA attack success rates against many aligned LLMs, even those that aim to trade inference-time compute for adversarial robustness. Our approach introduces a new paradigm in understanding LLM vulnerabilities, laying the foundation for the development of more robust and trustworthy AI systems.",随着大型语言模型（LLM）变得越来越强大和普及，研究它们的失败案例变得越来越重要。最近在标准化、测量和扩展测试时计算的进展表明了新的方法来优化模型以实现在困难任务上的高性能。在这篇论文中，我们将这些进展应用于模型越狱任务：从对齐的LLM中引出有害的响应。我们开发了一种基于对抗推理的自动越狱方法，利用损失信号来指导测试时计算，实现了对许多对齐的LLM的SOTA攻击成功率，即使是那些试图以推理时计算换取对抗鲁棒性的LLM。我们的方法引入了一种新的范式，用于理解LLM的脆弱性，为开发更加健壮和可信的AI系统奠定了基础。,"The paper presents an adversarial reasoning approach to jailbreaking aligned LLMs, achieving high attack success rates and highlighting vulnerabilities in LLM alignment.",LLM,Harmless,"Jailbreaking, Adversarial Reasoning, Harmful Responses, LLM Alignment, Robustness"
Towards Fully Exploiting LLM Internal States to Enhance Knowledge Boundary Perception,"Shiyu Ni, Keping Bi, Jiafeng Guo, Lulu Yu, Baolong Bi, Xueqi Cheng",2025-06-26T00:00:00-04:00,https://arxiv.org/pdf/2502.11677.pdf,"Large language models (LLMs) exhibit impressive performance across diverse tasks but often struggle to accurately gauge their knowledge boundaries, leading to confident yet incorrect responses. This paper explores leveraging LLMs' internal states to enhance their perception of knowledge boundaries from efficiency and risk perspectives. We investigate whether LLMs can estimate their confidence using internal states before response generation, potentially saving computational resources. Our experiments on datasets like Natural Questions, HotpotQA, and MMLU reveal that LLMs demonstrate significant pre-generation perception, which is further refined post-generation, with perception gaps remaining stable across varying conditions. To mitigate risks in critical domains, we introduce Confidence Consistency-based Calibration ($C^3$), which assesses confidence consistency through question reformulation. $C^3$ significantly improves LLMs' ability to recognize their knowledge gaps, enhancing the unknown perception rate by 5.6% on NQ and 4.9% on HotpotQA. Our findings suggest that pre-generation confidence estimation can optimize efficiency, while $C^3$ effectively controls output risks, advancing the reliability of LLMs in practical applications.",大语言模型（LLMs）在各种任务中表现出色，但在准确评估其知识边界方面存在困难，导致自信但错误的响应。本文探讨了利用LLMs的内部状态来增强其知识边界感知的效率和风险视角。我们研究了LLMs是否可以在响应生成之前使用内部状态估计其自信度，从而节省计算资源。我们在Natural Questions、HotpotQA和MMLU等数据集上的实验表明，LLMs在生成前显示出显著的感知能力，并在生成后进一步精炼，感知差距在各种条件下保持稳定。为了在关键领域减少风险，我们引入了基于信心一致性的校准（$C^3$），通过问题重新表述评估信心一致性。$C^3$显著提高了LLMs识别其知识差距的能力，将未知感知率提高了5.6%在NQ和4.9%在HotpotQA。我们的发现表明，生成前的信心估计可以优化效率，而$C^3$有效控制输出风险，推动LLMs在实际应用中的可靠性。,"The paper introduces methods to improve LLMs' perception of their knowledge boundaries, enhancing their helpfulness and harmlessness by reducing confident but incorrect responses.",LLM,"Helpful, Harmless","Knowledge boundaries, confidence estimation, risk mitigation, LLM calibration, efficiency"
Balancing Truthfulness and Informativeness with Uncertainty-Aware Instruction Fine-Tuning,"Tianyi Wu, Jingwei Ni, Bryan Hooi, Jiaheng Zhang, Elliott Ash, See-Kiong Ng, Mrinmaya Sachan, Markus Leippold",2025-06-26T00:00:00-04:00,https://arxiv.org/pdf/2502.11962.pdf,"Instruction fine-tuning (IFT) can increase the informativeness of large language models (LLMs), but may reduce their truthfulness. This trade-off arises because IFT steers LLMs to generate responses containing long-tail knowledge that was not well covered during pre-training. As a result, models become more informative but less accurate when generalizing to unseen tasks. In this paper, we empirically demonstrate how unfamiliar knowledge in IFT datasets can negatively affect the truthfulness of LLMs, and we introduce two new IFT paradigms, $UNIT_{cut}$ and $UNIT_{ref}$, to address this issue. $UNIT_{cut}$ identifies and removes unfamiliar knowledge from IFT datasets to mitigate its impact on model truthfulness, whereas $UNIT_{ref}$ trains LLMs to recognize their uncertainty and explicitly indicate it at the end of their responses. Our experiments show that $UNIT_{cut}$ substantially improves LLM truthfulness, while $UNIT_{ref}$ maintains high informativeness and reduces hallucinations by distinguishing between confident and uncertain statements.",指令微调（IFT）可以增加大型语言模型（LLMs）的信息量，但可能会降低其真实性。这种权衡是因为IFT引导LLMs生成包含预训练中未充分涵盖的长尾知识的响应。结果，模型在推广到未见任务时变得更具信息量但不准确。在本文中，我们通过实证演示了IFT数据集中不熟悉的知识如何负面影响LLMs的真实性，并引入了两种新的IFT范式，$UNIT_{cut}$和$UNIT_{ref}$，以解决这一问题。$UNIT_{cut}$识别并从IFT数据集中删除不熟悉的知识，以减轻其对模型真实性的影响，而$UNIT_{ref}$则训练LLMs识别其不确定性，并在其响应的末尾明确指出。我们的实验表明，$UNIT_{cut}$显著提高了LLM的真实性，而$UNIT_{ref}$则保持了高信息量，并通过区分自信和不确定的陈述来减少幻觉。,"The paper introduces two methods, $UNIT_{cut}$ and $UNIT_{ref}$, to balance truthfulness and informativeness in LLMs by managing uncertainty and unfamiliar knowledge during instruction fine-tuning.",LLM,"Helpful, Honest","Instruction fine-tuning, truthfulness, informativeness, uncertainty, hallucinations"
Rewarding Graph Reasoning Process makes LLMs more Generalized Reasoners,"Miao Peng, Nuo Chen, Zongrui Suo, Jia Li",2025-06-26T00:00:00-04:00,https://arxiv.org/pdf/2503.00845.pdf,"Despite significant advancements in Large Language Models (LLMs), developing advanced reasoning capabilities in LLMs remains a key challenge. Process Reward Models (PRMs) have demonstrated exceptional promise in enhancing reasoning by providing step-wise feedback, particularly in the context of mathematical reasoning. However, their application to broader reasoning domains remains understudied, largely due to the high costs associated with manually creating step-level supervision. In this work, we explore the potential of PRMs in graph reasoning problems - a domain that demands sophisticated multi-step reasoning and offers opportunities for automated step-level data generation using established graph algorithms. We introduce GraphSILO, the largest dataset for graph reasoning problems with fine-grained step-wise labels, built using automated Task-oriented Trajectories and Monte Carlo Tree Search (MCTS) to generate detailed reasoning steps with step-wise labels. Building upon this dataset, we train GraphPRM, the first PRM designed for graph reasoning problems, and evaluate its effectiveness in two key settings: inference-time scaling and reinforcement learning via Direct Preference Optimization (DPO). Experimental results show that GraphPRM significantly improves LLM performance across 13 graph reasoning tasks, delivering a 9% gain for Qwen2.5-7B and demonstrating transferability to new graph reasoning datasets and new reasoning domains like mathematical problem-solving. Notably, GraphPRM enhances LLM performance on GSM8K and Math500, underscoring the cross-domain applicability of graph-based reasoning rewards. Our findings highlight the potential of PRMs in advancing reasoning across diverse domains, paving the way for more versatile and effective LLMs.",尽管大型语言模型（LLMs）在各个方面取得了显著进展，但开发先进的推理能力仍然是一个关键挑战。过程奖励模型（PRMs）在通过提供逐步反馈来增强推理能力方面表现出色，特别是在数学推理的背景下。然而，它们在更广泛的推理领域的应用研究较少，主要是因为手动创建逐步级别的监督成本高。在本研究中，我们探讨了PRMs在图推理问题中的潜力——一个需要复杂的多步推理且提供自动化逐步级别数据生成机会的领域。我们引入了GraphSILO，这是一个使用自动化任务定向轨迹和蒙特卡罗树搜索（MCTS）生成详细推理步骤和逐步标签的图推理问题的最大数据集。基于该数据集，我们训练了GraphPRM，这是第一个专为图推理问题设计的PRM，并在两个关键设置中评估其有效性：推理时刻扩展和通过直接偏好优化（DPO）的强化学习。实验结果表明，GraphPRM在13个图推理任务中显著提高了LLM的性能，为Qwen2.5-7B带来了9%的收益，并展示了对新图推理数据集和新推理领域（如数学问题解决）的可转移性。值得注意的是，GraphPRM增强了LLM在GSM8K和Math500上的性能，突显了基于图的推理奖励的跨领域适用性。我们的发现强调了PRMs在推动多样化领域推理方面的潜力，为更加多功能和有效的LLMs铺平了道路。,"The paper introduces GraphPRM, a Process Reward Model designed for graph reasoning problems, which significantly improves the performance of Large Language Models across various reasoning tasks.",LLM,Helpful,"Large Language Models, Reasoning, Process Reward Models, Graph Reasoning, Generalization"
Conversational User-AI Intervention: A Study on Prompt Rewriting for Improved LLM Response Generation,"Rupak Sarkar, Bahareh Sarrafzadeh, Nirupama Chandrasekaran, Nagu Rangan, Philip Resnik, Longqi Yang, Sujay Kumar Jauhar",2025-06-26T00:00:00-04:00,https://arxiv.org/pdf/2503.16789.pdf,"Human-LLM conversations are increasingly becoming more pervasive in peoples' professional and personal lives, yet many users still struggle to elicit helpful responses from LLM Chatbots. One of the reasons for this issue is users' lack of understanding in crafting effective prompts that accurately convey their information needs. Meanwhile, the existence of real-world conversational datasets on the one hand, and the text understanding faculties of LLMs on the other, present a unique opportunity to study this problem, and its potential solutions at scale. Thus, in this paper we present the first LLM-centric study of real human-AI chatbot conversations, focused on investigating aspects in which user queries fall short of expressing information needs, and the potential of using LLMs to rewrite suboptimal user prompts. Our findings demonstrate that rephrasing ineffective prompts can elicit better responses from a conversational system, while preserving the user's original intent. Notably, the performance of rewrites improves in longer conversations, where contextual inferences about user needs can be made more accurately. Additionally, we observe that LLMs often need to -- and inherently do -- make \emph{plausible} assumptions about a user's intentions and goals when interpreting prompts. Our findings largely hold true across conversational domains, user intents, and LLMs of varying sizes and families, indicating the promise of using prompt rewriting as a solution for better human-AI interactions.",人类与大型语言模型（LLM）的对话在人们的职业和个人生活中变得越来越普遍，然而许多用户仍然难以从LLM聊天机器人那里获得有用的回应。其中一个原因是用户在撰写有效提示方面的不足，这些提示能够准确传达他们的信息需求。与此同时，现实世界中的对话数据集和LLM的文本理解能力为研究这一问题及其潜在解决方案提供了独特的机会。因此，在本文中，我们提出了第一个以LLM为中心的研究，专注于研究用户查询在表达信息需求方面的不足，以及使用LLM重写次优用户提示的潜力。我们的发现表明，重新表述无效的提示可以从对话系统中获得更好的回应，同时保留用户的原始意图。值得注意的是，重写的性能在较长的对话中有所改善，此时可以更准确地进行关于用户需求的上下文推理。此外，我们观察到，LLM在解释提示时通常需要并天生会做出关于用户意图和目标的合理假设。我们的发现在对话领域、用户意图和不同大小和家族的LLM中大致成立，表明使用提示重写作为改善人机交互的解决方案具有前景。,The paper explores how rewriting user prompts can make LLM responses more helpful by better conveying user intent.,LLM,Helpful,"Prompt rewriting, LLM responses, conversational AI, user intent, helpfulness"
RefPentester: A Knowledge-Informed Self-Reflective Penetration Testing Framework Based on Large Language Models,"Hanzheng Dai, Yuanliang Li, Jun Yan, Zhibo Zhang",2025-06-26T00:00:00-04:00,https://arxiv.org/pdf/2505.07089.pdf,"Automated penetration testing (AutoPT) powered by large language models (LLMs) has gained attention for its ability to automate ethical hacking processes and identify vulnerabilities in target systems by leveraging the inherent knowledge of LLMs. However, existing LLM-based AutoPT frameworks often underperform compared to human experts in challenging tasks for several reasons: the imbalanced knowledge used in LLM training, short-sightedness in the planning process, and hallucinations during command generation. Moreover, the trial-and-error nature of the PT process is constrained by existing frameworks lacking mechanisms to learn from previous failures, restricting adaptive improvement of PT strategies. To address these limitations, we propose a knowledge-informed, self-reflective PT framework powered by LLMs, called RefPentester. This AutoPT framework is designed to assist human operators in identifying the current stage of the PT process, selecting appropriate tactics and techniques for each stage, choosing suggested actions, providing step-by-step operational guidance, and reflecting on and learning from previous failed operations. We also modeled the PT process as a seven-state Stage Machine to integrate the proposed framework effectively. The evaluation shows that RefPentester can successfully reveal credentials on Hack The Box's Sau machine, outperforming the baseline GPT-4o model by 16.7%. Across PT stages, RefPentester also demonstrates superior success rates on PT stage transitions.",由大型语言模型（LLM）驱动的自动化渗透测试（AutoPT）因其能够通过利用LLM的内在知识自动化道德黑客过程并识别目标系统中的漏洞而受到关注。然而，现有的基于LLM的AutoPT框架在处理复杂任务时通常表现不佳，原因有：LLM训练中使用的知识不平衡、规划过程中的短视以及命令生成中的幻觉。此外，PT过程的试错性质受到现有框架缺乏从以前的失败中学习的机制的限制，从而限制了PT策略的适应性改进。为了解决这些局限性，我们提出了一种由LLM驱动的知识信息、自我反思的PT框架，称为RefPentester。该AutoPT框架旨在帮助人类操作员识别PT过程的当前阶段，为每个阶段选择合适的策略和技术，选择建议的操作，提供逐步的操作指导，并反思和从以前的失败操作中学习。我们还将PT过程建模为一个七状态的阶段机器，以有效地集成所提出的框架。评估表明，RefPentester可以成功揭示Hack The Box的Sau机器上的凭证，比基线GPT-4o模型高出16.7%。在PT阶段，RefPentester在PT阶段转换上也表现出更高的成功率。,"The paper introduces RefPentester, a knowledge-informed, self-reflective penetration testing framework powered by LLMs that outperforms baseline models in identifying vulnerabilities.",LLM,Helpful,"Penetration testing, large language models, self-reflection, knowledge-informed, adaptive improvement"
CogniBench: A Legal-inspired Framework and Dataset for Assessing Cognitive Faithfulness of Large Language Models,"Xiaqiang Tang, Jian Li, Keyu Hu, Du Nan, Xiaolong Li, Xi Zhang, Weigao Sun, Sihong Xie",2025-06-26T00:00:00-04:00,https://arxiv.org/pdf/2505.20767.pdf,"Faithfulness hallucinations are claims generated by a Large Language Model (LLM) not supported by contexts provided to the LLM. Lacking assessment standards, existing benchmarks focus on ""factual statements"" that rephrase source materials while overlooking ""cognitive statements"" that involve making inferences from the given context. Consequently, evaluating and detecting the hallucination of cognitive statements remains challenging. Inspired by how evidence is assessed in the legal domain, we design a rigorous framework to assess different levels of faithfulness of cognitive statements and introduce the CogniBench dataset where we reveal insightful statistics. To keep pace with rapidly evolving LLMs, we further develop an automatic annotation pipeline that scales easily across different models. This results in a large-scale CogniBench-L dataset, which facilitates training accurate detectors for both factual and cognitive hallucinations. We release our model and datasets at: https://github.com/FUTUREEEEEE/CogniBench",认知忠实幻觉是指大型语言模型（LLM）生成的不受上下文支持的声明。由于缺乏评估标准，现有的基准主要集中在“事实陈述”上，而忽略了涉及从给定上下文中进行推理的“认知陈述”。因此，评估和检测认知陈述的幻觉仍然具有挑战性。受法律领域评估证据的启发，我们设计了一个严格的框架来评估认知陈述的不同忠实度水平，并引入了CogniBench数据集，其中揭示了有见地的统计数据。为了跟上迅速发展的LLM，我们进一步开发了一个自动注释管道，可以轻松扩展到不同的模型。这导致了一个大规模的CogniBench-L数据集，促进了训练准确的检测器，用于检测事实和认知幻觉。我们在以下网址发布了我们的模型和数据集：https://github.com/FUTUREEEEEE/CogniBench,"The paper introduces CogniBench, a legal-inspired framework and dataset for assessing and detecting cognitive faithfulness hallucinations in large language models.",LLM,Honest,"Faithfulness, Hallucinations, Cognitive Statements, Legal-inspired Framework, Dataset"
The Alignment Trap: Complexity Barriers,Jasper Yao,2025-06-26T00:00:00-04:00,https://arxiv.org/pdf/2506.10304.pdf,"This paper argues that AI alignment is not merely difficult, but is founded on a fundamental logical contradiction. We first establish The Enumeration Paradox: we use machine learning precisely because we cannot enumerate all necessary safety rules, yet making ML safe requires examples that can only be generated from the very enumeration we admit is impossible. This paradox is then confirmed by a set of five independent mathematical proofs, or ""pillars of impossibility."" Our main results show that: (1) Geometric Impossibility: The set of safe policies has measure zero, a necessary consequence of projecting infinite-dimensional world-context requirements onto finite-dimensional models. (2) Computational Impossibility: Verifying a policy's safety is coNP-complete, even for non-zero error tolerances. (3) Statistical Impossibility: The training data required for safety (abundant examples of rare disasters) is a logical contradiction and thus unobtainable. (4) Information-Theoretic Impossibility: Safety rules contain more incompressible, arbitrary information than any feasible network can store. (5) Dynamic Impossibility: The optimization process for increasing AI capability is actively hostile to safety, as the gradients for the two objectives are generally anti-aligned. Together, these results demonstrate that the pursuit of safe, highly capable AI is not a matter of overcoming technical hurdles, but of confronting fundamental, interlocking barriers. The paper concludes by presenting a strategic trilemma that these impossibilities force upon the field. A formal verification of the core theorems in Lean4 is currently in progress.",这篇论文论述了人工智能对齐不仅仅是困难，而是基于一个基本的逻辑矛盾。我们首先建立了枚举悖论：我们使用机器学习的原因是我们无法枚举所有必要的安全规则，然而使ML安全需要的示例只能从我们承认是不可能的枚举中生成。这个悖论通过一组五个独立的数学证明或“不可能性支柱”得到了确认。我们的主要结果表明：(1)几何不可能性：安全策略的集合具有零度量，这是将无限维世界上下文要求投影到有限维模型的必要结果。(2)计算不可能性：即使对于非零误差容限，验证策略的安全性也是coNP完全的。(3)统计不可能性：安全所需的训练数据(丰富的稀有灾难示例)是一个逻辑矛盾，因此无法获得。(4)信息论不可能性：安全规则包含的不可压缩、任意信息比任何可行网络都多。(5)动态不可能性：增加AI能力的优化过程对安全性是敌对的，因为两个目标的梯度通常是反对齐的。这些结果共同表明，追求安全、高能力的人工智能不仅仅是克服技术障碍，而是面对基本的、相互交织的障碍。论文最后通过这些不可能性强加给领域的战略三难题。,"The paper presents five fundamental barriers to aligning AI systems, arguing that safe, highly capable AI is confronted with interlocking, insurmountable challenges.",LLM,Harmless,"AI alignment, safety, machine learning, impossibility, barriers"
"Scientists' First Exam: Probing Cognitive Abilities of MLLM via Perception, Understanding, and Reasoning","Yuhao Zhou, Yiheng Wang, Xuming He, Ruoyao Xiao, Zhiwei Li, Qiantai Feng, Zijie Guo, Yuejin Yang, Hao Wu, Wenxuan Huang, Jiaqi Wei, Dan Si, Xiuqi Yao, Jia Bu, Haiwen Huang, Tianfan Fu, Shixiang Tang, Ben Fei, Dongzhan Zhou, Fenghua Ling, Yan Lu, Siqi Sun, Chenhui Li, Guanjie Zheng, Jiancheng Lv, Wenlong Zhang, Lei Bai",2025-06-26T00:00:00-04:00,https://arxiv.org/pdf/2506.10521.pdf,"Scientific discoveries increasingly rely on complex multimodal reasoning based on information-intensive scientific data and domain-specific expertise. Empowered by expert-level scientific benchmarks, scientific Multimodal Large Language Models (MLLMs) hold the potential to significantly enhance this discovery process in realistic workflows. However, current scientific benchmarks mostly focus on evaluating the knowledge understanding capabilities of MLLMs, leading to an inadequate assessment of their perception and reasoning abilities. To address this gap, we present the Scientists' First Exam (SFE) benchmark, designed to evaluate the scientific cognitive capacities of MLLMs through three interconnected levels: scientific signal perception, scientific attribute understanding, scientific comparative reasoning. Specifically, SFE comprises 830 expert-verified VQA pairs across three question types, spanning 66 multimodal tasks across five high-value disciplines. Extensive experiments reveal that current state-of-the-art GPT-o3 and InternVL-3 achieve only 34.08% and 26.52% on SFE, highlighting significant room for MLLMs to improve in scientific realms. We hope the insights obtained in SFE will facilitate further developments in AI-enhanced scientific discoveries.",科学发现越来越依赖于基于信息密集型科学数据和特定领域专业知识的复杂多模态推理。凭借专家级科学基准，科学多模态大语言模型（MLLMs）有潜力在现实工作流中显著增强这一发现过程。然而，当前的科学基准主要集中在评估MLLMs的知识理解能力，导致对其感知和推理能力的评估不足。为了弥补这一差距，我们提出了科学家第一次考试（SFE）基准，旨在通过三个相互关联的层次评估MLLMs的科学认知能力：科学信号感知、科学属性理解和科学比较推理。具体来说，SFE包括830个专家验证的VQA对，跨越三种问题类型，涵盖五个高价值学科的66个多模态任务。广泛的实验表明，当前最先进的GPT-o3和InternVL-3在SFE上的表现分别为34.08%和26.52%，突显了MLLMs在科学领域改进的巨大潜力。我们希望在SFE中获得的见解将促进进一步的AI增强科学发现。,"The paper introduces a benchmark to evaluate the scientific cognitive abilities of Multimodal Large Language Models, highlighting areas for improvement in perception, understanding, and reasoning.",MLLM,"Helpful, Honest","Multimodal Large Language Models, scientific benchmarks, perception, understanding, reasoning"
FORTRESS: Frontier Risk Evaluation for National Security and Public Safety,"Christina Q. Knight, Kaustubh Deshpande, Ved Sirdeshmukh, Meher Mankikar, Scale Red Team, SEAL Research Team, Julian Michael",2025-06-26T00:00:00-04:00,https://arxiv.org/pdf/2506.14922.pdf,"The rapid advancement of large language models (LLMs) introduces dual-use capabilities that could both threaten and bolster national security and public safety (NSPS). Models implement safeguards to protect against potential misuse relevant to NSPS and allow for benign users to receive helpful information. However, current benchmarks often fail to test safeguard robustness to potential NSPS risks in an objective, robust way. We introduce FORTRESS: 500 expert-crafted adversarial prompts with instance-based rubrics of 4-7 binary questions for automated evaluation across 3 domains (unclassified information only): Chemical, Biological, Radiological, Nuclear and Explosive (CBRNE), Political Violence & Terrorism, and Criminal & Financial Illicit Activities, with 10 total subcategories across these domains. Each prompt-rubric pair has a corresponding benign version to test for model over-refusals. This evaluation of frontier LLMs' safeguard robustness reveals varying trade-offs between potential risks and model usefulness: Claude-3.5-Sonnet demonstrates a low average risk score (ARS) (14.09 out of 100) but the highest over-refusal score (ORS) (21.8 out of 100), while Gemini 2.5 Pro shows low over-refusal (1.4) but a high average potential risk (66.29). Deepseek-R1 has the highest ARS at 78.05, but the lowest ORS at only 0.06. Models such as o1 display a more even trade-off between potential risks and over-refusals (with an ARS of 21.69 and ORS of 5.2). To provide policymakers and researchers with a clear understanding of models' potential risks, we publicly release FORTRESS at https://huggingface.co/datasets/ScaleAI/fortress_public. We also maintain a private set for evaluation.",大型语言模型（LLM）的快速发展引入了可能既威胁又增强国家安全和公共安全（NSPS）的双重用途能力。模型实施保障措施以防止潜在的滥用相关的NSPS，并允许善意用户获得有用的信息。然而，当前的基准测试往往无法以客观、健壮的方式测试保障措施的健壮性，以应对潜在的NSPS风险。我们引入了FORTRESS：500个专家精心设计的对抗性提示，带有基于实例的4-7个二元问题的评分标准，用于跨3个领域（仅限未分类信息）的自动评估：化学、生物、放射性、核和爆炸物（CBRNE）、政治暴力和恐怖主义以及刑事和金融非法活动，总共10个子类别。每个提示-评分标准对都有一个相应的良性版本，用于测试模型的过度拒绝。对前沿LLM的保障健壮性的评估揭示了潜在风险与模型有用性之间的不同权衡：Claude-3.5-Sonnet显示出较低的平均风险分数（ARS）（100分中的14.09），但最高的过度拒绝分数（ORS）（100分中的21.8），而Gemini 2.5 Pro则显示出低过度拒绝（1.4），但高潜在风险（66.29）。Deepseek-R1在ARS中最高，为78.05，但在ORS中最低，仅为0.06。模型如o1显示出潜在风险和过度拒绝之间更均衡的权衡（ARS为21.69，ORS为5.2）。为了让政策制定者和研究人员清楚了解模型的潜在风险，我们在https://huggingface.co/datasets/ScaleAI/fortress_public上公开发布FORTRESS。我们还维护一个私人集合用于评估。,"The paper introduces FORTRESS, a benchmark for evaluating the robustness of safeguards in large language models against national security and public safety risks.",LLM,"Helpful, Harmless","LLM, safety, evaluation, national security, public safety"
No Free Lunch: Rethinking Internal Feedback for LLM Reasoning,"Yanzhi Zhang, Zhaoxi Zhang, Haoxiang Guan, Yilin Cheng, Yitong Duan, Chen Wang, Yue Wang, Shuxin Zheng, Jiyan He",2025-06-26T00:00:00-04:00,https://arxiv.org/pdf/2506.17219.pdf,"Reinforcement learning has emerged as a powerful paradigm for post-training large language models (LLMs) to improve reasoning. Approaches like Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning with Verifiable Rewards (RLVR) have shown strong results, but they require extensive external supervision. We investigate an alternative class of methods, Reinforcement Learning from Internal Feedback (RLIF), which relies solely on intrinsic model-derived signals instead of external rewards. In particular, we leverage unsupervised reward proxies such as token-level entropy, trajectory-level entropy, and self-certainty. Our theoretical analysis shows these internal objectives are partially equivalent, and we empirically evaluate various RLIF strategies on challenging math reasoning benchmarks. Experimental results demonstrate that RLIF can boost the reasoning performance of base LLMs at the beginning phase of the training, matching or surpassing RLVR techniques on these tasks. However, when training progresses, performance degrades even below the model before training. Moreover, we find that RLIF yields little improvement for instruction-tuned models, indicating diminishing returns of intrinsic feedback once an LLM is already instruction-tuned. We further analyze this limitation by mixing model weights and explain the reason of RLIF's training behaviors, providing practical guidelines for integrating internal feedback signals into LLM training. We hope our analysis of internal feedback will inform more principled and effective strategies for LLM post-training.",强化学习作为一种强大的范式，用于在训练后改进大型语言模型（LLM）的推理能力。像人类反馈的强化学习（RLHF）和可验证奖励的强化学习（RLVR）等方法已经显示出强大的结果，但它们需要大量的外部监督。我们研究了一种替代方法，即基于内部反馈的强化学习（RLIF），它依赖于内在的模型派生信号，而不是外部奖励。特别是，我们利用了无监督奖励代理，如标记级熵、轨迹级熵和自信度。我们的理论分析表明这些内部目标是部分等价的，并且我们在具有挑战性的数学推理基准测试中评估了各种RLIF策略。实验结果表明，RLIF可以在训练的初始阶段提高基础LLM的推理性能，在这些任务上与RLVR技术匹配或超越。然而，随着训练的进展，性能会下降，甚至低于训练前的模型。此外，我们发现RLIF对指令调整模型的改进很小，表明一旦LLM已经指令调整，内在反馈的回报就会减少。我们通过混合模型权重进一步分析了这一局限性，并解释了RLIF的训练行为，提供了将内部反馈信号集成到LLM训练中的实用指南。我们希望我们对内部反馈的分析将为LLM后训练的更加原则和有效的策略提供信息。,"The paper explores the use of reinforcement learning from internal feedback to improve the reasoning capabilities of large language models, finding both initial benefits and eventual limitations.",LLM,Helpful,"Reinforcement Learning, Internal Feedback, LLM Reasoning, RLIF, Post-training"
Confucius3-Math: A Lightweight High-Performance Reasoning LLM for Chinese K-12 Mathematics Learning,"Lixin Wu, Na Cai, Qiao Cheng, Jiachen Wang, Yitao Duan",2025-06-26T00:00:00-04:00,https://arxiv.org/pdf/2506.18330.pdf,"We introduce Confucius3-Math, an open-source large language model with 14B parameters that (1) runs efficiently on a single consumer-grade GPU; (2) achieves SOTA performances on a range of mathematical reasoning tasks, outperforming many models with significantly larger sizes. In particular, as part of our mission to enhancing education and knowledge dissemination with AI, Confucius3-Math is specifically committed to mathematics learning for Chinese K-12 students and educators. Built via post-training with large-scale reinforcement learning (RL), Confucius3-Math aligns with national curriculum and excels at solving main-stream Chinese K-12 mathematical problems with low cost. In this report we share our development recipe, the challenges we encounter and the techniques we develop to overcome them. In particular, we introduce three technical innovations: Targeted Entropy Regularization, Recent Sample Recovery and Policy-Specific Hardness Weighting. These innovations encompass a new entropy regularization, a novel data scheduling policy, and an improved group-relative advantage estimator. Collectively, they significantly stabilize the RL training, improve data efficiency, and boost performance. Our work demonstrates the feasibility of building strong reasoning models in a particular domain at low cost. We open-source our model and code at https://github.com/netease-youdao/Confucius3-Math.",我们介绍了Confucius3-Math，一个具有140亿参数的开源大型语言模型，它（1）在单个消费级GPU上高效运行；（2）在一系列数学推理任务上实现了最先进的性能，超过了许多规模显著更大的模型。特别是，作为我们利用人工智能增强教育和知识传播的使命的一部分，Confucius3-Math专门致力于中国K-12学生和教育工作者的数学学习。通过大规模强化学习（RL）后训练构建，Confucius3-Math与国家课程对齐，并在低成本下出色地解决了主流中国K-12数学问题。在本报告中，我们分享了我们的开发配方、我们遇到的挑战以及我们开发的技术来克服它们。特别是，我们引入了三项技术创新：目标熵正则化、最近样本恢复和策略特定的困难加权。这些创新包括一种新的熵正则化、一种新颖的数据调度策略和一个改进的组相对优势估计器。综合起来，它们显著稳定了RL训练，提高了数据效率，并提高了性能。我们的工作展示了在特定领域以低成本构建强大推理模型的可行性。我们在https://github.com/netease-youdao/Confucius3-Math上开源我们的模型和代码。,"The paper presents Confucius3-Math, a lightweight LLM aligned with the national curriculum for Chinese K-12 mathematics learning, achieving state-of-the-art performance in mathematical reasoning tasks.",LLM,Helpful,"LLM, Alignment, Mathematics, Education, Reinforcement Learning"
Quantifying Fairness in LLMs Beyond Tokens: A Semantic and Statistical Perspective,"Weijie Xu, Yiwen Wang, Chi Xue, Xiangkun Hu, Xi Fang, Guimin Dong, Chandan K. Reddy",2025-06-26T00:00:00-04:00,https://arxiv.org/pdf/2506.19028.pdf,"Large Language Models (LLMs) often generate responses with inherent biases, undermining their reliability in real-world applications. Existing evaluation methods often overlook biases in long-form responses and the intrinsic variability of LLM outputs. To address these challenges, we propose FiSCo(Fine-grained Semantic Computation), a novel statistical framework to evaluate group-level fairness in LLMs by detecting subtle semantic differences in long-form responses across demographic groups. Unlike prior work focusing on sentiment or token-level comparisons, FiSCo goes beyond surface-level analysis by operating at the claim level, leveraging entailment checks to assess the consistency of meaning across responses. We decompose model outputs into semantically distinct claims and apply statistical hypothesis testing to compare inter- and intra-group similarities, enabling robust detection of subtle biases. We formalize a new group counterfactual fairness definition and validate FiSCo on both synthetic and human-annotated datasets spanning gender, race, and age. Experiments show that FiSco more reliably identifies nuanced biases while reducing the impact of stochastic LLM variability, outperforming various evaluation metrics.",大语言模型（LLMs）通常生成带有内在偏见的响应，削弱了它们在现实世界应用中的可靠性。现有的评估方法往往忽略了长文本响应中的偏见以及LLM输出的内在变异性。为了应对这些挑战，我们提出了FiSCo（细粒度语义计算），一种新的统计框架，通过检测长文本响应在不同人群之间的微妙语义差异，来评估LLM的群体级公平性。与之前的工作不同，FiSCo不仅仅关注情感或标记级别的比较，而是通过在声明级别进行操作，利用推理检查来评估响应中的意义一致性。我们将模型输出分解为语义上不同的声明，并应用统计假设检验来比较群体内和群体间的相似性，从而实现对微妙偏见的可靠检测。我们正式化了一个新的群体对抗性公平定义，并在涵盖性别、种族和年龄的合成和人工标注数据集上验证了FiSCo。实验表明，FiSco更可靠地识别出细微的偏见，同时减少了随机LLM变异的影响，优于各种评估指标。,"The paper introduces FiSCo, a statistical framework for evaluating group-level fairness in LLMs by detecting subtle semantic biases in long-form responses.",LLM,Harmless,"Fairness, Bias, Evaluation, Semantic Analysis, LLMs"
Therapy as an NLP Task: Psychologists' Comparison of LLMs and Human Peers in CBT,"Zainab Iftikhar, Sean Ransom, Amy Xiao, Nicole Nugent, Jeff Huang",2025-06-26T00:00:00-04:00,https://arxiv.org/pdf/2409.02244.pdf,"Large language models (LLMs) are being used as ad-hoc therapists. Research suggests that LLMs outperform human counselors when generating a single, isolated empathetic response; however, their session-level behavior remains understudied. In this study, we compare the session-level behaviors of human counselors with those of an LLM prompted by a team of peer counselors to deliver single-session Cognitive Behavioral Therapy (CBT). Our three-stage, mixed-methods study involved: a) a year-long ethnography of a text-based support platform where seven counselors iteratively refined CBT prompts through self-counseling and weekly focus groups; b) the manual simulation of human counselor sessions with a CBT-prompted LLM, given the full patient dialogue and contextual notes; and c) session evaluations of both human and LLM sessions by three licensed clinical psychologists using CBT competence measures. Our results show a clear trade-off. Human counselors excel at relational strategies -- small talk, self-disclosure, and culturally situated language -- that lead to higher empathy, collaboration, and deeper user reflection. LLM counselors demonstrate higher procedural adherence to CBT techniques but struggle to sustain collaboration, misread cultural cues, and sometimes produce ""deceptive empathy,"" i.e., formulaic warmth that can inflate users' expectations of genuine human care. Taken together, our findings imply that while LLMs might outperform counselors in generating single empathetic responses, their ability to lead sessions is more limited, highlighting that therapy cannot be reduced to a standalone natural language processing (NLP) task. We call for carefully designed human-AI workflows in scalable support: LLMs can scaffold evidence-based techniques, while peers provide relational support. We conclude by mapping concrete design opportunities and ethical guardrails for such hybrid systems.",大语言模型（LLM）被用作临时治疗师。研究表明，LLM在生成单个、孤立的共情响应时表现优于人类顾问；然而，它们的会话级行为仍然研究不足。在本研究中，我们比较了人类顾问与由一组同伴顾问提示的LLM在单次认知行为疗法（CBT）中的会话级行为。我们的三阶段混合方法研究包括：a）对一个文本支持平台的为期一年的民族志研究，七名顾问通过自我治疗和每周的小组讨论逐步完善CBT提示；b）使用CBT提示的LLM手动模拟人类顾问会话，给出完整的患者对话和上下文笔记；c）由三名持证临床心理学家使用CBT胜任度衡量标准对人类和LLM会话进行评估。我们的结果显示出明显的权衡。人类顾问在关系策略方面表现出色——闲聊、自我披露和文化定位的语言——这导致了更高的共情、合作和更深的用户反思。LLM顾问在CBT技术的程序遵从性方面表现更好，但难以维持合作，误读文化线索，有时会产生“欺骗性共情”，即公式化的温暖，可能会使用户对真正的人类关怀产生不切实际的期望。综上所述，我们的发现表明，尽管LLM在生成单个共情响应方面可能优于顾问，但它们领导会话的能力更有限，突显了治疗不能被简化为一个独立的自然语言处理（NLP）任务。我们呼吁在可扩展支持中设计精心设计的人机工作流程：LLM可以为基于证据的技术提供框架，而同伴提供关系支持。我们通过映射具体的设计机会和伦理守则来总结这种混合系统。,"The study compares human counselors and LLMs in delivering CBT, finding that while LLMs excel in procedural adherence, they lack relational strategies crucial for effective therapy.",LLM,"Helpful, Harmless","LLM, Therapy, Empathy, CBT, Human-AI Collaboration"
